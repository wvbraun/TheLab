.\" Man page generated from reStructuredText.
.
.TH "SCRAPY" "1" "January 09, 2014" "0.21" "Scrapy"
.SH NAME
scrapy \- Scrapy 0.21.0
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.sp
This documentation contains everything you need to know about Scrapy.
.SH GETTING HELP
.sp
Having trouble? We\(aqd like to help!
.INDENT 0.0
.IP \(bu 2
Try the \fBFAQ\fP \-\- it\(aqs got answers to some common questions.
.IP \(bu 2
Looking for specific information? Try the \fIgenindex\fP or \fImodindex\fP\&.
.IP \(bu 2
Search for information in the \fI\%archives of the scrapy\-users mailing list\fP, or
\fI\%post a question\fP\&.
.IP \(bu 2
Ask a question in the \fI\%#scrapy IRC channel\fP\&.
.IP \(bu 2
Report bugs with Scrapy in our \fI\%issue tracker\fP\&.
.UNINDENT
.SH FIRST STEPS
.SS Scrapy at a glance
.sp
Scrapy is an application framework for crawling web sites and extracting
structured data which can be used for a wide range of useful applications, like
data mining, information processing or historical archival.
.sp
Even though Scrapy was originally designed for \fI\%screen scraping\fP (more
precisely, \fI\%web scraping\fP), it can also be used to extract data using APIs
(such as \fI\%Amazon Associates Web Services\fP) or as a general purpose web
crawler.
.sp
The purpose of this document is to introduce you to the concepts behind Scrapy
so you can get an idea of how it works and decide if Scrapy is what you need.
.sp
When you\(aqre ready to start a project, you can \fIstart with the tutorial\fP\&.
.SS Pick a website
.sp
So you need to extract some information from a website, but the website doesn\(aqt
provide any API or mechanism to access that info programmatically.  Scrapy can
help you extract that information.
.sp
Let\(aqs say we want to extract the URL, name, description and size of all torrent
files added today in the \fI\%Mininova\fP site.
.sp
The list of all torrents added today can be found on this page:
.INDENT 0.0
.INDENT 3.5
\fI\%http://www.mininova.org/today\fP
.UNINDENT
.UNINDENT
.SS Define the data you want to scrape
.sp
The first thing is to define the data we want to scrape. In Scrapy, this is
done through \fIScrapy Items\fP (Torrent files, in this case).
.sp
This would be our Item:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item, Field

class TorrentItem(Item):
    url = Field()
    name = Field()
    description = Field()
    size = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Write a Spider to extract the data
.sp
The next thing is to write a Spider which defines the start URL
(\fI\%http://www.mininova.org/today\fP), the rules for following links and the rules
for extracting the data from pages.
.sp
If we take a look at that page content we\(aqll see that all torrent URLs are like
\fI\%http://www.mininova.org/tor/NUMBER\fP where \fBNUMBER\fP is an integer. We\(aqll use
that to construct the regular expression for the links to follow: \fB/tor/\ed+\fP\&.
.sp
We\(aqll use \fI\%XPath\fP for selecting the data to extract from the web page HTML
source. Let\(aqs take one of those torrent pages:
.INDENT 0.0
.INDENT 3.5
\fI\%http://www.mininova.org/tor/2657665\fP
.UNINDENT
.UNINDENT
.sp
And look at the page HTML source to construct the XPath to select the data we
want which is: torrent name, description and size.
.sp
By looking at the page HTML source we can see that the file name is contained
inside a \fB<h1>\fP tag:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<h1>Home[2009][Eng]XviD\-ovd</h1>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
An XPath expression to extract the name could be:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
//h1/text()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And the description is contained inside a \fB<div>\fP tag with \fBid="description"\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<h2>Description:</h2>

<div id="description">
"HOME" \- a documentary film by Yann Arthus\-Bertrand
<br/>
<br/>
***
<br/>
<br/>
"We are living in exceptional times. Scientists tell us that we have 10 years to change the way we live, avert the depletion of natural resources and the catastrophic evolution of the Earth\(aqs climate.

\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
An XPath expression to select the description could be:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
//div[@id=\(aqdescription\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Finally, the file size is contained in the second \fB<p>\fP tag inside the \fB<div>\fP
tag with \fBid=specifications\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<div id="specifications">

<p>
<strong>Category:</strong>
<a href="/cat/4">Movies</a> &gt; <a href="/sub/35">Documentary</a>
</p>

<p>
<strong>Total size:</strong>
699.79&nbsp;megabyte</p>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
An XPath expression to select the file size could be:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
//div[@id=\(aqspecifications\(aq]/p[2]/text()[2]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For more information about XPath see the \fI\%XPath reference\fP\&.
.sp
Finally, here\(aqs the spider code:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MininovaSpider(CrawlSpider):

    name = \(aqmininova\(aq
    allowed_domains = [\(aqmininova.org\(aq]
    start_urls = [\(aqhttp://www.mininova.org/today\(aq]
    rules = [Rule(SgmlLinkExtractor(allow=[\(aq/tor/\ed+\(aq]), \(aqparse_torrent\(aq)]

    def parse_torrent(self, response):
        sel = Selector(response)
        torrent = TorrentItem()
        torrent[\(aqurl\(aq] = response.url
        torrent[\(aqname\(aq] = sel.xpath("//h1/text()").extract()
        torrent[\(aqdescription\(aq] = sel.xpath("//div[@id=\(aqdescription\(aq]").extract()
        torrent[\(aqsize\(aq] = sel.xpath("//div[@id=\(aqinfo\-left\(aq]/p[2]/text()[2]").extract()
        return torrent
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For brevity\(aqs sake, we intentionally left out the import statements. The
Torrent item is \fIdefined above\fP\&.
.SS Run the spider to extract the data
.sp
Finally, we\(aqll run the spider to crawl the site an output file
\fBscraped_data.json\fP with the scraped data in JSON format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl mininova \-o scraped_data.json \-t json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This uses \fIfeed exports\fP to generate the JSON file.
You can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or \fI\%Amazon S3\fP, for example).
.sp
You can also write an \fIitem pipeline\fP to store the
items in a database very easily.
.SS Review scraped data
.sp
If you check the \fBscraped_data.json\fP file after the process finishes, you\(aqll
see the scraped items there:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[{"url": "http://www.mininova.org/tor/2657665", "name": ["Home[2009][Eng]XviD\-ovd"], "description": ["HOME \- a documentary film by ..."], "size": ["699.69 megabyte"]},
# ... other items ...
]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You\(aqll notice that all field values (except for the \fBurl\fP which was assigned
directly) are actually lists. This is because the \fIselectors\fP return lists. You may want to store single values, or
perform some additional parsing/cleansing to the values. That\(aqs what
\fIItem Loaders\fP are for.
.SS What else?
.sp
You\(aqve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:
.INDENT 0.0
.IP \(bu 2
Built\-in support for \fIselecting and extracting\fP data
from HTML and XML sources
.IP \(bu 2
Built\-in support for cleaning and sanitizing the scraped data using a
collection of reusable filters (called \fIItem Loaders\fP)
shared between all the spiders.
.IP \(bu 2
Built\-in support for \fIgenerating feed exports\fP in
multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,
S3, local filesystem)
.IP \(bu 2
A media pipeline for \fIautomatically downloading images\fP
(or any other media) associated with the scraped items
.IP \(bu 2
Support for \fI\%extending Scrapy\fP by plugging
your own functionality using \fIsignals\fP and a
well\-defined API (middlewares, \fIextensions\fP, and
\fIpipelines\fP).
.IP \(bu 2
Wide range of built\-in middlewares and extensions for:
.INDENT 2.0
.IP \(bu 2
cookies and session handling
.IP \(bu 2
HTTP compression
.IP \(bu 2
HTTP authentication
.IP \(bu 2
HTTP cache
.IP \(bu 2
user\-agent spoofing
.IP \(bu 2
robots.txt
.IP \(bu 2
crawl depth restriction
.IP \(bu 2
and more
.UNINDENT
.IP \(bu 2
Robust encoding support and auto\-detection, for dealing with foreign,
non\-standard and broken encoding declarations.
.IP \(bu 2
Support for creating spiders based on pre\-defined templates, to speed up
spider creation and make their code more consistent on large projects. See
\fBgenspider\fP command for more details.
.IP \(bu 2
Extensible \fIstats collection\fP for multiple spider
metrics, useful for monitoring the performance of your spiders and detecting
when they get broken
.IP \(bu 2
An \fIInteractive shell console\fP for trying XPaths, very
useful for writing and debugging your spiders
.IP \(bu 2
A \fISystem service\fP designed to ease the deployment and
run of your spiders in production.
.IP \(bu 2
A built\-in \fIWeb service\fP for monitoring and
controlling your bot
.IP \(bu 2
A \fITelnet console\fP for hooking into a Python
console running inside your Scrapy process, to introspect and debug your
crawler
.IP \(bu 2
\fILogging\fP facility that you can hook on to for catching
errors during the scraping process.
.IP \(bu 2
Support for crawling based on URLs discovered through \fI\%Sitemaps\fP
.IP \(bu 2
A caching DNS resolver
.UNINDENT
.SS What\(aqs next?
.sp
The next obvious steps are for you to \fI\%download Scrapy\fP, read \fIthe
tutorial\fP and join \fI\%the community\fP\&. Thanks for your
interest!
.SS Installation guide
.SS Pre\-requisites
.sp
The installation steps assume that you have the following things installed:
.INDENT 0.0
.IP \(bu 2
\fI\%Python\fP 2.7
.IP \(bu 2
\fI\%lxml\fP\&. Most Linux distributions ships prepackaged versions of lxml. Otherwise refer to \fI\%http://lxml.de/installation.html\fP
.IP \(bu 2
\fI\%OpenSSL\fP\&. This comes preinstalled in all operating systems except Windows (see \fIintro\-install\-platform\-notes\fP)
.IP \(bu 2
\fI\%pip\fP or \fI\%easy_install\fP Python package managers
.UNINDENT
.SS Installing Scrapy
.sp
You can install Scrapy using easy_install or pip (which is the canonical way to
distribute and install Python packages).
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Check \fIintro\-install\-platform\-notes\fP first.
.UNINDENT
.UNINDENT
.sp
To install using pip:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
pip install Scrapy
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To install using easy_install:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
easy_install Scrapy
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Platform specific installation notes
.SS Windows
.sp
After installing Python, follow these steps before installing Scrapy:
.INDENT 0.0
.IP \(bu 2
add the \fBC:\epython27\eScripts\fP and \fBC:\epython27\fP folders to the system
path by adding those directories to the \fBPATH\fP environment variable from
the \fI\%Control Panel\fP\&.
.IP \(bu 2
install OpenSSL by following these steps:
.INDENT 2.0
.IP 1. 3
go to \fI\%Win32 OpenSSL page\fP
.IP 2. 3
download Visual C++ 2008 redistributables for your Windows and architecture
.IP 3. 3
download OpenSSL for your Windows and architecture (the regular version, not the light one)
.IP 4. 3
add the \fBc:\eopenssl\-win32\ebin\fP (or similar) directory to your \fBPATH\fP, the same way you added \fBpython27\fP in the first step\(ga\(ga in the first step
.UNINDENT
.IP \(bu 2
some binary packages that Scrapy depends on (like Twisted, lxml and pyOpenSSL) require a compiler available to install, and fail if you don\(aqt have Visual Studio installed. You can find Windows installers for those in the following links. Make sure you respect your Python version and Windows architecture.
.INDENT 2.0
.IP \(bu 2
pywin32: \fI\%http://sourceforge.net/projects/pywin32/files/\fP
.IP \(bu 2
Twisted: \fI\%http://twistedmatrix.com/trac/wiki/Downloads\fP
.IP \(bu 2
zope.interface: download the egg from \fI\%zope.interface pypi page\fP and install it by running \fBeasy_install file.egg\fP
.IP \(bu 2
lxml: \fI\%http://pypi.python.org/pypi/lxml/\fP
.IP \(bu 2
pyOpenSSL: \fI\%https://launchpad.net/pyopenssl\fP
.UNINDENT
.UNINDENT
.sp
Finally, this page contains many precompiled Python binary libraries, which may
come handy to fulfill Scrapy dependencies:
.INDENT 0.0
.INDENT 3.5
\fI\%http://www.lfd.uci.edu/~gohlke/pythonlibs/\fP
.UNINDENT
.UNINDENT
.SS Ubuntu 9.10 or above
.sp
\fBDon\(aqt\fP use the \fBpython\-scrapy\fP package provided by Ubuntu, they are
typically too old and slow to catch up with latest Scrapy.
.sp
Instead, use the official \fIUbuntu Packages\fP, which already
solve all dependencies for you and are continuously updated with the latest bug
fixes.
.SS Scrapy Tutorial
.sp
In this tutorial, we\(aqll assume that Scrapy is already installed on your system.
If that\(aqs not the case, see \fIintro\-install\fP\&.
.sp
We are going to use \fI\%Open directory project (dmoz)\fP as
our example domain to scrape.
.sp
This tutorial will walk you through these tasks:
.INDENT 0.0
.IP 1. 3
Creating a new Scrapy project
.IP 2. 3
Defining the Items you will extract
.IP 3. 3
Writing a \fIspider\fP to crawl a site and extract
\fIItems\fP
.IP 4. 3
Writing an \fIItem Pipeline\fP to store the
extracted Items
.UNINDENT
.sp
Scrapy is written in \fI\%Python\fP\&. If you\(aqre new to the language you might want to
start by getting an idea of what the language is like, to get the most out of
Scrapy.  If you\(aqre already familiar with other languages, and want to learn
Python quickly, we recommend \fI\%Learn Python The Hard Way\fP\&.  If you\(aqre new to programming
and want to start with Python, take a look at \fI\%this list of Python resources
for non\-programmers\fP\&.
.SS Creating a project
.sp
Before you start scraping, you will have set up a new Scrapy project. Enter a
directory where you\(aqd like to store your code and then run:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy startproject tutorial
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This will create a \fBtutorial\fP directory with the following contents:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
These are basically:
.INDENT 0.0
.IP \(bu 2
\fBscrapy.cfg\fP: the project configuration file
.IP \(bu 2
\fBtutorial/\fP: the project\(aqs python module, you\(aqll later import your code from
here.
.IP \(bu 2
\fBtutorial/items.py\fP: the project\(aqs items file.
.IP \(bu 2
\fBtutorial/pipelines.py\fP: the project\(aqs pipelines file.
.IP \(bu 2
\fBtutorial/settings.py\fP: the project\(aqs settings file.
.IP \(bu 2
\fBtutorial/spiders/\fP: a directory where you\(aqll later put your spiders.
.UNINDENT
.SS Defining our Item
.sp
\fIItems\fP are containers that will be loaded with the scraped data; they work
like simple python dicts but provide additional protecting against populating
undeclared fields, to prevent typos.
.sp
They are declared by creating an \fBscrapy.item.Item\fP class and defining
its attributes as \fBscrapy.item.Field\fP objects, like you will in an ORM
(don\(aqt worry if you\(aqre not familiar with ORMs, you will see that this is an
easy task).
.sp
We begin by modeling the item that we will use to hold the sites data obtained
from dmoz.org, as we want to capture the name, url and description of the
sites, we define fields for each of these three attributes. To do that, we edit
items.py, found in the \fBtutorial\fP directory. Our Item class looks like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item, Field

class DmozItem(Item):
    title = Field()
    link = Field()
    desc = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This may seem complicated at first, but defining the item allows you to use other handy
components of Scrapy that need to know how your item looks like.
.SS Our first Spider
.sp
Spiders are user\-written classes used to scrape information from a domain (or group
of domains).
.sp
They define an initial list of URLs to download, how to follow links, and how
to parse the contents of those pages to extract \fIitems\fP\&.
.sp
To create a Spider, you must subclass \fBscrapy.spider.Spider\fP, and
define the three main, mandatory, attributes:
.INDENT 0.0
.IP \(bu 2
\fBname\fP: identifies the Spider. It must be
unique, that is, you can\(aqt set the same name for different Spiders.
.IP \(bu 2
\fBstart_urls\fP: is a list of URLs where the
Spider will begin to crawl from.  So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.
.IP \(bu 2
\fBparse()\fP is a method of the spider, which will
be called with the downloaded \fBResponse\fP object of each
start URL. The response is passed to the method as the first and only
argument.
.sp
This method is responsible for parsing the response data and extracting
scraped data (as scraped items) and more URLs to follow.
.sp
The \fBparse()\fP method is in charge of processing
the response and returning scraped data (as \fBItem\fP
objects) and more URLs to follow (as \fBRequest\fP objects).
.UNINDENT
.sp
This is the code for our first Spider; save it in a file named
\fBdmoz_spider.py\fP under the \fBtutorial/spiders\fP directory:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.spider import Spider

class DmozSpider(Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        filename = response.url.split("/")[\-2]
        open(filename, \(aqwb\(aq).write(response.body)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Crawling
.sp
To put our spider to work, go to the project\(aqs top level directory and run:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl dmoz
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBcrawl dmoz\fP command runs the spider for the \fBdmoz.org\fP domain. You
will get an output similar to this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
2008\-08\-20 03:51:13\-0300 [scrapy] INFO: Started project: dmoz
2008\-08\-20 03:51:13\-0300 [tutorial] INFO: Enabled extensions: ...
2008\-08\-20 03:51:13\-0300 [tutorial] INFO: Enabled downloader middlewares: ...
2008\-08\-20 03:51:13\-0300 [tutorial] INFO: Enabled spider middlewares: ...
2008\-08\-20 03:51:13\-0300 [tutorial] INFO: Enabled item pipelines: ...
2008\-08\-20 03:51:14\-0300 [dmoz] INFO: Spider opened
2008\-08\-20 03:51:14\-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: <None>)
2008\-08\-20 03:51:14\-0300 [dmoz] DEBUG: Crawled <http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: <None>)
2008\-08\-20 03:51:14\-0300 [dmoz] INFO: Spider closed (finished)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Pay attention to the lines containing \fB[dmoz]\fP, which corresponds to our
spider. You can see a log line for each URL defined in \fBstart_urls\fP\&. Because
these URLs are the starting ones, they have no referrers, which is shown at the
end of the log line, where it says \fB(referer: <None>)\fP\&.
.sp
But more interesting, as our \fBparse\fP method instructs, two files have been
created: \fIBooks\fP and \fIResources\fP, with the content of both URLs.
.SS What just happened under the hood?
.sp
Scrapy creates \fBscrapy.http.Request\fP objects for each URL in the
\fBstart_urls\fP attribute of the Spider, and assigns them the \fBparse\fP method of
the spider as their callback function.
.sp
These Requests are scheduled, then executed, and
\fBscrapy.http.Response\fP objects are returned and then fed back to the
spider, through the \fBparse()\fP method.
.SS Extracting Items
.SS Introduction to Selectors
.sp
There are several ways to extract data from web pages. Scrapy uses a mechanism
based on \fI\%XPath\fP or \fI\%CSS\fP expressions called \fIScrapy Selectors\fP\&.  For more information about selectors and other extraction
mechanisms see the \fISelectors documentation\fP\&.
.sp
Here are some examples of XPath expressions and their meanings:
.INDENT 0.0
.IP \(bu 2
\fB/html/head/title\fP: selects the \fB<title>\fP element, inside the \fB<head>\fP
element of a HTML document
.IP \(bu 2
\fB/html/head/title/text()\fP: selects the text inside the aforementioned
\fB<title>\fP element.
.IP \(bu 2
\fB//td\fP: selects all the \fB<td>\fP elements
.IP \(bu 2
\fB//div[@class="mine"]\fP: selects all \fBdiv\fP elements which contain an
attribute \fBclass="mine"\fP
.UNINDENT
.sp
These are just a couple of simple examples of what you can do with XPath, but
XPath expressions are indeed much more powerful. To learn more about XPath we
recommend \fI\%this XPath tutorial\fP\&.
.sp
For working with XPaths, Scrapy provides a \fBSelector\fP
class, it is instantiated with a \fBHtmlResponse\fP or
\fBXmlResponse\fP object as first argument.
.sp
You can see selectors as objects that represent nodes in the document
structure. So, the first instantiated selectors are associated to the root
node, or the entire document.
.sp
Selectors have four basic methods (click on the method to see the complete API
documentation).
.INDENT 0.0
.IP \(bu 2
\fBxpath()\fP: returns a list of selectors, each of
them representing the nodes selected by the xpath expression given as
argument.
.IP \(bu 2
\fBcss()\fP: returns a list of selectors, each of
them representing the nodes selected by the CSS expression given as argument.
.IP \(bu 2
\fBextract()\fP: returns a unicode string with the
selected data.
.IP \(bu 2
\fBre()\fP: returns a list of unicode strings
extracted by applying the regular expression given as argument.
.UNINDENT
.SS Trying Selectors in the Shell
.sp
To illustrate the use of Selectors we\(aqre going to use the built\-in \fIScrapy
shell\fP, which also requires IPython (an extended Python console)
installed on your system.
.sp
To start a shell, you must go to the project\(aqs top level directory and run:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Remember to always enclose urls with quotes when running Scrapy shell from
command\-line, otherwise urls containing arguments (ie. \fB&\fP character)
will not work.
.UNINDENT
.UNINDENT
.sp
This is what the shell looks like:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[ ... Scrapy log here ... ]

[s] Available Scrapy objects:
[s] 2010\-08\-19 21:45:59\-0300 [default] INFO: Spider closed (finished)
[s]   sel        <Selector (http://www.dmoz.org/Computers/Programming/Languages/Python/Books/) xpath=None>
[s]   item       Item()
[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   spider     <Spider \(aqdefault\(aq at 0x1b6c2d0>
[s] Useful shortcuts:
[s]   shelp()           Print this help
[s]   fetch(req_or_url) Fetch a new request or URL and update shell objects
[s]   view(response)    View response in a browser

In [1]:
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
After the shell loads, you will have the response fetched in a local
\fBresponse\fP variable, so if you type \fBresponse.body\fP you will see the body
of the response, or you can type \fBresponse.headers\fP to see its headers.
.sp
The shell also pre\-instantiate a selector for this response in variable \fBsel\fP,
the selector automatically chooses the best parsing rules (XML vs HTML) based
on response\(aqs type.
.sp
So let\(aqs try it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
In [1]: sel.xpath(\(aq//title\(aq)
Out[1]: [<Selector (title) xpath=//title>]

In [2]: sel.xpath(\(aq//title\(aq).extract()
Out[2]: [u\(aq<title>Open Directory \- Computers: Programming: Languages: Python: Books</title>\(aq]

In [3]: sel.xpath(\(aq//title/text()\(aq)
Out[3]: [<Selector (text) xpath=//title/text()>]

In [4]: sel.xpath(\(aq//title/text()\(aq).extract()
Out[4]: [u\(aqOpen Directory \- Computers: Programming: Languages: Python: Books\(aq]

In [5]: sel.xpath(\(aq//title/text()\(aq).re(\(aq(\ew+):\(aq)
Out[5]: [u\(aqComputers\(aq, u\(aqProgramming\(aq, u\(aqLanguages\(aq, u\(aqPython\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Extracting the data
.sp
Now, let\(aqs try to extract some real information from those pages.
.sp
You could type \fBresponse.body\fP in the console, and inspect the source code to
figure out the XPaths you need to use. However, inspecting the raw HTML code
there could become a very tedious task. To make this an easier task, you can
use some Firefox extensions like Firebug. For more information see
\fItopics\-firebug\fP and \fItopics\-firefox\fP\&.
.sp
After inspecting the page source, you\(aqll find that the web sites information
is inside a \fB<ul>\fP element, in fact the \fIsecond\fP \fB<ul>\fP element.
.sp
So we can select each \fB<li>\fP element belonging to the sites list with this
code:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath(\(aq//ul/li\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And from them, the sites descriptions:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath(\(aq//ul/li/text()\(aq).extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The sites titles:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath(\(aq//ul/li/a/text()\(aq).extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And the sites links:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath(\(aq//ul/li/a/@href\(aq).extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As we said before, each \fB\&.xpath()\fP call returns a list of selectors, so we can
concatenate further \fB\&.xpath()\fP calls to dig deeper into a node. We are going to use
that property here, so:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sites = sel.xpath(\(aq//ul/li\(aq)
for site in sites:
    title = site.xpath(\(aqa/text()\(aq).extract()
    link = site.xpath(\(aqa/@href\(aq).extract()
    desc = site.xpath(\(aqtext()\(aq).extract()
    print title, link, desc
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
For a more detailed description of using nested selectors, see
\fItopics\-selectors\-nesting\-selectors\fP and
\fItopics\-selectors\-relative\-xpaths\fP in the \fItopics\-selectors\fP
documentation
.UNINDENT
.UNINDENT
.sp
Let\(aqs add this code to our spider:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.spider import Spider
from scrapy.selector import Selector

class DmozSpider(Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        sel = Selector(response)
        sites = sel.xpath(\(aq//ul/li\(aq)
        for site in sites:
            title = site.xpath(\(aqa/text()\(aq).extract()
            link = site.xpath(\(aqa/@href\(aq).extract()
            desc = site.xpath(\(aqtext()\(aq).extract()
            print title, link, desc
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Notice we import our Selector class from scrapy.selector and instantiate a
new Selector object.  We can now specify our XPaths just as we did in the shell.
Now try crawling the dmoz.org domain again and you\(aqll see sites being printed
in your output, run:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl dmoz
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using our item
.sp
\fBItem\fP objects are custom python dicts; you can access the
values of their fields (attributes of the class we defined earlier) using the
standard dict syntax like:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> item = DmozItem()
>>> item[\(aqtitle\(aq] = \(aqExample title\(aq
>>> item[\(aqtitle\(aq]
\(aqExample title\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Spiders are expected to return their scraped data inside
\fBItem\fP objects. So, in order to return the data we\(aqve
scraped so far, the final code for our Spider would be like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.spider import Spider
from scrapy.selector import Selector

from tutorial.items import DmozItem

class DmozSpider(Spider):
   name = "dmoz"
   allowed_domains = ["dmoz.org"]
   start_urls = [
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
   ]

   def parse(self, response):
       sel = Selector(response)
       sites = sel.xpath(\(aq//ul/li\(aq)
       items = []
       for site in sites:
           item = DmozItem()
           item[\(aqtitle\(aq] = site.xpath(\(aqa/text()\(aq).extract()
           item[\(aqlink\(aq] = site.xpath(\(aqa/@href\(aq).extract()
           item[\(aqdesc\(aq] = site.xpath(\(aqtext()\(aq).extract()
           items.append(item)
       return items
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
You can find a fully\-functional variant of this spider in the \fI\%dirbot\fP
project available at \fI\%https://github.com/scrapy/dirbot\fP
.UNINDENT
.UNINDENT
.sp
Now doing a crawl on the dmoz.org domain yields \fBDmozItem\fP\(aqs:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
     {\(aqdesc\(aq: [u\(aq \- By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\en],
      \(aqlink\(aq: [u\(aqhttp://gnosis.cx/TPiP/\(aq],
      \(aqtitle\(aq: [u\(aqText Processing in Python\(aq]}
[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
     {\(aqdesc\(aq: [u\(aq \- By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD\-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\en\(aq],
      \(aqlink\(aq: [u\(aqhttp://www.informit.com/store/product.aspx?isbn=0130211192\(aq],
      \(aqtitle\(aq: [u\(aqXML Processing with Python\(aq]}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Storing the scraped data
.sp
The simplest way to store the scraped data is by using the \fIFeed exports\fP, with the following command:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl dmoz \-o items.json \-t json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
That will generate a \fBitems.json\fP file containing all scraped items,
serialized in \fI\%JSON\fP\&.
.sp
In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an \fIItem Pipeline\fP\&. As with Items, a
placeholder file for Item Pipelines has been set up for you when the project is
created, in \fBtutorial/pipelines.py\fP\&. Though you don\(aqt need to implement any item
pipeline if you just want to store the scraped items.
.SS Next steps
.sp
This tutorial covers only the basics of Scrapy, but there\(aqs a lot of other
features not mentioned here. Check the \fItopics\-whatelse\fP section in
\fIintro\-overview\fP chapter for a quick overview of the most important ones.
.sp
Then, we recommend you continue by playing with an example project (see
\fIintro\-examples\fP), and then continue with the section
\fI\%Basic concepts\fP\&.
.SS Examples
.sp
The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named \fI\%dirbot\fP, that you can use to
play and learn more about Scrapy. It contains the dmoz spider described in the
tutorial.
.sp
This \fI\%dirbot\fP project is available at: \fI\%https://github.com/scrapy/dirbot\fP
.sp
It contains a README file with a detailed description of the project contents.
.sp
If you\(aqre familiar with git, you can checkout the code. Otherwise you can
download a tarball or zip file of the project by clicking on \fI\%Downloads\fP\&.
.sp
The \fI\%scrapy tag on Snipplr\fP is used for sharing code snippets such as spiders,
middlewares, extensions, or scripts. Feel free (and encouraged!) to share any
code there.
.INDENT 0.0
.TP
.B \fBintro/overview\fP
Understand what Scrapy is and how it can help you.
.TP
.B \fBintro/install\fP
Get Scrapy installed on your computer.
.TP
.B \fBintro/tutorial\fP
Write your first Scrapy project.
.TP
.B \fBintro/examples\fP
Learn more by playing with a pre\-made Scrapy project.
.UNINDENT
.SH BASIC CONCEPTS
.SS Command line tool
.sp
New in version 0.10.

.sp
Scrapy is controlled through the \fBscrapy\fP command\-line tool, to be referred
here as the "Scrapy tool" to differentiate it from their sub\-commands which we
just call "commands", or "Scrapy commands".
.sp
The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.
.SS Default structure of Scrapy projects
.sp
Before delving into the command\-line tool and its sub\-commands, let\(aqs first
understand the directory structure of a Scrapy project.
.sp
Even thought it can be modified, all Scrapy projects have the same file
structure by default, similar to this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The directory where the \fBscrapy.cfg\fP file resides is known as the \fIproject
root directory\fP\&. That file contains the name of the python module that defines
the project settings. Here is an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[settings]
default = myproject.settings
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using the \fBscrapy\fP tool
.sp
You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Scrapy X.Y \- no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The first line will print the currently active project, if you\(aqre inside a
Scrapy project. In this, it was run from outside a project. If run from inside
a project it would have printed something like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Scrapy X.Y \- project: myproject

Usage:
  scrapy <command> [options] [args]

[...]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Creating projects
.sp
The first thing you typically do with the \fBscrapy\fP tool is create your Scrapy
project:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy startproject myproject
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
That will create a Scrapy project under the \fBmyproject\fP directory.
.sp
Next, you go inside the new project directory:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
cd myproject
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And you\(aqre ready to use the \fBscrapy\fP command to manage and control your
project from there.
.SS Controlling projects
.sp
You use the \fBscrapy\fP tool from inside your projects to control and manage
them.
.sp
For example, to create a new spider:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy genspider mydomain mydomain.com
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Some Scrapy commands (like \fBcrawl\fP) must be run from inside a Scrapy
project. See the \fIcommands reference\fP below for more
information on which commands must be run from inside projects, and which not.
.sp
Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider\-overridden behaviours (such as the \fBuser_agent\fP attribute to override
the user\-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the \fBfetch\fP command is meant to be used to
check how spiders are downloading pages.
.SS Available tool commands
.sp
This section contains a list of the available built\-in commands with a
description and some usage examples. Remember you can always get more info
about each command by running:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy <command> \-h
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And you can see all available commands with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy \-h
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
There are two kinds of commands, those that only work from inside a Scrapy
project (Project\-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly different
when running from inside a project (as they would use the project overridden
settings).
.sp
Global commands:
.INDENT 0.0
.IP \(bu 2
\fBstartproject\fP
.IP \(bu 2
\fBsettings\fP
.IP \(bu 2
\fBrunspider\fP
.IP \(bu 2
\fBshell\fP
.IP \(bu 2
\fBfetch\fP
.IP \(bu 2
\fBview\fP
.IP \(bu 2
\fBversion\fP
.UNINDENT
.sp
Project\-only commands:
.INDENT 0.0
.IP \(bu 2
\fBcrawl\fP
.IP \(bu 2
\fBcheck\fP
.IP \(bu 2
\fBlist\fP
.IP \(bu 2
\fBedit\fP
.IP \(bu 2
\fBparse\fP
.IP \(bu 2
\fBgenspider\fP
.IP \(bu 2
\fBdeploy\fP
.IP \(bu 2
\fBbench\fP
.UNINDENT
.SS startproject
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy startproject <project_name>\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Creates a new Scrapy project named \fBproject_name\fP, under the \fBproject_name\fP
directory.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy startproject myproject
.ft P
.fi
.UNINDENT
.UNINDENT
.SS genspider
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy genspider [\-t template] <name> <domain>\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Create a new spider in the current project.
.sp
This is just a convenient shortcut command for creating spiders based on
pre\-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy genspider \-l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider \-d basic
from scrapy.spider import Spider

class $classname(Spider):
    name = "$name"
    allowed_domains = ["$domain"]
    start_urls = (
        \(aqhttp://www.$domain/\(aq,
        )

    def parse(self, response):
        pass

$ scrapy genspider \-t basic example example.com
Created spider \(aqexample\(aq using template \(aqbasic\(aq in module:
  mybot.spiders.example
.ft P
.fi
.UNINDENT
.UNINDENT
.SS crawl
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy crawl <spider>\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Start crawling a spider.
.sp
Usage examples:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS check
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy check [\-l] <spider>\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Run contract checks.
.sp
Usage examples:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy check \-l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> \(aqRetailPricex\(aq field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
.ft P
.fi
.UNINDENT
.UNINDENT
.SS list
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy list\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
List all available spiders in the current project. The output is one spider per
line.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy list
spider1
spider2
.ft P
.fi
.UNINDENT
.UNINDENT
.SS edit
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy edit <spider>\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Edit the given spider using the editor defined in the \fBEDITOR\fP
setting.
.sp
This command is provided only as a convenient shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug his spiders.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy edit spider1
.ft P
.fi
.UNINDENT
.UNINDENT
.SS fetch
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy fetch <url>\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.
.sp
The interesting thing about this command is that it fetches the page how the
the spider would download it. For example, if the spider has an \fBUSER_AGENT\fP
attribute which overrides the User Agent, it will use that one.
.sp
So this command can be used to "see" how your spider would fetch certain page.
.sp
If used outside a project, no particular per\-spider behaviour would be applied
and it will just use the default Scrapy downloder settings.
.sp
Usage examples:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy fetch \-\-nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch \-\-nolog \-\-headers http://www.example.com/
{\(aqAccept\-Ranges\(aq: [\(aqbytes\(aq],
 \(aqAge\(aq: [\(aq1263   \(aq],
 \(aqConnection\(aq: [\(aqclose     \(aq],
 \(aqContent\-Length\(aq: [\(aq596\(aq],
 \(aqContent\-Type\(aq: [\(aqtext/html; charset=UTF\-8\(aq],
 \(aqDate\(aq: [\(aqWed, 18 Aug 2010 23:59:46 GMT\(aq],
 \(aqEtag\(aq: [\(aq"573c1\-254\-48c9c87349680"\(aq],
 \(aqLast\-Modified\(aq: [\(aqFri, 30 Jul 2010 15:30:18 GMT\(aq],
 \(aqServer\(aq: [\(aqApache/2.2.3 (CentOS)\(aq]}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS view
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy view <url>\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Opens the given URL in a browser, as your Scrapy spider would "see" it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider "sees" and confirm it\(aqs what you expect.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS shell
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy shell [url]\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Starts the Scrapy shell for the given URL (if given) or empty if not URL is
given. See \fItopics\-shell\fP for more info.
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS parse
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy parse <url> [options]\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Fetches the given URL and parses with the spider that handles it, using the
method passed with the \fB\-\-callback\fP option, or \fBparse\fP if not given.
.sp
Supported options:
.INDENT 0.0
.IP \(bu 2
\fB\-\-callback\fP or \fB\-c\fP: spider method to use as callback for parsing the
response
.IP \(bu 2
\fB\-\-rules\fP or \fB\-r\fP: use \fBCrawlSpider\fP
rules to discover the callback (ie. spider method) to use for parsing the
response
.IP \(bu 2
\fB\-\-noitems\fP: don\(aqt show scraped items
.IP \(bu 2
\fB\-\-nolinks\fP: don\(aqt show extracted links
.IP \(bu 2
\fB\-\-depth\fP or \fB\-d\fP: depth level for which the requests should be followed
recursively (default: 1)
.IP \(bu 2
\fB\-\-verbose\fP or \fB\-v\fP: display information for each depth level
.UNINDENT
.sp
Usage example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy parse http://www.example.com/ \-c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[{\(aqname\(aq: u\(aqExample item\(aq,
 \(aqcategory\(aq: u\(aqFurniture\(aq,
 \(aqlength\(aq: u\(aq12 cm\(aq}]

# Requests  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS settings
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy settings [options]\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Get the value of a Scrapy setting.
.sp
If used inside a project it\(aqll show the project setting value, otherwise it\(aqll
show the default Scrapy value for that setting.
.sp
Example usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy settings \-\-get BOT_NAME
scrapybot
$ scrapy settings \-\-get DOWNLOAD_DELAY
0
.ft P
.fi
.UNINDENT
.UNINDENT
.SS runspider
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy runspider <spider_file.py>\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Run a spider self\-contained in a Python file, without having to create a
project.
.sp
Example usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS version
.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy version [\-v]\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Prints the Scrapy version. If used with \fB\-v\fP it also prints Python, Twisted
and Platform info, which is useful for bug reports.
.SS deploy
.sp
New in version 0.11.

.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy deploy [ <target:project> | \-l <target> | \-L ]\fP
.IP \(bu 2
Requires project: \fIyes\fP
.UNINDENT
.sp
Deploy the project into a Scrapyd server. See \fI\%Deploying your project\fP\&.
.SS bench
.sp
New in version 0.17.

.INDENT 0.0
.IP \(bu 2
Syntax: \fBscrapy bench\fP
.IP \(bu 2
Requires project: \fIno\fP
.UNINDENT
.sp
Run quick benchmark test. \fIbenchmarking\fP\&.
.SS Custom project commands
.sp
You can also add your custom project commands by using the
\fBCOMMANDS_MODULE\fP setting. See the Scrapy commands in
\fI\%scrapy/commands\fP for examples on how to implement your commands.
.SS COMMANDS_MODULE
.sp
Default: \fB\(aq\(aq\fP (empty string)
.sp
A module to use for looking custom Scrapy commands. This is used to add custom
commands for your Scrapy project.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
COMMANDS_MODULE = \(aqmybot.commands\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Items
.sp
The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. Scrapy provides the \fBItem\fP class for this
purpose.
.sp
\fBItem\fP objects are simple containers used to collect the scraped data.
They provide a \fI\%dictionary\-like\fP API with a convenient syntax for declaring
their available fields.
.SS Declaring Items
.sp
Items are declared using a simple class definition syntax and \fBField\fP
objects. Here is an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item, Field

class Product(Item):
    name = Field()
    price = Field()
    stock = Field()
    last_updated = Field(serializer=str)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Those familiar with \fI\%Django\fP will notice that Scrapy Items are
declared similar to \fI\%Django Models\fP, except that Scrapy Items are much
simpler as there is no concept of different field types.
.UNINDENT
.UNINDENT
.SS Item Fields
.sp
\fBField\fP objects are used to specify metadata for each field. For
example, the serializer function for the \fBlast_updated\fP field illustrated in
the example above.
.sp
You can specify any kind of metadata for each field. There is no restriction on
the values accepted by \fBField\fP objects. For this same
reason, there isn\(aqt a reference list of all available metadata keys. Each key
defined in \fBField\fP objects could be used by a different components, and
only those components know about it. You can also define and use any other
\fBField\fP key in your project too, for your own needs. The main goal of
\fBField\fP objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.
.sp
It\(aqs important to note that the \fBField\fP objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the \fBItem.fields\fP attribute.
.sp
And that\(aqs all you need to know about declaring items.
.SS Working with Items
.sp
Here are some examples of common tasks performed with items, using the
\fBProduct\fP item \fIdeclared above\fP\&. You will
notice the API is very similar to the \fI\%dict API\fP\&.
.SS Creating items
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> product = Product(name=\(aqDesktop PC\(aq, price=1000)
>>> print product
Product(name=\(aqDesktop PC\(aq, price=1000)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Getting field values
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> product[\(aqname\(aq]
Desktop PC
>>> product.get(\(aqname\(aq)
Desktop PC

>>> product[\(aqprice\(aq]
1000

>>> product[\(aqlast_updated\(aq]
Traceback (most recent call last):
    ...
KeyError: \(aqlast_updated\(aq

>>> product.get(\(aqlast_updated\(aq, \(aqnot set\(aq)
not set

>>> product[\(aqlala\(aq] # getting unknown field
Traceback (most recent call last):
    ...
KeyError: \(aqlala\(aq

>>> product.get(\(aqlala\(aq, \(aqunknown field\(aq)
\(aqunknown field\(aq

>>> \(aqname\(aq in product  # is name field populated?
True

>>> \(aqlast_updated\(aq in product  # is last_updated populated?
False

>>> \(aqlast_updated\(aq in product.fields  # is last_updated a declared field?
True

>>> \(aqlala\(aq in product.fields  # is lala a declared field?
False
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Setting field values
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> product[\(aqlast_updated\(aq] = \(aqtoday\(aq
>>> product[\(aqlast_updated\(aq]
today

>>> product[\(aqlala\(aq] = \(aqtest\(aq # setting unknown field
Traceback (most recent call last):
    ...
KeyError: \(aqProduct does not support field: lala\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Accessing all populated values
.sp
To access all populated values, just use the typical \fI\%dict API\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> product.keys()
[\(aqprice\(aq, \(aqname\(aq]

>>> product.items()
[(\(aqprice\(aq, 1000), (\(aqname\(aq, \(aqDesktop PC\(aq)]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Other common tasks
.sp
Copying items:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> product2 = Product(product)
>>> print product2
Product(name=\(aqDesktop PC\(aq, price=1000)

>>> product3 = product2.copy()
>>> print product3
Product(name=\(aqDesktop PC\(aq, price=1000)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Creating dicts from items:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> dict(product) # create a dict from all populated values
{\(aqprice\(aq: 1000, \(aqname\(aq: \(aqDesktop PC\(aq}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Creating items from dicts:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> Product({\(aqname\(aq: \(aqLaptop PC\(aq, \(aqprice\(aq: 1500})
Product(price=1500, name=\(aqLaptop PC\(aq)

>>> Product({\(aqname\(aq: \(aqLaptop PC\(aq, \(aqlala\(aq: 1500}) # warning: unknown field in dict
Traceback (most recent call last):
    ...
KeyError: \(aqProduct does not support field: lala\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Extending Items
.sp
You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class DiscountedProduct(Product):
    discount_percent = Field(serializer=str)
    discount_expiration_date = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class SpecificProduct(Product):
    name = Field(Product.fields[\(aqname\(aq], serializer=my_serializer)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
That adds (or replaces) the \fBserializer\fP metadata key for the \fBname\fP field,
keeping all the previously existing metadata values.
.SS Item objects
.INDENT 0.0
.TP
.B class scrapy.item.Item([arg])
Return a new Item optionally initialized from the given argument.
.sp
Items replicate the standard \fI\%dict API\fP, including its constructor. The
only additional attribute provided by Items is:
.INDENT 7.0
.TP
.B fields
A dictionary containing \fIall declared fields\fP for this Item, not only
those populated. The keys are the field names and the values are the
\fBField\fP objects used in the \fIItem declaration\fP\&.
.UNINDENT
.UNINDENT
.SS Field objects
.INDENT 0.0
.TP
.B class scrapy.item.Field([arg])
The \fBField\fP class is just an alias to the built\-in \fI\%dict\fP class and
doesn\(aqt provide any extra functionality or attributes. In other words,
\fBField\fP objects are plain\-old Python dicts. A separate class is used
to support the \fIitem declaration syntax\fP
based on class attributes.
.UNINDENT
.SS Spiders
.sp
Spiders are classes which define how a certain site (or group of sites) will be
scraped, including how to perform the crawl (ie. follow links) and how to
extract structured data from their pages (ie. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, group of sites).
.sp
For spiders, the scraping cycle goes through something like this:
.INDENT 0.0
.IP 1. 3
You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.
.sp
The first requests to perform are obtained by calling the
\fBstart_requests()\fP method which (by default)
generates \fBRequest\fP for the URLs specified in the
\fBstart_urls\fP and the
\fBparse\fP method as callback function for the
Requests.
.IP 2. 3
In the callback function, you parse the response (web page) and return either
\fBItem\fP objects, \fBRequest\fP objects,
or an iterable of both. Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.
.IP 3. 3
In callback functions, you parse the page contents, typically using
\fItopics\-selectors\fP (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.
.IP 4. 3
Finally, the items returned from the spider will be typically persisted to a
database (in some \fIItem Pipeline\fP) or written to
a file using \fItopics\-feed\-exports\fP\&.
.UNINDENT
.sp
Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.
.SS Spider arguments
.sp
Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.
.sp
Spider arguments are passed through the \fBcrawl\fP command using the
\fB\-a\fP option. For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl myspider \-a category=electronics
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Spiders receive arguments in their constructors:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MySpider(Spider):
    name = \(aqmyspider\(aq

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = [\(aqhttp://www.example.com/categories/%s\(aq % category]
        # ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Spider arguments can also be passed through the Scrapyd \fBschedule.json\fP API.
See \fI\%Scrapyd documentation\fP\&.
.SS Built\-in spiders reference
.sp
Scrapy comes with some useful generic spiders that you can use, to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from \fI\%Sitemaps\fP, or parsing a XML/CSV feed.
.sp
For the examples used in the following spiders, we\(aqll assume you have a project
with a \fBTestItem\fP declared in a \fBmyproject.items\fP module:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item

class TestItem(Item):
    id = Field()
    name = Field()
    description = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Spider
.INDENT 0.0
.TP
.B class scrapy.spider.Spider
This is the simplest spider, and the one from which every other spider
must inherit from (either the ones that come bundled with Scrapy, or the ones
that you write yourself). It doesn\(aqt provide any special functionality. It just
requests the given \fBstart_urls\fP/\fBstart_requests\fP, and calls the spider\(aqs
method \fBparse\fP for each of the resulting responses.
.INDENT 7.0
.TP
.B name
A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it\(aqs required.
.sp
If the spider scrapes a single domain, a common practice is to name the
spider after the domain, or without the \fI\%TLD\fP\&. So, for example, a
spider that crawls \fBmywebsite.com\fP would often be called
\fBmywebsite\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B allowed_domains
An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list won\(aqt be followed if
\fBOffsiteMiddleware\fP is enabled.
.UNINDENT
.INDENT 7.0
.TP
.B start_urls
A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.
.UNINDENT
.INDENT 7.0
.TP
.B start_requests()
This method must return an iterable with the first Requests to crawl for
this spider.
.sp
This is the method called by Scrapy when the spider is opened for
scraping when no particular URLs are specified. If particular URLs are
specified, the \fBmake_requests_from_url()\fP is used instead to create
the Requests. This method is also called only once from Scrapy, so it\(aqs
safe to implement it as a generator.
.sp
The default implementation uses \fBmake_requests_from_url()\fP to
generate Requests for each url in \fBstart_urls\fP\&.
.sp
If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
def start_requests(self):
    return [FormRequest("http://www.example.com/login",
                        formdata={\(aquser\(aq: \(aqjohn\(aq, \(aqpass\(aq: \(aqsecret\(aq},
                        callback=self.logged_in)]

def logged_in(self, response):
    # here you would extract links to follow and return Requests for
    # each of them, with another callback
    pass
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B make_requests_from_url(url)
A method that receives a URL and returns a \fBRequest\fP
object (or a list of \fBRequest\fP objects) to scrape. This
method is used to construct the initial requests in the
\fBstart_requests()\fP method, and is typically used to convert urls to
requests.
.sp
Unless overridden, this method returns Requests with the \fBparse()\fP
method as their callback function, and with dont_filter parameter enabled
(see \fBRequest\fP class for more info).
.UNINDENT
.INDENT 7.0
.TP
.B parse(response)
This is the default callback used by Scrapy to process downloaded
responses, when their requests don\(aqt specify a callback.
.sp
The \fBparse\fP method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the \fBSpider\fP class.
.sp
This method, as well as any other Request callback, must return an
iterable of \fBRequest\fP and/or
\fBItem\fP objects.
.INDENT 7.0
.TP
.B Parameters
\fBresponse\fP (\fI:class:~scrapy.http.Response\(ga\fP) \-\- the response to parse
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B log(message[, level, component])
Log a message using the \fBscrapy.log.msg()\fP function, automatically
populating the spider argument with the \fBname\fP of this
spider. For more information see \fItopics\-logging\fP\&.
.UNINDENT
.UNINDENT
.SS Spider example
.sp
Let\(aqs see an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import log # This module is useful for printing out debug information
from scrapy.spider import Spider

class MySpider(Spider):
    name = \(aqexample.com\(aq
    allowed_domains = [\(aqexample.com\(aq]
    start_urls = [
        \(aqhttp://www.example.com/1.html\(aq,
        \(aqhttp://www.example.com/2.html\(aq,
        \(aqhttp://www.example.com/3.html\(aq,
    ]

    def parse(self, response):
        self.log(\(aqA response from %s just arrived!\(aq % response.url)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Another example returning multiples Requests and Items from a single callback:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.selector import Selector
from scrapy.spider import Spider
from scrapy.http import Request
from myproject.items import MyItem

class MySpider(Spider):
    name = \(aqexample.com\(aq
    allowed_domains = [\(aqexample.com\(aq]
    start_urls = [
        \(aqhttp://www.example.com/1.html\(aq,
        \(aqhttp://www.example.com/2.html\(aq,
        \(aqhttp://www.example.com/3.html\(aq,
    ]

    def parse(self, response):
        sel = Selector(response)
        for h3 in sel.xpath(\(aq//h3\(aq).extract():
            yield MyItem(title=h3)

        for url in sel.xpath(\(aq//a/@href\(aq).extract():
            yield Request(url, callback=self.parse)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS CrawlSpider
.INDENT 0.0
.TP
.B class scrapy.contrib.spiders.CrawlSpider
This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it\(aqs generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.
.sp
Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:
.INDENT 7.0
.TP
.B rules
Which is a list of one (or more) \fBRule\fP objects.  Each \fBRule\fP
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they\(aqre defined in this attribute.
.UNINDENT
.sp
This spider also exposes an overrideable method:
.INDENT 7.0
.TP
.B parse_start_url(response)
This method is called for the start_urls responses. It allows to parse
the initial responses and must return either a
\fBItem\fP object, a \fBRequest\fP
object, or an iterable containing any of them.
.UNINDENT
.UNINDENT
.SS Crawling rules
.INDENT 0.0
.TP
.B class scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
\fBlink_extractor\fP is a \fILink Extractor\fP object which
defines how links will be extracted from each crawled page.
.sp
\fBcallback\fP is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link_extractor. This callback receives a response as its first
argument and must return a list containing \fBItem\fP and/or
\fBRequest\fP objects (or any subclass of them).
.sp
\fBWARNING:\fP
.INDENT 7.0
.INDENT 3.5
When writing crawl spider rules, avoid using \fBparse\fP as
callback, since the \fBCrawlSpider\fP uses the \fBparse\fP method
itself to implement its logic. So if you override the \fBparse\fP method,
the crawl spider will no longer work.
.UNINDENT
.UNINDENT
.sp
\fBcb_kwargs\fP is a dict containing the keyword arguments to be passed to the
callback function
.sp
\fBfollow\fP is a boolean which specifies if links should be followed from each
response extracted with this rule. If \fBcallback\fP is None \fBfollow\fP defaults
to \fBTrue\fP, otherwise it default to \fBFalse\fP\&.
.sp
\fBprocess_links\fP is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified \fBlink_extractor\fP\&.
This is mainly used for filtering purposes.
.sp
\fBprocess_request\fP is a callable, or a string (in which case a method from
the spider object with that name will be used) which will be called with
every request extracted by this rule, and must return a request or None (to
filter out the request).
.UNINDENT
.SS CrawlSpider example
.sp
Let\(aqs now take a look at an example CrawlSpider with rules:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
from scrapy.item import Item

class MySpider(CrawlSpider):
    name = \(aqexample.com\(aq
    allowed_domains = [\(aqexample.com\(aq]
    start_urls = [\(aqhttp://www.example.com\(aq]

    rules = (
        # Extract links matching \(aqcategory.php\(aq (but not matching \(aqsubsection.php\(aq)
        # and follow links from them (since no callback means follow=True by default).
        Rule(SgmlLinkExtractor(allow=(\(aqcategory\e.php\(aq, ), deny=(\(aqsubsection\e.php\(aq, ))),

        # Extract links matching \(aqitem.php\(aq and parse them with the spider\(aqs method parse_item
        Rule(SgmlLinkExtractor(allow=(\(aqitem\e.php\(aq, )), callback=\(aqparse_item\(aq),
    )

    def parse_item(self, response):
        self.log(\(aqHi, this is an item page! %s\(aq % response.url)

        sel = Selector(response)
        item = Item()
        item[\(aqid\(aq] = sel.xpath(\(aq//td[@id="item_id"]/text()\(aq).re(r\(aqID: (\ed+)\(aq)
        item[\(aqname\(aq] = sel.xpath(\(aq//td[@id="item_name"]/text()\(aq).extract()
        item[\(aqdescription\(aq] = sel.xpath(\(aq//td[@id="item_description"]/text()\(aq).extract()
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This spider would start crawling example.com\(aqs home page, collecting category
links, and item links, parsing the latter with the \fBparse_item\fP method. For
each item response, some data will be extracted from the HTML using XPath, and
a \fBItem\fP will be filled with it.
.SS XMLFeedSpider
.INDENT 0.0
.TP
.B class scrapy.contrib.spiders.XMLFeedSpider
XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: \fBiternodes\fP, \fBxml\fP,
and \fBhtml\fP\&.  It\(aqs recommended to use the \fBiternodes\fP iterator for
performance reasons, since the \fBxml\fP and \fBhtml\fP iterators generate the
whole DOM at once in order to parse it.  However, using \fBhtml\fP as the
iterator may be useful when parsing XML with bad markup.
.sp
To set the iterator and the tag name, you must define the following class
attributes:
.INDENT 7.0
.TP
.B iterator
A string which defines the iterator to use. It can be either:
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fB\(aqiternodes\(aq\fP \- a fast iterator based on regular expressions
.IP \(bu 2
\fB\(aqhtml\(aq\fP \- an iterator which uses \fBSelector\fP\&.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds
.IP \(bu 2
\fB\(aqxml\(aq\fP \- an iterator which uses \fBSelector\fP\&.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds
.UNINDENT
.UNINDENT
.UNINDENT
.sp
It defaults to: \fB\(aqiternodes\(aq\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B itertag
A string with the name of the node (or element) to iterate in. Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
itertag = \(aqproduct\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B namespaces
A list of \fB(prefix, uri)\fP tuples which define the namespaces
available in that document that will be processed with this spider. The
\fBprefix\fP and \fBuri\fP will be used to automatically register
namespaces using the
\fBregister_namespace()\fP method.
.sp
You can then specify nodes with namespaces in the \fBitertag\fP
attribute.
.sp
Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
class YourSpider(XMLFeedSpider):

    namespaces = [(\(aqn\(aq, \(aqhttp://www.sitemaps.org/schemas/sitemap/0.9\(aq)]
    itertag = \(aqn:url\(aq
    # ...
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Apart from these new attributes, this spider has the following overrideable
methods too:
.INDENT 7.0
.TP
.B adapt_response(response)
A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).
.UNINDENT
.INDENT 7.0
.TP
.B parse_node(response, selector)
This method is called for the nodes matching the provided tag name
(\fBitertag\fP).  Receives the response and an
\fBSelector\fP for each node.  Overriding this
method is mandatory. Otherwise, you spider won\(aqt work.  This method
must return either a \fBItem\fP object, a
\fBRequest\fP object, or an iterable containing any of
them.
.UNINDENT
.INDENT 7.0
.TP
.B process_results(response, results)
This method is called for each result (item or request) returned by the
spider, and it\(aqs intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (Items or Requests).
.UNINDENT
.UNINDENT
.SS XMLFeedSpider example
.sp
These spiders are pretty easy to use, let\(aqs have a look at one example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import log
from scrapy.contrib.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = \(aqexample.com\(aq
    allowed_domains = [\(aqexample.com\(aq]
    start_urls = [\(aqhttp://www.example.com/feed.xml\(aq]
    iterator = \(aqiternodes\(aq # This is actually unnecessary, since it\(aqs the default value
    itertag = \(aqitem\(aq

    def parse_node(self, response, node):
        log.msg(\(aqHi, this is a <%s> node!: %s\(aq % (self.itertag, \(aq\(aq.join(node.extract())))

        item = Item()
        item[\(aqid\(aq] = node.xpath(\(aq@id\(aq).extract()
        item[\(aqname\(aq] = node.xpath(\(aqname\(aq).extract()
        item[\(aqdescription\(aq] = node.xpath(\(aqdescription\(aq).extract()
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Basically what we did up there was to create a spider that downloads a feed from
the given \fBstart_urls\fP, and then iterates through each of its \fBitem\fP tags,
prints them out, and stores some random data in an \fBItem\fP\&.
.SS CSVFeedSpider
.INDENT 0.0
.TP
.B class scrapy.contrib.spiders.CSVFeedSpider
This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is \fBparse_row()\fP\&.
.INDENT 7.0
.TP
.B delimiter
A string with the separator character for each field in the CSV file
Defaults to \fB\(aq,\(aq\fP (comma).
.UNINDENT
.INDENT 7.0
.TP
.B headers
A list of the rows contained in the file CSV feed which will be used to
extract fields from it.
.UNINDENT
.INDENT 7.0
.TP
.B parse_row(response, row)
Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override \fBadapt_response\fP and \fBprocess_results\fP methods
for pre\- and post\-processing purposes.
.UNINDENT
.UNINDENT
.SS CSVFeedSpider example
.sp
Let\(aqs see an example similar to the previous one, but using a
\fBCSVFeedSpider\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import log
from scrapy.contrib.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = \(aqexample.com\(aq
    allowed_domains = [\(aqexample.com\(aq]
    start_urls = [\(aqhttp://www.example.com/feed.csv\(aq]
    delimiter = \(aq;\(aq
    headers = [\(aqid\(aq, \(aqname\(aq, \(aqdescription\(aq]

    def parse_row(self, response, row):
        log.msg(\(aqHi, this is a row!: %r\(aq % row)

        item = TestItem()
        item[\(aqid\(aq] = row[\(aqid\(aq]
        item[\(aqname\(aq] = row[\(aqname\(aq]
        item[\(aqdescription\(aq] = row[\(aqdescription\(aq]
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.SS SitemapSpider
.INDENT 0.0
.TP
.B class scrapy.contrib.spiders.SitemapSpider
SitemapSpider allows you to crawl a site by discovering the URLs using
\fI\%Sitemaps\fP\&.
.sp
It supports nested sitemaps and discovering sitemap urls from
\fI\%robots.txt\fP\&.
.INDENT 7.0
.TP
.B sitemap_urls
A list of urls pointing to the sitemaps whose urls you want to crawl.
.sp
You can also point to a \fI\%robots.txt\fP and it will be parsed to extract
sitemap urls from it.
.UNINDENT
.INDENT 7.0
.TP
.B sitemap_rules
A list of tuples \fB(regex, callback)\fP where:
.INDENT 7.0
.IP \(bu 2
\fBregex\fP is a regular expression to match urls extracted from sitemaps.
\fBregex\fP can be either a str or a compiled regex object.
.IP \(bu 2
callback is the callback to use for processing the urls that match
the regular expression. \fBcallback\fP can be a string (indicating the
name of a spider method) or a callable.
.UNINDENT
.sp
For example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
sitemap_rules = [(\(aq/product/\(aq, \(aqparse_product\(aq)]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Rules are applied in order, and only the first one that matches will be
used.
.sp
If you omit this attribute, all urls found in sitemaps will be
processed with the \fBparse\fP callback.
.UNINDENT
.INDENT 7.0
.TP
.B sitemap_follow
A list of regexes of sitemap that should be followed. This is is only
for sites that use \fI\%Sitemap index files\fP that point to other sitemap
files.
.sp
By default, all sitemaps are followed.
.UNINDENT
.INDENT 7.0
.TP
.B sitemap_alternate_links
Specifies if alternate links for one \fBurl\fP should be followed. These
are links for the same website in another language passed within
the same \fBurl\fP block.
.sp
For example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
<url>
    <loc>http://example.com/</loc>
    <xhtml:link rel="alternate" hreflang="de" href="http://example.com/de"/>
</url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
With \fBsitemap_alternate_links\fP set, this would retrieve both URLs. With
\fBsitemap_alternate_links\fP disabled, only \fBhttp://example.com/\fP would be
retrieved.
.sp
Default is \fBsitemap_alternate_links\fP disabled.
.UNINDENT
.UNINDENT
.SS SitemapSpider examples
.sp
Simplest example: process all urls discovered through sitemaps using the
\fBparse\fP callback:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = [\(aqhttp://www.example.com/sitemap.xml\(aq]

    def parse(self, response):
        pass # ... scrape item here ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Process some urls with certain callback and other urls with a different
callback:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = [\(aqhttp://www.example.com/sitemap.xml\(aq]
    sitemap_rules = [
        (\(aq/product/\(aq, \(aqparse_product\(aq),
        (\(aq/category/\(aq, \(aqparse_category\(aq),
    ]

    def parse_product(self, response):
        pass # ... scrape product ...

    def parse_category(self, response):
        pass # ... scrape category ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Follow sitemaps defined in the \fI\%robots.txt\fP file and only follow sitemaps
whose url contains \fB/sitemap_shop\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = [\(aqhttp://www.example.com/robots.txt\(aq]
    sitemap_rules = [
        (\(aq/shop/\(aq, \(aqparse_shop\(aq),
    ]
    sitemap_follow = [\(aq/sitemap_shops\(aq]

    def parse_shop(self, response):
        pass # ... scrape shop here ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Combine SitemapSpider with other sources of urls:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = [\(aqhttp://www.example.com/robots.txt\(aq]
    sitemap_rules = [
        (\(aq/shop/\(aq, \(aqparse_shop\(aq),
    ]

    other_urls = [\(aqhttp://www.example.com/about\(aq]

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [Request(x, callback=self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass # ... scrape shop here ...

    def parse_other(self, response):
        pass # ... scrape other here ...
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Selectors
.sp
When you\(aqre scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fI\%BeautifulSoup\fP is a very popular screen scraping library among Python
programmers which constructs a Python object based on the structure of the
HTML code and also deals with bad markup reasonably well, but it has one
drawback: it\(aqs slow.
.IP \(bu 2
\fI\%lxml\fP is a XML parsing library (which also parses HTML) with a pythonic
API based on \fI\%ElementTree\fP (which is not part of the Python standard
library).
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Scrapy comes with its own mechanism for extracting data. They\(aqre called
selectors because they "select" certain parts of the HTML document specified
either by \fI\%XPath\fP or \fI\%CSS\fP expressions.
.sp
\fI\%XPath\fP is a language for selecting nodes in XML documents, which can also be
used with HTML. \fI\%CSS\fP is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.
.sp
Scrapy selectors are built over the \fI\%lxml\fP library, which means they\(aqre very
similar in speed and parsing accuracy.
.sp
This page explains how selectors work and describes their API which is very
small and simple, unlike the \fI\%lxml\fP API which is much bigger because the
\fI\%lxml\fP library can be used for many other tasks, besides selecting markup
documents.
.sp
For a complete reference of the selectors API see
\fISelector reference\fP
.SS Using selectors
.SS Constructing selectors
.sp
Scrapy selectors are instances of \fBSelector\fP class
constructed by passing a \fIResponse\fP object as first argument, the response\(aqs
body is what they\(aqre going to be "selecting":
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.spider import Spider
from scrapy.selector import Selector

class MySpider(Spider):
    # ...
    def parse(self, response):
        sel = Selector(response)
        # Using XPath query
        print sel.xpath(\(aq//p\(aq)
        # Using CSS query
        print sel.css(\(aqp\(aq)
        # Nesting queries
        print sel.xpath(\(aq//div[@foo="bar"]\(aq).css(\(aqspan#bold\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using selectors
.sp
To explain how to use the selectors we\(aqll use the \fIScrapy shell\fP (which
provides interactive testing) and an example page located in the Scrapy
documentation server:
.INDENT 0.0
.INDENT 3.5
\fI\%http://doc.scrapy.org/en/latest/_static/selectors\-sample1.html\fP
.UNINDENT
.UNINDENT
.sp
Here\(aqs its HTML code:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<html>
 <head>
  <base href=\(aqhttp://example.com/\(aq />
  <title>Example website</title>
 </head>
 <body>
  <div id=\(aqimages\(aq>
   <a href=\(aqimage1.html\(aq>Name: My image 1 <br /><img src=\(aqimage1_thumb.jpg\(aq /></a>
   <a href=\(aqimage2.html\(aq>Name: My image 2 <br /><img src=\(aqimage2_thumb.jpg\(aq /></a>
   <a href=\(aqimage3.html\(aq>Name: My image 3 <br /><img src=\(aqimage3_thumb.jpg\(aq /></a>
   <a href=\(aqimage4.html\(aq>Name: My image 4 <br /><img src=\(aqimage4_thumb.jpg\(aq /></a>
   <a href=\(aqimage5.html\(aq>Name: My image 5 <br /><img src=\(aqimage5_thumb.jpg\(aq /></a>
  </div>
 </body>
</html>


.ft P
.fi
.UNINDENT
.UNINDENT
.sp
First, let\(aqs open the shell:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy shell http://doc.scrapy.org/en/latest/_static/selectors\-sample1.html
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then, after the shell loads, you\(aqll have a selector already instantiated and
ready to use in \fBsel\fP shell variable.
.sp
Since we\(aqre dealing with HTML, the selector will automatically use an HTML parser.
.sp
So, by looking at the \fIHTML code\fP of that
page, let\(aqs construct an XPath (using an HTML selector) for selecting the text
inside the title tag:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath(\(aq//title/text()\(aq)
[<Selector (text) xpath=//title/text()>]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As you can see, the \fB\&.xpath()\fP method returns an
\fBSelectorList\fP instance, which is a list of new
selectors. This API can be used quickly for extracting nested data.
.sp
To actually extract the textual data, you must call the selector \fB\&.extract()\fP
method, as follows:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath(\(aq//title/text()\(aq).extract()
[u\(aqExample website\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo\-elements:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.css(\(aqtitle::text\(aq).extract()
[u\(aqExample website\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Now we\(aqre going to get the base URL and some image links:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath(\(aq//base/@href\(aq).extract()
[u\(aqhttp://example.com/\(aq]

>>> sel.css(\(aqbase::attr(href)\(aq).extract()
[u\(aqhttp://example.com/\(aq]

>>> sel.xpath(\(aq//a[contains(@href, "image")]/@href\(aq).extract()
[u\(aqimage1.html\(aq,
 u\(aqimage2.html\(aq,
 u\(aqimage3.html\(aq,
 u\(aqimage4.html\(aq,
 u\(aqimage5.html\(aq]

>>> sel.css(\(aqa[href*=image]::attr(href)\(aq).extract()
[u\(aqimage1.html\(aq,
 u\(aqimage2.html\(aq,
 u\(aqimage3.html\(aq,
 u\(aqimage4.html\(aq,
 u\(aqimage5.html\(aq]

>>> sel.xpath(\(aq//a[contains(@href, "image")]/img/@src\(aq).extract()
[u\(aqimage1_thumb.jpg\(aq,
 u\(aqimage2_thumb.jpg\(aq,
 u\(aqimage3_thumb.jpg\(aq,
 u\(aqimage4_thumb.jpg\(aq,
 u\(aqimage5_thumb.jpg\(aq]

>>> sel.css(\(aqa[href*=image] img::attr(src)\(aq).extract()
[u\(aqimage1_thumb.jpg\(aq,
 u\(aqimage2_thumb.jpg\(aq,
 u\(aqimage3_thumb.jpg\(aq,
 u\(aqimage4_thumb.jpg\(aq,
 u\(aqimage5_thumb.jpg\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Nesting selectors
.sp
The selection methods (\fB\&.xpath()\fP or \fB\&.css()\fP) returns a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here\(aqs an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> links = sel.xpath(\(aq//a[contains(@href, "image")]\(aq)
>>> links.extract()
[u\(aq<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg"></a>\(aq,
 u\(aq<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg"></a>\(aq,
 u\(aq<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg"></a>\(aq,
 u\(aq<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg"></a>\(aq,
 u\(aq<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg"></a>\(aq]

>>> for index, link in enumerate(links):
        args = (index, link.xpath(\(aq@href\(aq).extract(), link.xpath(\(aqimg/@src\(aq).extract())
        print \(aqLink number %d points to url %s and image %s\(aq % args

Link number 0 points to url [u\(aqimage1.html\(aq] and image [u\(aqimage1_thumb.jpg\(aq]
Link number 1 points to url [u\(aqimage2.html\(aq] and image [u\(aqimage2_thumb.jpg\(aq]
Link number 2 points to url [u\(aqimage3.html\(aq] and image [u\(aqimage3_thumb.jpg\(aq]
Link number 3 points to url [u\(aqimage4.html\(aq] and image [u\(aqimage4_thumb.jpg\(aq]
Link number 4 points to url [u\(aqimage5.html\(aq] and image [u\(aqimage5_thumb.jpg\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using selectors with regular expressions
.sp
\fBSelector\fP also have a \fB\&.re()\fP method for extracting
data using regular expressions. However, unlike using \fB\&.xpath()\fP or
\fB\&.css()\fP methods, \fB\&.re()\fP method returns a list of unicode strings. So you
can\(aqt construct nested \fB\&.re()\fP calls.
.sp
Here\(aqs an example used to extract images names from the \fIHTML code\fP above:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath(\(aq//a[contains(@href, "image")]/text()\(aq).re(r\(aqName:\es*(.*)\(aq)
[u\(aqMy image 1\(aq,
 u\(aqMy image 2\(aq,
 u\(aqMy image 3\(aq,
 u\(aqMy image 4\(aq,
 u\(aqMy image 5\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Working with relative XPaths
.sp
Keep in mind that if you are nesting selectors and use an XPath that starts
with \fB/\fP, that XPath will be absolute to the document and not relative to the
\fBSelector\fP you\(aqre calling it from.
.sp
For example, suppose you want to extract all \fB<p>\fP elements inside \fB<div>\fP
elements. First, you would get all \fB<div>\fP elements:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> divs = sel.xpath(\(aq//div\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all \fB<p>\fP elements from the document, not only those
inside \fB<div>\fP elements:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> for p in divs.xpath(\(aq//p\(aq)  # this is wrong \- gets all <p> from the whole document
>>>     print p.extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This is the proper way to do it (note the dot prefixing the \fB\&.//p\fP XPath):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> for p in divs.xpath(\(aq.//p\(aq)  # extracts all <p> inside
>>>     print p.extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Another common case would be to extract all direct \fB<p>\fP children:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> for p in divs.xpath(\(aqp\(aq)
>>>     print p.extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For more details about relative XPaths see the \fI\%Location Paths\fP section in the
XPath specification.
.SS Built\-in Selectors reference
.INDENT 0.0
.TP
.B class scrapy.selector.Selector(response=None, text=None, type=None)
An instance of \fBSelector\fP is a wrapper over response to select
certain parts of its content.
.sp
\fBresponse\fP is a \fBHtmlResponse\fP or
\fBXmlResponse\fP object that will be used for selecting and
extracting data.
.sp
\fBtext\fP is a unicode string or utf\-8 encoded text for cases when a
\fBresponse\fP isn\(aqt available. Using \fBtext\fP and \fBresponse\fP together is
undefined behavior.
.sp
\fBtype\fP defines the selector type, it can be \fB"html"\fP, \fB"xml"\fP or \fBNone\fP (default).
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.INDENT 3.5
If \fBtype\fP is \fBNone\fP, the selector automatically chooses the best type
based on \fBresponse\fP type (see below), or defaults to \fB"html"\fP in case it
is used together with \fBtext\fP\&.
.sp
If \fBtype\fP is \fBNone\fP and a \fBresponse\fP is passed, the selector type is
inferred from the response type as follow:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fB"html"\fP for \fBHtmlResponse\fP type
.IP \(bu 2
\fB"xml"\fP for \fBXmlResponse\fP type
.IP \(bu 2
\fB"html"\fP for anything else
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Otherwise, if \fBtype\fP is set, the selector type will be forced and no
detection will occur.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B xpath(query)
Find nodes matching the xpath \fBquery\fP and return the result as a
\fBSelectorList\fP instance with all elements flattened. List
elements implement \fBSelector\fP interface too.
.sp
\fBquery\fP is a string containing the XPATH query to apply.
.UNINDENT
.INDENT 7.0
.TP
.B css(query)
Apply the given CSS selector and return a \fBSelectorList\fP instance.
.sp
\fBquery\fP is a string containing the CSS selector to apply.
.sp
In the background, CSS queries are translated into XPath queries using
\fI\%cssselect\fP library and run \fB\&.xpath()\fP method.
.UNINDENT
.INDENT 7.0
.TP
.B extract()
Serialize and return the matched nodes as a list of unicode strings.
Percent encoded content is unquoted.
.UNINDENT
.INDENT 7.0
.TP
.B re(regex)
Apply the given regex and return a list of unicode strings with the
matches.
.sp
\fBregex\fP can be either a compiled regular expression or a string which
will be compiled to a regular expression using \fBre.compile(regex)\fP
.UNINDENT
.INDENT 7.0
.TP
.B register_namespace(prefix, uri)
Register the given namespace to be used in this \fBSelector\fP\&.
Without registering namespaces you can\(aqt select or extract data from
non\-standard namespaces. See examples below.
.UNINDENT
.INDENT 7.0
.TP
.B remove_namespaces()
Remove all namespaces, allowing to traverse the document using
namespace\-less xpaths. See example below.
.UNINDENT
.INDENT 7.0
.TP
.B __nonzero__()
Returns \fBTrue\fP if there is any real content selected or \fBFalse\fP
otherwise.  In other words, the boolean value of a \fBSelector\fP is
given by the contents it selects.
.UNINDENT
.UNINDENT
.SS SelectorList objects
.INDENT 0.0
.TP
.B class scrapy.selector.SelectorList
The \fBSelectorList\fP class is subclass of the builtin \fBlist\fP
class, which provides a few additional methods.
.INDENT 7.0
.TP
.B xpath(query)
Call the \fB\&.xpath()\fP method for each element in this list and return
their results flattened as another \fBSelectorList\fP\&.
.sp
\fBquery\fP is the same argument as the one in \fBSelector.xpath()\fP
.UNINDENT
.INDENT 7.0
.TP
.B css(query)
Call the \fB\&.css()\fP method for each element in this list and return
their results flattened as another \fBSelectorList\fP\&.
.sp
\fBquery\fP is the same argument as the one in \fBSelector.css()\fP
.UNINDENT
.INDENT 7.0
.TP
.B extract()
Call the \fB\&.extract()\fP method for each element is this list and return
their results flattened, as a list of unicode strings.
.UNINDENT
.INDENT 7.0
.TP
.B re()
Call the \fB\&.re()\fP method for each element is this list and return
their results flattened, as a list of unicode strings.
.UNINDENT
.INDENT 7.0
.TP
.B __nonzero__()
returns True if the list is not empty, False otherwise.
.UNINDENT
.UNINDENT
.SS Selector examples on HTML response
.sp
Here\(aqs a couple of \fBSelector\fP examples to illustrate several concepts.
In all cases, we assume there is already an \fBSelector\fP instantiated with
a \fBHtmlResponse\fP object like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel = Selector(html_response)
.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.IP 1. 3
Select all \fB<h1>\fP elements from a HTML response body, returning a list of
\fBSelector\fP objects (ie. a \fBSelectorList\fP object):
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath("//h1")
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 2. 3
Extract the text of all \fB<h1>\fP elements from a HTML response body,
returning a list of unicode strings:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath("//h1").extract()         # this includes the h1 tag
sel.xpath("//h1/text()").extract()  # this excludes the h1 tag
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 3. 3
Iterate over all \fB<p>\fP tags and print their class attribute:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
for node in sel.xpath("//p"):
\&...    print node.xpath("@class").extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS Selector examples on XML response
.sp
Here\(aqs a couple of examples to illustrate several concepts. In both cases we
assume there is already an \fBSelector\fP instantiated with a
\fBXmlResponse\fP object like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
sel = Selector(xml_response)
.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.IP 1. 3
Select all \fB<product>\fP elements from a XML response body, returning a list
of \fBSelector\fP objects (ie. a \fBSelectorList\fP object):
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sel.xpath("//product")
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 2. 3
Extract all prices from a \fI\%Google Base XML feed\fP which requires registering
a namespace:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sel.register_namespace("g", "http://base.google.com/ns/1.0")
sel.xpath("//g:price").extract()
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS Removing namespaces
.sp
When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
\fBSelector.remove_namespaces()\fP method for that.
.sp
Let\(aqs show an example that illustrates this with Github blog atom feed.
.sp
First, we open the shell with the url we want to scrape:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy shell https://github.com/blog.atom
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Once in the shell we can try selecting all \fB<link>\fP objects and see that it
doesn\(aqt work (because the Atom XML namespace is obfuscating those nodes):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath("//link")
[]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
But once we call the \fBSelector.remove_namespaces()\fP method, all
nodes can be accessed directly by their names:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.remove_namespaces()
>>> sel.xpath("//link")
[<Selector xpath=\(aq//link\(aq data=u\(aq<link xmlns="http://www.w3.org/2005/Atom\(aq>,
 <Selector xpath=\(aq//link\(aq data=u\(aq<link xmlns="http://www.w3.org/2005/Atom\(aq>,
 ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
If you wonder why the namespace removal procedure is not always called, instead
of having to call it manually. This is because of two reasons which, in order
of relevance, are:
.INDENT 0.0
.IP 1. 3
Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to performs for all
documents crawled by Scrapy
.IP 2. 3
There could be some cases where using namespaces is actually required, in
case some element names clash between namespaces. These cases are very rare
though.
.UNINDENT
.SS Item Loaders
.sp
Item Loaders provide a convenient mechanism for populating scraped \fIItems\fP\&. Even though Items can be populated using their own
dictionary\-like API, the Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.
.sp
In other words, \fIItems\fP provide the \fIcontainer\fP of
scraped data, while Item Loaders provide the mechanism for \fIpopulating\fP that
container.
.sp
Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.
.SS Using Item Loaders to populate items
.sp
To use an Item Loader, you must first instantiate it. You can either
instantiate it with an dict\-like object (e.g. Item or dict) or without one, in
which case an Item is automatically instantiated in the Item Loader constructor
using the Item class specified in the \fBItemLoader.default_item_class\fP
attribute.
.sp
Then, you start collecting values into the Item Loader, typically using
\fISelectors\fP\&. You can add more than one value to
the same item field; the Item Loader will know how to "join" those values later
using a proper processing function.
.sp
Here is a typical Item Loader usage in a \fISpider\fP, using
the \fIProduct item\fP declared in the \fIItems
chapter\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.loader import ItemLoader
from myproject.items import Product

def parse(self, response):
    l = ItemLoader(item=Product(), response=response)
    l.add_xpath(\(aqname\(aq, \(aq//div[@class="product_name"]\(aq)
    l.add_xpath(\(aqname\(aq, \(aq//div[@class="product_title"]\(aq)
    l.add_xpath(\(aqprice\(aq, \(aq//p[@id="price"]\(aq)
    l.add_css(\(aqstock\(aq, \(aqp#stock]\(aq)
    l.add_value(\(aqlast_updated\(aq, \(aqtoday\(aq) # you can also use literal values
    return l.load_item()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
By quickly looking at that code, we can see the \fBname\fP field is being
extracted from two different XPath locations in the page:
.INDENT 0.0
.IP 1. 3
\fB//div[@class="product_name"]\fP
.IP 2. 3
\fB//div[@class="product_title"]\fP
.UNINDENT
.sp
In other words, data is being collected by extracting it from two XPath
locations, using the \fBadd_xpath()\fP method. This is the
data that will be assigned to the \fBname\fP field later.
.sp
Afterwords, similar calls are used for \fBprice\fP and \fBstock\fP fields
(the later using a CSS selector with the \fBadd_css()\fP method),
and finally the \fBlast_update\fP field is populated directly with a literal value
(\fBtoday\fP) using a different method: \fBadd_value()\fP\&.
.sp
Finally, when all data is collected, the \fBItemLoader.load_item()\fP method is
called which actually populates and returns the item populated with the data
previously extracted and collected with the \fBadd_xpath()\fP,
\fBadd_css()\fP, and \fBadd_value()\fP calls.
.SS Input and Output processors
.sp
An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it\(aqs
received (through the \fBadd_xpath()\fP, \fBadd_css()\fP or
\fBadd_value()\fP methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
\fBItemLoader.load_item()\fP method is called to populate and get the populated
\fBItem\fP object.  That\(aqs when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.
.sp
Let\(aqs see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
l = ItemLoader(Product(), some_selector)
l.add_xpath(\(aqname\(aq, xpath1) # (1)
l.add_xpath(\(aqname\(aq, xpath2) # (2)
l.add_css(\(aqname\(aq, css) # (3)
l.add_value(\(aqname\(aq, \(aqtest\(aq) # (4)
return l.load_item() # (5)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
So what happens is:
.INDENT 0.0
.IP 1. 3
Data from \fBxpath1\fP is extracted, and passed through the \fIinput processor\fP of
the \fBname\fP field. The result of the input processor is collected and kept in
the Item Loader (but not yet assigned to the item).
.IP 2. 3
Data from \fBxpath2\fP is extracted, and passed through the same \fIinput
processor\fP used in (1). The result of the input processor is appended to the
data collected in (1) (if any).
.IP 3. 3
This case is similar to the previous ones, except that the data is extracted
from the \fBcss\fP CSS selector, and passed through the same \fIinput
processor\fP used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).
.IP 4. 3
This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.
.IP 5. 3
The data collected in steps (1), (2), (3) and (4) is passed through
the \fIoutput processor\fP of the \fBname\fP field.
The result of the output processor is the value assigned to the \fBname\fP
field in the item.
.UNINDENT
.sp
It\(aqs worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.
.UNINDENT
.UNINDENT
.sp
The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.
.sp
Last, but not least, Scrapy comes with some \fIcommonly used processors\fP built\-in for convenience.
.SS Declaring Item Loaders
.sp
Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.loader import ItemLoader
from scrapy.contrib.loader.processor import TakeFirst, MapCompose, Join

class ProductLoader(ItemLoader):

    default_output_processor = TakeFirst()

    name_in = MapCompose(unicode.title)
    name_out = Join()

    price_in = MapCompose(unicode.strip)

    # ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As you can see, input processors are declared using the \fB_in\fP suffix while
output processors are declared using the \fB_out\fP suffix. And you can also
declare a default input/output processors using the
\fBItemLoader.default_input_processor\fP and
\fBItemLoader.default_output_processor\fP attributes.
.SS Declaring Input and Output Processors
.sp
As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it\(aqs very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the \fIItem Field\fP
metadata. Here is an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item, Field
from scrapy.contrib.loader.processor import MapCompose, Join, TakeFirst

from scrapy.utils.markup import remove_entities
from myproject.utils import filter_prices

class Product(Item):
    name = Field(
        input_processor=MapCompose(remove_entities),
        output_processor=Join(),
    )
    price = Field(
        default=0,
        input_processor=MapCompose(remove_entities, filter_prices),
        output_processor=TakeFirst(),
    )
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The precedence order, for both input and output processors, is as follows:
.INDENT 0.0
.IP 1. 3
Item Loader field\-specific attributes: \fBfield_in\fP and \fBfield_out\fP (most
precedence)
.IP 2. 3
Field metadata (\fBinput_processor\fP and \fBoutput_processor\fP key)
.IP 3. 3
Item Loader defaults: \fBItemLoader.default_input_processor()\fP and
\fBItemLoader.default_output_processor()\fP (least precedence)
.UNINDENT
.sp
See also: \fItopics\-loaders\-extending\fP\&.
.SS Item Loader Context
.sp
The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.
.sp
For example, suppose you have a function \fBparse_length\fP which receives a text
value and extracts a length from it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_length(text, loader_context):
    unit = loader_context.get(\(aqunit\(aq, \(aqm\(aq)
    # ... length parsing code goes here ...
    return parsed_length
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
By accepting a \fBloader_context\fP argument the function is explicitly telling
the Item Loader that is able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (\fBparse_length\fP in this case) can thus use them.
.sp
There are several ways to modify Item Loader context values:
.INDENT 0.0
.IP 1. 3
By modifying the currently active Item Loader context
(\fBcontext\fP attribute):
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
loader = ItemLoader(product)
loader.context[\(aqunit\(aq] = \(aqcm\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 2. 3
On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
loader = ItemLoader(product, unit=\(aqcm\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 3. 3
On Item Loader declaration, for those input/output processors that support
instatiating them with a Item Loader context. \fBMapCompose\fP is one of
them:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
class ProductLoader(ItemLoader):
    length_out = MapCompose(parse_length, unit=\(aqcm\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS ItemLoader objects
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.ItemLoader([item, selector, response], **kwargs)
Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
\fBdefault_item_class\fP\&.
.sp
When instantiated with a \fIselector\fP or a \fIresponse\fP parameters
the \fBItemLoader\fP class provides convenient mechanisms for extracting
data from web pages using \fIselectors\fP\&.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBitem\fP (\fBItem\fP object) \-\- The item instance to populate using subsequent calls to
\fBadd_xpath()\fP, \fBadd_css()\fP,
or \fBadd_value()\fP\&.
.IP \(bu 2
\fBselector\fP (\fBSelector\fP object) \-\- The selector to extract data from, when using the
\fBadd_xpath()\fP (resp. \fBadd_css()\fP) or \fBreplace_xpath()\fP
(resp. \fBreplace_css()\fP) method.
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- The response used to construct the selector using the
\fBdefault_selector_class\fP, unless the selector argument is given,
in which case this argument is ignored.
.UNINDENT
.UNINDENT
.sp
The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the \fBcontext\fP attribute).
.sp
\fBItemLoader\fP instances have the following methods:
.INDENT 7.0
.TP
.B get_value(value, *processors, **kwargs)
Process the given \fBvalue\fP by the given \fBprocessors\fP and keyword
arguments.
.sp
Available keyword arguments:
.INDENT 7.0
.TP
.B Parameters
\fBre\fP (\fIstr or compiled regex\fP) \-\- a regular expression to use for extracting data from the
given value using \fBextract_regex()\fP method,
applied before processors
.UNINDENT
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.contrib.loader.processor import TakeFirst
>>> loader.get_value(u\(aqname: foo\(aq, TakeFirst(), unicode.upper, re=\(aqname: (.+)\(aq)
\(aqFOO\(ga
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B add_value(field_name, value, *processors, **kwargs)
Process and then add the given \fBvalue\fP for the given field.
.sp
The value is first passed through \fBget_value()\fP by giving the
\fBprocessors\fP and \fBkwargs\fP, and then passed through the
\fIfield input processor\fP and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.
.sp
The given \fBfield_name\fP can be \fBNone\fP, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
loader.add_value(\(aqname\(aq, u\(aqColor TV\(aq)
loader.add_value(\(aqcolours\(aq, [u\(aqwhite\(aq, u\(aqblue\(aq])
loader.add_value(\(aqlength\(aq, u\(aq100\(aq)
loader.add_value(\(aqname\(aq, u\(aqname: foo\(aq, TakeFirst(), re=\(aqname: (.+)\(aq)
loader.add_value(None, {\(aqname\(aq: u\(aqfoo\(aq, \(aqsex\(aq: u\(aqmale\(aq})
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B replace_value(field_name, value)
Similar to \fBadd_value()\fP but replaces the collected data with the
new value instead of adding it.
.UNINDENT
.INDENT 7.0
.TP
.B get_xpath(xpath, *processors, **kwargs)
Similar to \fBItemLoader.get_value()\fP but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this \fBItemLoader\fP\&.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBxpath\fP (\fIstr\fP) \-\- the XPath to extract data from
.IP \(bu 2
\fBre\fP (\fIstr or compiled regex\fP) \-\- a regular expression to use for extracting data from the
selected XPath region
.UNINDENT
.UNINDENT
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
# HTML snippet: <p class="product\-name">Color TV</p>
loader.get_xpath(\(aq//p[@class="product\-name"]\(aq)
# HTML snippet: <p id="price">the price is $1200</p>
loader.get_xpath(\(aq//p[@id="price"]\(aq, TakeFirst(), re=\(aqthe price is (.*)\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B add_xpath(field_name, xpath, *processors, **kwargs)
Similar to \fBItemLoader.add_value()\fP but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this \fBItemLoader\fP\&.
.sp
See \fBget_xpath()\fP for \fBkwargs\fP\&.
.INDENT 7.0
.TP
.B Parameters
\fBxpath\fP (\fIstr\fP) \-\- the XPath to extract data from
.UNINDENT
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
# HTML snippet: <p class="product\-name">Color TV</p>
loader.add_xpath(\(aqname\(aq, \(aq//p[@class="product\-name"]\(aq)
# HTML snippet: <p id="price">the price is $1200</p>
loader.add_xpath(\(aqprice\(aq, \(aq//p[@id="price"]\(aq, re=\(aqthe price is (.*)\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B replace_xpath(field_name, xpath, *processors, **kwargs)
Similar to \fBadd_xpath()\fP but replaces collected data instead of
adding it.
.UNINDENT
.INDENT 7.0
.TP
.B get_css(css, *processors, **kwargs)
Similar to \fBItemLoader.get_value()\fP but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this \fBItemLoader\fP\&.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBcss\fP (\fIstr\fP) \-\- the CSS selector to extract data from
.IP \(bu 2
\fBre\fP (\fIstr or compiled regex\fP) \-\- a regular expression to use for extracting data from the
selected CSS region
.UNINDENT
.UNINDENT
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
# HTML snippet: <p class="product\-name">Color TV</p>
loader.get_css(\(aqp.product\-name\(aq)
# HTML snippet: <p id="price">the price is $1200</p>
loader.get_css(\(aqp#price\(aq, TakeFirst(), re=\(aqthe price is (.*)\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B add_css(field_name, css, *processors, **kwargs)
Similar to \fBItemLoader.add_value()\fP but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this \fBItemLoader\fP\&.
.sp
See \fBget_css()\fP for \fBkwargs\fP\&.
.INDENT 7.0
.TP
.B Parameters
\fBcss\fP (\fIstr\fP) \-\- the CSS selector to extract data from
.UNINDENT
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
# HTML snippet: <p class="product\-name">Color TV</p>
loader.add_css(\(aqname\(aq, \(aqp.product\-name\(aq)
# HTML snippet: <p id="price">the price is $1200</p>
loader.add_css(\(aqprice\(aq, \(aqp#price\(aq, re=\(aqthe price is (.*)\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B replace_css(field_name, css, *processors, **kwargs)
Similar to \fBadd_css()\fP but replaces collected data instead of
adding it.
.UNINDENT
.INDENT 7.0
.TP
.B load_item()
Populate the item with the data collected so far, and return it. The
data collected is first passed through the \fIoutput processors\fP to get the final value to assign to each
item field.
.UNINDENT
.INDENT 7.0
.TP
.B get_collected_values(field_name)
Return the collected values for the given field.
.UNINDENT
.INDENT 7.0
.TP
.B get_output_value(field_name)
Return the collected values parsed using the output processor, for the
given field. This method doesn\(aqt populate or modify the item at all.
.UNINDENT
.INDENT 7.0
.TP
.B get_input_processor(field_name)
Return the input processor for the given field.
.UNINDENT
.INDENT 7.0
.TP
.B get_output_processor(field_name)
Return the output processor for the given field.
.UNINDENT
.sp
\fBItemLoader\fP instances have the following attributes:
.INDENT 7.0
.TP
.B item
The \fBItem\fP object being parsed by this Item Loader.
.UNINDENT
.INDENT 7.0
.TP
.B context
The currently active \fIContext\fP of this
Item Loader.
.UNINDENT
.INDENT 7.0
.TP
.B default_item_class
An Item class (or factory), used to instantiate items when not given in
the constructor.
.UNINDENT
.INDENT 7.0
.TP
.B default_input_processor
The default input processor to use for those fields which don\(aqt specify
one.
.UNINDENT
.INDENT 7.0
.TP
.B default_output_processor
The default output processor to use for those fields which don\(aqt specify
one.
.UNINDENT
.INDENT 7.0
.TP
.B default_selector_class
The class used to construct the \fBselector\fP of this
\fBItemLoader\fP, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.
.UNINDENT
.INDENT 7.0
.TP
.B selector
The \fBSelector\fP object to extract data from.
It\(aqs either the selector given in the constructor or one created from
the response given in the constructor using the
\fBdefault_selector_class\fP\&. This attribute is meant to be
read\-only.
.UNINDENT
.UNINDENT
.SS Reusing and extending Item Loaders
.sp
As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, specially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.
.sp
Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).
.sp
Suppose, for example, that some particular site encloses their product names in
three dashes (ie. \fB\-\-\-Plasma TV\-\-\-\fP) and you don\(aqt want to end up scraping
those dashes in the final product names.
.sp
Here\(aqs how you can remove those dashes by reusing and extending the default
Product Item Loader (\fBProductLoader\fP):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.loader.processor import MapCompose
from myproject.ItemLoaders import ProductLoader

def strip_dashes(x):
    return x.strip(\(aq\-\(aq)

class SiteSpecificLoader(ProductLoader):
    name_in = MapCompose(strip_dashes, ProductLoader.name_in)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove \fBCDATA\fP occurrences. Here\(aqs an example of how to do it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.loader.processor import MapCompose
from myproject.ItemLoaders import ProductLoader
from myproject.utils.xml import remove_cdata

class XmlProductLoader(ProductLoader):
    name_in = MapCompose(remove_cdata, ProductLoader.name_in)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And that\(aqs how you typically extend input processors.
.sp
As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
\fItopics\-loaders\-processors\-declaring\fP\&.
.sp
There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn\(aqt impose any specific
organization of your Loaders collection \- that\(aqs up to you and your project\(aqs
needs.
.SS Available built\-in processors
.sp
Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the \fBMapCompose\fP (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.
.sp
Here is a list of all built\-in processors:
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.processor.Identity
The simplest processor, which doesn\(aqt do anything. It returns the original
values unchanged. It doesn\(aqt receive any constructor arguments nor accepts
Loader contexts.
.sp
Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.contrib.loader.processor import Identity
>>> proc = Identity()
>>> proc([\(aqone\(aq, \(aqtwo\(aq, \(aqthree\(aq])
[\(aqone\(aq, \(aqtwo\(aq, \(aqthree\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.processor.TakeFirst
Return the first non\-null/non\-empty value from the values received,
so it\(aqs typically used as an output processor to single\-valued fields.
It doesn\(aqt receive any constructor arguments, nor accept Loader contexts.
.sp
Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.contrib.loader.processor import TakeFirst
>>> proc = TakeFirst()
>>> proc([\(aq\(aq, \(aqone\(aq, \(aqtwo\(aq, \(aqthree\(aq])
\(aqone\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.processor.Join(separator=u\(aq \(aq)
Returns the values joined with the separator given in the constructor, which
defaults to \fBu\(aq \(aq\fP\&. It doesn\(aqt accept Loader contexts.
.sp
When using the default separator, this processor is equivalent to the
function: \fBu\(aq \(aq.join\fP
.sp
Examples:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.contrib.loader.processor import Join
>>> proc = Join()
>>> proc([\(aqone\(aq, \(aqtwo\(aq, \(aqthree\(aq])
u\(aqone two three\(aq
>>> proc = Join(\(aq<br>\(aq)
>>> proc([\(aqone\(aq, \(aqtwo\(aq, \(aqthree\(aq])
u\(aqone<br>two<br>three\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.processor.Compose(*functions, **default_loader_context)
A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.
.sp
By default, stop process on None value. This behaviour can be changed by
passing keyword argument stop_on_none=False.
.sp
Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.contrib.loader.processor import Compose
>>> proc = Compose(lambda v: v[0], str.upper)
>>> proc([\(aqhello\(aq, \(aqworld\(aq])
\(aqHELLO\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Each function can optionally receive a \fBloader_context\fP parameter. For
those which do, this processor will pass the currently active \fILoader
context\fP through that parameter.
.sp
The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the \fBItemLoader.context()\fP
attribute.
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contrib.loader.processor.MapCompose(*functions, **default_loader_context)
A processor which is constructed from the composition of the given
functions, similar to the \fBCompose\fP processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:
.sp
The input value of this processor is \fIiterated\fP and each element is passed
to the first function, and the result of that function (for each element)
is concatenated to construct a new iterable, which is then passed to the
second function, and so on, until the last function is applied for each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.
.sp
Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return \fBNone\fP in which
case the output of that function is ignored for further processing over the
chain.
.sp
This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
\fBMapCompose\fP processor is typically used as input processor, since
data is often extracted using the
\fBextract()\fP method of \fIselectors\fP, which returns a list of unicode strings.
.sp
The example below should clarify how it works:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
>>> def filter_world(x):
\&...     return None if x == \(aqworld\(aq else x
\&...
>>> from scrapy.contrib.loader.processor import MapCompose
>>> proc = MapCompose(filter_world, unicode.upper)
>>> proc([u\(aqhello\(aq, u\(aqworld\(aq, u\(aqthis\(aq, u\(aqis\(aq, u\(aqscrapy\(aq])
[u\(aqHELLO, u\(aqTHIS\(aq, u\(aqIS\(aq, u\(aqSCRAPY\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
\fBCompose\fP processor for more info.
.UNINDENT
.SS Scrapy shell
.sp
The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It\(aqs meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.
.sp
The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you\(aqre trying to scrape. It
allows you to interactively test your expressions while you\(aqre writing your
spider, without having to run the spider to test every change.
.sp
Once you get familiarized with the Scrapy shell, you\(aqll see that it\(aqs an
invaluable tool for developing and debugging your spiders.
.sp
If you have \fI\%IPython\fP installed, the Scrapy shell will use it (instead of the
standard Python console). The \fI\%IPython\fP console is much more powerful and
provides smart auto\-completion and colorized output, among other things.
.sp
We highly recommend you install \fI\%IPython\fP, specially if you\(aqre working on
Unix systems (where \fI\%IPython\fP excels). See the \fI\%IPython installation guide\fP
for more info.
.SS Launch the shell
.sp
To launch the Scrapy shell you can use the \fBshell\fP command like
this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy shell <url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Where the \fB<url>\fP is the URL you want to scrape.
.SS Using the shell
.sp
The Scrapy shell is just a regular Python console (or \fI\%IPython\fP console if you
have it available) which provides some additional shortcut functions for
convenience.
.SS Available Shortcuts
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBshelp()\fP \- print a help with the list of available objects and shortcuts
.IP \(bu 2
\fBfetch(request_or_url)\fP \- fetch a new response from the given request or
URL and update all related objects accordingly.
.IP \(bu 2
\fBview(response)\fP \- open the given response in your local web browser, for
inspection. This will add a \fI\%<base> tag\fP to the response body in order
for external links (such as images and style sheets) to display properly.
Note, however,that this will create a temporary file in your computer,
which won\(aqt be removed automatically.
.UNINDENT
.UNINDENT
.UNINDENT
.SS Available Scrapy objects
.sp
The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the \fBResponse\fP object and the
\fBSelector\fP objects (for both HTML and XML
content).
.sp
Those objects are:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBspider\fP \- the Spider which is known to handle the URL, or a
\fBSpider\fP object if there is no spider found for
the current URL
.IP \(bu 2
\fBrequest\fP \- a \fBRequest\fP object of the last fetched
page. You can modify this request using \fBreplace()\fP
or fetch a new request (without leaving the shell) using the \fBfetch\fP
shortcut.
.IP \(bu 2
\fBresponse\fP \- a \fBResponse\fP object containing the last
fetched page
.IP \(bu 2
\fBsel\fP \- a \fBSelector\fP object constructed
with the last response fetched
.IP \(bu 2
\fBsettings\fP \- the current \fIScrapy settings\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS Example of shell session
.sp
Here\(aqs an example of a typical shell session where we start by scraping the
\fI\%http://scrapy.org\fP page, and then proceed to scrape the \fI\%http://slashdot.org\fP
page. Finally, we modify the (Slashdot) request method to POST and re\-fetch it
getting a HTTP 405 (method not allowed) error. We end the session by typing
Ctrl\-D (in Unix systems) or Ctrl\-Z in Windows.
.sp
Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.
.sp
First, we launch the shell:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy shell \(aqhttp://scrapy.org\(aq \-\-nolog
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you\(aqll notice that these lines
all start with the \fB[s]\fP prefix):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[s] Available objects
[s]   sel       <Selector (http://scrapy.org) xpath=None>
[s]   item      Item()
[s]   request   <http://scrapy.org>
[s]   response  <http://scrapy.org>
[s]   settings  <Settings \(aqmybot.settings\(aq>
[s]   spider    <Spider \(aqdefault\(aq at 0x2bed9d0>
[s] Useful shortcuts:
[s]   shelp()           Prints this help.
[s]   fetch(req_or_url) Fetch a new request or URL and update objects
[s]   view(response)    View response in a browser

>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
After that, we can star playing with the objects:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath("//h2/text()").extract()[0]
u\(aqWelcome to Scrapy\(aq

>>> fetch("http://slashdot.org")
[s] Available Scrapy objects:
[s]   sel        <Selector (http://slashdot.org) xpath=None>
[s]   item       JobItem()
[s]   request    <GET http://slashdot.org>
[s]   response   <200 http://slashdot.org>
[s]   settings   <Settings \(aqjobsbot.settings\(aq>
[s]   spider     <Spider \(aqdefault\(aq at 0x3c44a10>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

>>> sel.xpath("//h2/text()").extract()
[u\(aqNews for nerds, stuff that matters\(aq]

>>> request = request.replace(method="POST")

>>> fetch(request)
2009\-04\-03 00:57:39\-0300 [default] ERROR: Downloading <http://slashdot.org> from <None>: 405 Method Not Allowed

>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Invoking the shell from spiders to inspect responses
.sp
Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.
.sp
This can be achieved by using the \fBscrapy.shell.inspect_response\fP function.
.sp
Here\(aqs an example of how you would call it from your spider:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MySpider(Spider):
    ...

    def parse(self, response):
        if response.url == \(aqhttp://www.example.com/products.php\(aq:
            from scrapy.shell import inspect_response
            inspect_response(response)

        # ... your parsing code ..
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
When you run the spider, you will get something similar to this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
2009\-08\-27 19:15:25\-0300 [example.com] DEBUG: Crawled <http://www.example.com/> (referer: <None>)
2009\-08\-27 19:15:26\-0300 [example.com] DEBUG: Crawled <http://www.example.com/products.php> (referer: <http://www.example.com/>)
[s] Available objects
[s]   sel       <Selector (http://www.example.com/products.php) xpath=None>
\&...

>>> response.url
\(aqhttp://www.example.com/products.php\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then, you can check if the extraction code is working:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> sel.xpath(\(aq//h1\(aq)
[]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Nope, it doesn\(aqt. So you can open the response in your web browser and see if
it\(aqs the response you were expecting:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> view(response)
>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Finally you hit Ctrl\-D (or Ctrl\-Z in Windows) to exit the shell and resume the
crawling:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> ^D
2009\-08\-27 19:15:25\-0300 [example.com] DEBUG: Crawled <http://www.example.com/product.php?id=1> (referer: <None>)
2009\-08\-27 19:15:25\-0300 [example.com] DEBUG: Crawled <http://www.example.com/product.php?id=2> (referer: <None>)
# ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Note that you can\(aqt use the \fBfetch\fP shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.
.SS Item Pipeline
.sp
After an item has been scraped by a spider, it is sent to the Item Pipeline
which process it through several components that are executed sequentially.
.sp
Each item pipeline component (sometimes referred as just "Item Pipeline") is a
Python class that implements a simple method. They receive an Item and perform
an action over it, also deciding if the Item should continue through the
pipeline or be dropped and no longer processed.
.sp
Typical use for item pipelines are:
.INDENT 0.0
.IP \(bu 2
cleansing HTML data
.IP \(bu 2
validating scraped data (checking that the items contain certain fields)
.IP \(bu 2
checking for duplicates (and dropping them)
.IP \(bu 2
storing the scraped item in a database
.UNINDENT
.SS Writing your own item pipeline
.sp
Writing your own item pipeline is easy. Each item pipeline component is a
single Python class that must implement the following method:
.INDENT 0.0
.TP
.B process_item(item, spider)
This method is called for every item pipeline component and must either return
a \fBItem\fP (or any descendant class) object or raise a
\fBDropItem\fP exception. Dropped items are no longer
processed by further pipeline components.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBitem\fP (\fBItem\fP object) \-\- the item scraped
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider which scraped the item
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Additionally, they may also implement the following methods:
.INDENT 0.0
.TP
.B open_spider(spider)
This method is called when the spider is opened.
.INDENT 7.0
.TP
.B Parameters
\fBspider\fP (\fBSpider\fP object) \-\- the spider which was opened
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B close_spider(spider)
This method is called when the spider is closed.
.INDENT 7.0
.TP
.B Parameters
\fBspider\fP (\fBSpider\fP object) \-\- the spider which was closed
.UNINDENT
.UNINDENT
.SS Item pipeline example
.SS Price validation and dropping items with no prices
.sp
Let\(aqs take a look at the following hypothetical pipeline that adjusts the \fBprice\fP
attribute for those items that do not include VAT (\fBprice_excludes_vat\fP
attribute), and drops those items which don\(aqt contain a price:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item[\(aqprice\(aq]:
            if item[\(aqprice_excludes_vat\(aq]:
                item[\(aqprice\(aq] = item[\(aqprice\(aq] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Write items to a JSON file
.sp
The following pipeline stores all scraped items (from all spiders) into a a
single \fBitems.jl\fP file, containing one item per line serialized in JSON
format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import json

class JsonWriterPipeline(object):

    def __init__(self):
        self.file = open(\(aqitems.jl\(aq, \(aqwb\(aq)

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\en"
        self.file.write(line)
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the \fIFeed exports\fP\&.
.UNINDENT
.UNINDENT
.SS Duplicates filter
.sp
A filter that looks for duplicate items, and drops those items that were
already processed. Let say that our items have an unique id, but our spider
returns multiples items with the same id:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.exceptions import DropItem

class DuplicatesPipeline(object):

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item[\(aqid\(aq] in self.ids_seen:
            raise DropItem("Duplicate item found: %s" % item)
        else:
            self.ids_seen.add(item[\(aqid\(aq])
            return item
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Activating an Item Pipeline component
.sp
To activate an Item Pipeline component you must add its class to the
\fBITEM_PIPELINES\fP setting, like in the following example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ITEM_PIPELINES = {
    \(aqmyproject.pipeline.PricePipeline\(aq: 300,
    \(aqmyproject.pipeline.JsonWriterPipeline\(aq: 800,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The integer values you assign to classes in this setting determine the
order they run in\- items go through pipelines from order number low to
high. It\(aqs customary to define these numbers in the 0\-1000 range.
.SS Feed exports
.sp
New in version 0.10.

.sp
One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating a "export file" with the scraped data (commonly called "export
feed") to be consumed by other systems.
.sp
Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate a feed with the scraped items, using multiple
serialization formats and storage backends.
.SS Serialization formats
.sp
For serializing the scraped data, the feed exports use the \fIItem exporters\fP and these formats are supported out of the box:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fItopics\-feed\-format\-json\fP
.IP \(bu 2
\fItopics\-feed\-format\-jsonlines\fP
.IP \(bu 2
\fItopics\-feed\-format\-csv\fP
.IP \(bu 2
\fItopics\-feed\-format\-xml\fP
.UNINDENT
.UNINDENT
.UNINDENT
.sp
But you can also extend the supported format through the
\fBFEED_EXPORTERS\fP setting.
.SS JSON
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBjson\fP
.IP \(bu 2
Exporter used: \fBJsonItemExporter\fP
.IP \(bu 2
See \fIthis warning\fP if you\(aqre using JSON with large feeds
.UNINDENT
.UNINDENT
.UNINDENT
.SS JSON lines
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBjsonlines\fP
.IP \(bu 2
Exporter used: \fBJsonLinesItemExporter\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS CSV
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBcsv\fP
.IP \(bu 2
Exporter used: \fBCsvItemExporter\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS XML
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBxml\fP
.IP \(bu 2
Exporter used: \fBXmlItemExporter\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS Pickle
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBpickle\fP
.IP \(bu 2
Exporter used: \fBPickleItemExporter\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS Marshal
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_FORMAT\fP: \fBmarshal\fP
.IP \(bu 2
Exporter used: \fBMarshalItemExporter\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS Storages
.sp
When using the feed exports you define where to store the feed using a \fI\%URI\fP
(through the \fBFEED_URI\fP setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.
.sp
The storages backends supported out of the box are:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fItopics\-feed\-storage\-fs\fP
.IP \(bu 2
\fItopics\-feed\-storage\-ftp\fP
.IP \(bu 2
\fItopics\-feed\-storage\-s3\fP (requires \fI\%boto\fP)
.IP \(bu 2
\fItopics\-feed\-storage\-stdout\fP
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the \fI\%boto\fP
library is installed.
.SS Storage URI parameters
.sp
The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fB%(time)s\fP \- gets replaced by a timestamp when the feed is being created
.IP \(bu 2
\fB%(name)s\fP \- gets replaced by the spider name
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Any other named parameter gets replaced by the spider attribute of the same
name. For example, \fB%(site_id)s\fP would get replaced by the \fBspider.site_id\fP
attribute the moment the feed is being created.
.sp
Here are some examples to illustrate:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
Store in FTP using one directory per spider:
.INDENT 2.0
.IP \(bu 2
\fBftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json\fP
.UNINDENT
.IP \(bu 2
Store in S3 using one directory per spider:
.INDENT 2.0
.IP \(bu 2
\fBs3://mybucket/scraping/feeds/%(name)s/%(time)s.json\fP
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Storage backends
.SS Local filesystem
.sp
The feeds are stored in the local filesystem.
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
URI scheme: \fBfile\fP
.IP \(bu 2
Example URI: \fBfile:///tmp/export.csv\fP
.IP \(bu 2
Required external libraries: none
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like \fB/tmp/export.csv\fP\&. This only works on Unix
systems though.
.SS FTP
.sp
The feeds are stored in a FTP server.
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
URI scheme: \fBftp\fP
.IP \(bu 2
Example URI: \fBftp://user:pass@ftp.example.com/path/to/export.csv\fP
.IP \(bu 2
Required external libraries: none
.UNINDENT
.UNINDENT
.UNINDENT
.SS S3
.sp
The feeds are stored on \fI\%Amazon S3\fP\&.
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
URI scheme: \fBs3\fP
.IP \(bu 2
Example URIs:
.INDENT 2.0
.IP \(bu 2
\fBs3://mybucket/path/to/export.csv\fP
.IP \(bu 2
\fBs3://aws_key:aws_secret@mybucket/path/to/export.csv\fP
.UNINDENT
.IP \(bu 2
Required external libraries: \fI\%boto\fP
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBAWS_ACCESS_KEY_ID\fP
.IP \(bu 2
\fBAWS_SECRET_ACCESS_KEY\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS Standard output
.sp
The feeds are written to the standard output of the Scrapy process.
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
URI scheme: \fBstdout\fP
.IP \(bu 2
Example URI: \fBstdout:\fP
.IP \(bu 2
Required external libraries: none
.UNINDENT
.UNINDENT
.UNINDENT
.SS Settings
.sp
These are the settings used for configuring the feed exports:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBFEED_URI\fP (mandatory)
.IP \(bu 2
\fBFEED_FORMAT\fP
.IP \(bu 2
\fBFEED_STORAGES\fP
.IP \(bu 2
\fBFEED_EXPORTERS\fP
.IP \(bu 2
\fBFEED_STORE_EMPTY\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS FEED_URI
.sp
Default: \fBNone\fP
.sp
The URI of the export feed. See \fItopics\-feed\-storage\-backends\fP for
supported URI schemes.
.sp
This setting is required for enabling the feed exports.
.SS FEED_FORMAT
.sp
The serialization format to be used for the feed. See
\fItopics\-feed\-format\fP for possible values.
.SS FEED_STORE_EMPTY
.sp
Default: \fBFalse\fP
.sp
Whether to export empty feeds (ie. feeds with no items).
.SS FEED_STORAGES
.sp
Default:: \fB{}\fP
.sp
A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.
.SS FEED_STORAGES_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aq\(aq: \(aqscrapy.contrib.feedexport.FileFeedStorage\(aq,
    \(aqfile\(aq: \(aqscrapy.contrib.feedexport.FileFeedStorage\(aq,
    \(aqstdout\(aq: \(aqscrapy.contrib.feedexport.StdoutFeedStorage\(aq,
    \(aqs3\(aq: \(aqscrapy.contrib.feedexport.S3FeedStorage\(aq,
    \(aqftp\(aq: \(aqscrapy.contrib.feedexport.FTPFeedStorage\(aq,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the built\-in feed storage backends supported by Scrapy.
.SS FEED_EXPORTERS
.sp
Default:: \fB{}\fP
.sp
A dict containing additional exporters supported by your project. The keys are
URI schemes and the values are paths to \fIItem exporter\fP
classes.
.SS FEED_EXPORTERS_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
FEED_EXPORTERS_BASE = {
    \(aqjson\(aq: \(aqscrapy.contrib.exporter.JsonItemExporter\(aq,
    \(aqjsonlines\(aq: \(aqscrapy.contrib.exporter.JsonLinesItemExporter\(aq,
    \(aqcsv\(aq: \(aqscrapy.contrib.exporter.CsvItemExporter\(aq,
    \(aqxml\(aq: \(aqscrapy.contrib.exporter.XmlItemExporter\(aq,
    \(aqmarshal\(aq: \(aqscrapy.contrib.exporter.MarshalItemExporter\(aq,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the built\-in feed exporters supported by Scrapy.
.SS Link Extractors
.sp
LinkExtractors are objects whose only purpose is to extract links from web
pages (\fBscrapy.http.Response\fP objects) which will be eventually
followed.
.sp
There are two Link Extractors available in Scrapy by default, but you create
your own custom Link Extractors to suit your needs by implementing a simple
interface.
.sp
The only public method that every LinkExtractor has is \fBextract_links\fP,
which receives a \fBResponse\fP object and returns a list
of links. Link Extractors are meant to be instantiated once and their
\fBextract_links\fP method called several times with different responses, to
extract links to follow.
.sp
Link extractors are used in the \fBCrawlSpider\fP
class (available in Scrapy), through a set of rules, but you can also use it in
your spiders, even if you don\(aqt subclass from
\fBCrawlSpider\fP, as its purpose is very simple: to
extract links.
.SS Built\-in link extractors reference
.sp
All available link extractors classes bundled with Scrapy are provided in the
\fBscrapy.contrib.linkextractors\fP module.
.SS SgmlLinkExtractor
.INDENT 0.0
.TP
.B class scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), tags=(\(aqa\(aq, \(aqarea\(aq), attrs=(\(aqhref\(aq), canonicalize=True, unique=True, process_value=None)
The SgmlLinkExtractor extends the base \fBBaseSgmlLinkExtractor\fP by
providing additional filters that you can specify to extract links,
including regular expressions patterns that the links must match to be
extracted. All those filters are configured through these constructor
parameters:
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBallow\fP (\fIa regular expression (or list of)\fP) \-\- a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.
.IP \(bu 2
\fBdeny\fP (\fIa regular expression (or list of)\fP) \-\- a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the \fBallow\fP parameter. If not
given (or empty) it won\(aqt exclude any links.
.IP \(bu 2
\fBallow_domains\fP (\fIstr or list\fP) \-\- a single value or a list of string containing
domains which will be considered for extracting the links
.IP \(bu 2
\fBdeny_domains\fP (\fIstr or list\fP) \-\- a single value or a list of strings containing
domains which won\(aqt be considered for extracting the links
.IP \(bu 2
\fBdeny_extensions\fP (\fIlist\fP) \-\- a list of extensions that should be ignored when
extracting links. If not given, it will default to the
\fBIGNORED_EXTENSIONS\fP list defined in the \fI\%scrapy.linkextractor\fP
module.
.IP \(bu 2
\fBrestrict_xpaths\fP (\fIstr or list\fP) \-\- is a XPath (or list of XPath\(aqs) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.
.IP \(bu 2
\fBtags\fP (\fIstr or list\fP) \-\- a tag or a list of tags to consider when extracting links.
Defaults to \fB(\(aqa\(aq, \(aqarea\(aq)\fP\&.
.IP \(bu 2
\fBattrs\fP (\fIlist\fP) \-\- list of attributes which should be considered when looking
for links to extract (only for those tags specified in the \fBtags\fP
parameter). Defaults to \fB(\(aqhref\(aq,)\fP
.IP \(bu 2
\fBcanonicalize\fP (\fIboolean\fP) \-\- canonicalize each extracted url (using
scrapy.utils.url.canonicalize_url). Defaults to \fBTrue\fP\&.
.IP \(bu 2
\fBunique\fP (\fIboolean\fP) \-\- whether duplicate filtering should be applied to extracted
links.
.IP \(bu 2
\fBprocess_value\fP (\fIcallable\fP) \-\- see \fBprocess_value\fP argument of
\fBBaseSgmlLinkExtractor\fP class constructor
.UNINDENT
.UNINDENT
.UNINDENT
.SS BaseSgmlLinkExtractor
.INDENT 0.0
.TP
.B class scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor(tag="a", attr="href", unique=False, process_value=None)
The purpose of this Link Extractor is only to serve as a base class for the
\fBSgmlLinkExtractor\fP\&. You should use that one instead.
.sp
The constructor arguments are:
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBtag\fP (\fIstr or callable\fP) \-\- either a string (with the name of a tag) or a function that
receives a tag name and returns \fBTrue\fP if links should be extracted from
that tag, or \fBFalse\fP if they shouldn\(aqt. Defaults to \fB\(aqa\(aq\fP\&.  request
(once it\(aqs downloaded) as its first parameter. For more information, see
\fItopics\-request\-response\-ref\-request\-callback\-arguments\fP\&.
.IP \(bu 2
\fBattr\fP (\fIstr or callable\fP) \-\- either string (with the name of a tag attribute), or a
function that receives an attribute name and returns \fBTrue\fP if
links should be extracted from it, or \fBFalse\fP if they shouldn\(aqt.
Defaults to \fBhref\fP\&.
.IP \(bu 2
\fBunique\fP (\fIboolean\fP) \-\- is a boolean that specifies if a duplicate filtering should
be applied to links extracted.
.IP \(bu 2
\fBprocess_value\fP (\fIcallable\fP) \-\- 
.sp
a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return \fBNone\fP to ignore the link altogether. If not
given, \fBprocess_value\fP defaults to \fBlambda x: x\fP\&.
.sp
For example, to extract links from this code:
.INDENT 2.0
.INDENT 3.5
.sp
.nf
.ft C
<a href="javascript:goToPage(\(aq../other/page.html\(aq); return false">Link text</a>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You can use the following function in \fBprocess_value\fP:
.INDENT 2.0
.INDENT 3.5
.sp
.nf
.ft C
def process_value(value):
    m = re.search("javascript:goToPage\e(\(aq(.*?)\(aq", value)
    if m:
        return m.group(1)
.ft P
.fi
.UNINDENT
.UNINDENT

.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \fBtopics/commands\fP
Learn about the command\-line tool used to manage your Scrapy project.
.TP
.B \fBtopics/items\fP
Define the data you want to scrape.
.TP
.B \fBtopics/spiders\fP
Write the rules to crawl your websites.
.TP
.B \fBtopics/selectors\fP
Extract the data from web pages using XPath.
.TP
.B \fBtopics/shell\fP
Test your extraction code in an interactive environment.
.TP
.B \fBtopics/loaders\fP
Populate your items with the extracted data.
.TP
.B \fBtopics/item\-pipeline\fP
Post\-process and store your scraped data.
.TP
.B \fBtopics/feed\-exports\fP
Output your scraped data using different formats and storages.
.TP
.B \fBtopics/link\-extractors\fP
Convenient classes to extract links to follow from pages.
.UNINDENT
.SH BUILT-IN SERVICES
.SS Logging
.sp
Scrapy provides a logging facility which can be used through the
\fBscrapy.log\fP module. The current underlying implementation uses \fI\%Twisted
logging\fP but this may change in the future.
.sp
The logging service must be explicitly started through the \fBscrapy.log.start()\fP function.
.SS Log levels
.sp
Scrapy provides 5 logging levels:
.INDENT 0.0
.IP 1. 3
\fBCRITICAL\fP \- for critical errors
.IP 2. 3
\fBERROR\fP \- for regular errors
.IP 3. 3
\fBWARNING\fP \- for warning messages
.IP 4. 3
\fBINFO\fP \- for informational messages
.IP 5. 3
\fBDEBUG\fP \- for debugging messages
.UNINDENT
.SS How to set the log level
.sp
You can set the log level using the \fI\-\-loglevel/\-L\fP command line option, or
using the \fBLOG_LEVEL\fP setting.
.SS How to log messages
.sp
Here\(aqs a quick example of how to log a message using the \fBWARNING\fP level:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import log
log.msg("This is a warning", level=log.WARNING)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Logging from Spiders
.sp
The recommended way to log from spiders is by using the Spider
\fBlog()\fP method, which already populates the
\fBspider\fP argument of the \fBscrapy.log.msg()\fP function. The other arguments
are passed directly to the \fBmsg()\fP function.
.SS scrapy.log module
.INDENT 0.0
.TP
.B scrapy.log.start(logfile=None, loglevel=None, logstdout=None)
Start the logging facility. This must be called before actually logging any
messages. Otherwise, messages logged before this call will get lost.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBlogfile\fP (\fIstr\fP) \-\- the file path to use for logging output. If omitted, the
\fBLOG_FILE\fP setting will be used. If both are \fBNone\fP, the log
will be sent to standard error.
.IP \(bu 2
\fBloglevel\fP \-\- the minimum logging level to log. Available values are:
\fBCRITICAL\fP, \fBERROR\fP, \fBWARNING\fP, \fBINFO\fP and
\fBDEBUG\fP\&.
.IP \(bu 2
\fBlogstdout\fP (\fIboolean\fP) \-\- if \fBTrue\fP, all standard output (and error) of your
application will be logged instead. For example if you "print \(aqhello\(aq"
it will appear in the Scrapy log. If omitted, the \fBLOG_STDOUT\fP
setting will be used.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.msg(message, level=INFO, spider=None)
Log a message
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBmessage\fP (\fIstr\fP) \-\- the message to log
.IP \(bu 2
\fBlevel\fP \-\- the log level for this message. See
\fItopics\-logging\-levels\fP\&.
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider to use for logging this message. This parameter
should always be used when logging things related to a particular
spider.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.CRITICAL
Log level for critical errors
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.ERROR
Log level for errors
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.WARNING
Log level for warnings
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.INFO
Log level for informational messages (recommended level for production
deployments)
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.log.DEBUG
Log level for debugging messages (recommended level for development)
.UNINDENT
.SS Logging settings
.sp
These settings can be used to configure the logging:
.INDENT 0.0
.IP \(bu 2
\fBLOG_ENABLED\fP
.IP \(bu 2
\fBLOG_ENCODING\fP
.IP \(bu 2
\fBLOG_FILE\fP
.IP \(bu 2
\fBLOG_LEVEL\fP
.IP \(bu 2
\fBLOG_STDOUT\fP
.UNINDENT
.SS Stats Collection
.sp
Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the \fBstats\fP
attribute of the \fItopics\-api\-crawler\fP, as illustrated by the examples in
the \fItopics\-stats\-usecases\fP section below.
.sp
However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it\(aqs disabled, the API
will still work but it won\(aqt collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you\(aqre
using the Stats Collector from.
.sp
Another feature of the Stats Collector is that it\(aqs very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.
.sp
The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.
.SS Common Stats Collector uses
.sp
Access the stats collector through the \fBstats\fP
attribute. Here is an example of an extension that access stats:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class ExtensionThatAccessStats(object):

    def __init__(self, stats):
        self.stats = stats

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Set stat value:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
stats.set_value(\(aqhostname\(aq, socket.gethostname())
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Increment stat value:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
stats.inc_value(\(aqpages_crawled\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Set stat value only if greater than previous:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
stats.max_value(\(aqmax_items_scraped\(aq, value)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Set stat value only if lower than previous:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
stats.min_value(\(aqmin_free_memory_percent\(aq, value)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Get stat value:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> stats.get_value(\(aqpages_crawled\(aq)
8
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Get all stats:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> stats.get_stats()
{\(aqpages_crawled\(aq: 1238, \(aqstart_time\(aq: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Available Stats Collectors
.sp
Besides the basic \fBStatsCollector\fP there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the \fBSTATS_CLASS\fP setting. The
default Stats Collector used is the \fBMemoryStatsCollector\fP\&.
.SS MemoryStatsCollector
.INDENT 0.0
.TP
.B class scrapy.statscol.MemoryStatsCollector
A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they\(aqre closed. The stats can be accessed
through the \fBspider_stats\fP attribute, which is a dict keyed by spider
domain name.
.sp
This is the default Stats Collector used in Scrapy.
.INDENT 7.0
.TP
.B spider_stats
A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.
.UNINDENT
.UNINDENT
.SS DummyStatsCollector
.INDENT 0.0
.TP
.B class scrapy.statscol.DummyStatsCollector
A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the \fBSTATS_CLASS\fP
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.
.UNINDENT
.SS Sending e\-mail
.sp
Although Python makes sending e\-mails relatively easy via the \fI\%smtplib\fP
library, Scrapy provides its own facility for sending e\-mails which is very
easy to use and it\(aqs implemented using \fI\%Twisted non\-blocking IO\fP, to avoid
interfering with the non\-blocking IO of the crawler. It also provides a
simple API for sending attachments and it\(aqs very easy to configure, with a few
\fIsettings\fP\&.
.SS Quick example
.sp
There are two ways to instantiate the mail sender. You can instantiate it using
the standard constructor:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.mail import MailSender
mailer = MailSender()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Or you can instantiate it passing a Scrapy settings object, which will respect
the \fIsettings\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mailer = MailSender.from_settings(settings)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And here is how to use it to send an e\-mail (without attachments):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mailer.send(to=["someone@example.com"], subject="Some subject", body="Some body", cc=["another@example.com"])
.ft P
.fi
.UNINDENT
.UNINDENT
.SS MailSender class reference
.sp
MailSender is the preferred class to use for sending emails from Scrapy, as it
uses \fI\%Twisted non\-blocking IO\fP, like the rest of the framework.
.INDENT 0.0
.TP
.B class scrapy.mail.MailSender(smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, smtpport=None)
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBsmtphost\fP (\fIstr\fP) \-\- the SMTP host to use for sending the emails. If omitted, the
\fBMAIL_HOST\fP setting will be used.
.IP \(bu 2
\fBmailfrom\fP (\fIstr\fP) \-\- the address used to send emails (in the \fBFrom:\fP header).
If omitted, the \fBMAIL_FROM\fP setting will be used.
.IP \(bu 2
\fBsmtpuser\fP \-\- the SMTP user. If omitted, the \fBMAIL_USER\fP
setting will be used. If not given, no SMTP authentication will be
performed.
.IP \(bu 2
\fBsmtppass\fP (\fIstr\fP) \-\- the SMTP pass for authentication.
.IP \(bu 2
\fBsmtpport\fP (\fIboolean\fP) \-\- the SMTP port to connect to
.IP \(bu 2
\fBsmtptls\fP \-\- enforce using SMTP STARTTLS
.IP \(bu 2
\fBsmtpssl\fP \-\- enforce using a secure SSL connection
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B classmethod from_settings(settings)
Instantiate using a Scrapy settings object, which will respect
\fIthese Scrapy settings\fP\&.
.INDENT 7.0
.TP
.B Parameters
\fBsettings\fP (\fBscrapy.settings.Settings\fP object) \-\- the e\-mail recipients
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B send(to, subject, body, cc=None, attachs=())
Send email to the given recipients.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBto\fP (\fIlist\fP) \-\- the e\-mail recipients
.IP \(bu 2
\fBsubject\fP (\fIstr\fP) \-\- the subject of the e\-mail
.IP \(bu 2
\fBcc\fP (\fIlist\fP) \-\- the e\-mails to CC
.IP \(bu 2
\fBbody\fP (\fIstr\fP) \-\- the e\-mail body
.IP \(bu 2
\fBattachs\fP (\fIiterable\fP) \-\- an iterable of tuples \fB(attach_name, mimetype,
file_object)\fP where  \fBattach_name\fP is a string with the name that will
appear on the e\-mail\(aqs attachment, \fBmimetype\fP is the mimetype of the
attachment and \fBfile_object\fP is a readable file object with the
contents of the attachment
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Mail settings
.sp
These settings define the default constructor values of the \fBMailSender\fP
class, and can be used to configure e\-mail notifications in your project without
writing any code (for those extensions and code that uses \fBMailSender\fP).
.SS MAIL_FROM
.sp
Default: \fB\(aqscrapy@localhost\(aq\fP
.sp
Sender email to use (\fBFrom:\fP header) for sending emails.
.SS MAIL_HOST
.sp
Default: \fB\(aqlocalhost\(aq\fP
.sp
SMTP host to use for sending emails.
.SS MAIL_PORT
.sp
Default: \fB25\fP
.sp
SMTP port to use for sending emails.
.SS MAIL_USER
.sp
Default: \fBNone\fP
.sp
User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.
.SS MAIL_PASS
.sp
Default: \fBNone\fP
.sp
Password to use for SMTP authentication, along with \fBMAIL_USER\fP\&.
.SS MAIL_TLS
.sp
Default: \fBFalse\fP
.sp
Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.
.SS MAIL_SSL
.sp
Default: \fBFalse\fP
.sp
Enforce connecting using an SSL encrypted connection
.SS Telnet Console
.sp
Scrapy comes with a built\-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.
.sp
The telnet console is a \fIbuilt\-in Scrapy extension\fP which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
\fItopics\-extensions\-ref\-telnetconsole\fP\&.
.SS How to access the telnet console
.sp
The telnet console listens in the TCP port defined in the
\fBTELNETCONSOLE_PORT\fP setting, which defaults to \fB6023\fP\&. To access
the console you need to type:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023
>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You need the telnet program which comes installed by default in Windows, and
most Linux distros.
.SS Available variables in the telnet console
.sp
The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.
.sp
However, the telnet console comes with some default variables defined for
convenience:
.TS
center;
|l|l|.
_
T{
Shortcut
T}	T{
Description
T}
_
T{
\fBcrawler\fP
T}	T{
the Scrapy Crawler (\fBscrapy.crawler.Crawler\fP object)
T}
_
T{
\fBengine\fP
T}	T{
Crawler.engine attribute
T}
_
T{
\fBspider\fP
T}	T{
the active spider
T}
_
T{
\fBslot\fP
T}	T{
the engine slot
T}
_
T{
\fBextensions\fP
T}	T{
the Extension Manager (Crawler.extensions attribute)
T}
_
T{
\fBstats\fP
T}	T{
the Stats Collector (Crawler.stats attribute)
T}
_
T{
\fBsettings\fP
T}	T{
the Scrapy settings object (Crawler.settings attribute)
T}
_
T{
\fBest\fP
T}	T{
print a report of the engine status
T}
_
T{
\fBprefs\fP
T}	T{
for memory debugging (see \fItopics\-leaks\fP)
T}
_
T{
\fBp\fP
T}	T{
a shortcut to the \fI\%pprint.pprint\fP function
T}
_
T{
\fBhpy\fP
T}	T{
for memory debugging (see \fItopics\-leaks\fP)
T}
_
.TE
.SS Telnet console usage examples
.sp
Here are some example tasks you can do with the telnet console:
.SS View engine status
.sp
You can use the \fBest()\fP method of the Scrapy engine to quickly show its state
using the telnet console:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023
>>> est()
Execution engine status

time()\-engine.start_time                        : 9.24237799644
engine.has_capacity()                           : False
engine.downloader.is_idle()                     : False
len(engine.downloader.slots)                    : 2
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False

Spider: <GayotSpider \(aqgayotcom\(aq at 0x2dc2b10>
  engine.spider_is_idle(spider)                      : False
  engine.slots[spider].closing                       : False
  len(engine.slots[spider].inprogress)               : 21
  len(engine.slots[spider].scheduler.dqs or [])      : 0
  len(engine.slots[spider].scheduler.mqs)            : 4453
  len(engine.scraper.slot.queue)                     : 0
  len(engine.scraper.slot.active)                    : 5
  engine.scraper.slot.active_size                    : 1515069
  engine.scraper.slot.itemproc_size                  : 0
  engine.scraper.slot.needs_backout()                : False
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Pause, resume and stop the Scrapy engine
.sp
To pause:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023
>>> engine.pause()
>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To resume:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023
>>> engine.unpause()
>>>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To stop:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023
>>> engine.stop()
Connection closed by foreign host.
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Telnet Console signals
.INDENT 0.0
.TP
.B scrapy.telnet.update_telnet_vars(telnet_vars)
Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
\fBtelnet_vars\fP dict in your handler.
.INDENT 7.0
.TP
.B Parameters
\fBtelnet_vars\fP (\fIdict\fP) \-\- the dict of telnet variables
.UNINDENT
.UNINDENT
.SS Telnet settings
.sp
These are the settings that control the telnet console\(aqs behaviour:
.SS TELNETCONSOLE_PORT
.sp
Default: \fB[6023, 6073]\fP
.sp
The port range to use for the telnet console. If set to \fBNone\fP or \fB0\fP, a
dynamically assigned port is used.
.SS TELNETCONSOLE_HOST
.sp
Default: \fB\(aq0.0.0.0\(aq\fP
.sp
The interface the telnet console should listen on
.SS Web Service
.sp
Scrapy comes with a built\-in web service for monitoring and controlling a
running crawler. The service exposes most resources using the \fI\%JSON\-RPC 2.0\fP
protocol, but there are also other (read\-only) resources which just output JSON
data.
.sp
Provides an extensible web service for managing a Scrapy process. It\(aqs enabled
by the \fBWEBSERVICE_ENABLED\fP setting. The web server will listen in the
port specified in \fBWEBSERVICE_PORT\fP, and will log to the file
specified in \fBWEBSERVICE_LOGFILE\fP\&.
.sp
The web service is a \fIbuilt\-in Scrapy extension\fP
which comes enabled by default, but you can also disable it if you\(aqre running
tight on memory.
.SS Web service resources
.sp
The web service contains several resources, defined in the
\fBWEBSERVICE_RESOURCES\fP setting. Each resource provides a different
functionality. See \fItopics\-webservice\-resources\-ref\fP for a list of
resources available by default.
.sp
Although you can implement your own resources using any protocol, there are
two kinds of resources bundled with Scrapy:
.INDENT 0.0
.IP \(bu 2
Simple JSON resources \- which are read\-only and just output JSON data
.IP \(bu 2
JSON\-RPC resources \- which provide direct access to certain Scrapy objects
using the \fI\%JSON\-RPC 2.0\fP protocol
.UNINDENT
.SS Available JSON\-RPC resources
.sp
These are the JSON\-RPC resources available by default in Scrapy:
.SS Crawler JSON\-RPC resource
.INDENT 0.0
.TP
.B class scrapy.contrib.webservice.crawler.CrawlerResource
Provides access to the main Crawler object that controls the Scrapy
process.
.sp
Available by default at: \fI\%http://localhost:6080/crawler\fP
.UNINDENT
.SS Stats Collector JSON\-RPC resource
.INDENT 0.0
.TP
.B class scrapy.contrib.webservice.stats.StatsResource
Provides access to the Stats Collector used by the crawler.
.sp
Available by default at: \fI\%http://localhost:6080/stats\fP
.UNINDENT
.SS Spider Manager JSON\-RPC resource
.sp
You can access the spider manager JSON\-RPC resource through the
\fItopics\-webservice\-crawler\fP at: \fI\%http://localhost:6080/crawler/spiders\fP
.SS Extension Manager JSON\-RPC resource
.sp
You can access the extension manager JSON\-RPC resource through the
\fItopics\-webservice\-crawler\fP at: \fI\%http://localhost:6080/crawler/spiders\fP
.SS Available JSON resources
.sp
These are the JSON resources available by default:
.SS Engine status JSON resource
.INDENT 0.0
.TP
.B class scrapy.contrib.webservice.enginestatus.EngineStatusResource
Provides access to engine status metrics.
.sp
Available by default at: \fI\%http://localhost:6080/enginestatus\fP
.UNINDENT
.SS Web service settings
.sp
These are the settings that control the web service behaviour:
.SS WEBSERVICE_ENABLED
.sp
Default: \fBTrue\fP
.sp
A boolean which specifies if the web service will be enabled (provided its
extension is also enabled).
.SS WEBSERVICE_LOGFILE
.sp
Default: \fBNone\fP
.sp
A file to use for logging HTTP requests made to the web service. If unset web
the log is sent to standard scrapy log.
.SS WEBSERVICE_PORT
.sp
Default: \fB[6080, 7030]\fP
.sp
The port range to use for the web service. If set to \fBNone\fP or \fB0\fP, a
dynamically assigned port is used.
.SS WEBSERVICE_HOST
.sp
Default: \fB\(aq0.0.0.0\(aq\fP
.sp
The interface the web service should listen on
.SS WEBSERVICE_RESOURCES
.sp
Default: \fB{}\fP
.sp
The list of web service resources enabled for your project. See
\fItopics\-webservice\-resources\fP\&. These are added to the ones available by
default in Scrapy, defined in the \fBWEBSERVICE_RESOURCES_BASE\fP setting.
.SS WEBSERVICE_RESOURCES_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqscrapy.contrib.webservice.crawler.CrawlerResource\(aq: 1,
    \(aqscrapy.contrib.webservice.enginestatus.EngineStatusResource\(aq: 1,
    \(aqscrapy.contrib.webservice.stats.StatsResource\(aq: 1,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The list of web service resources available by default in Scrapy. You shouldn\(aqt
change this setting in your project, change \fBWEBSERVICE_RESOURCES\fP
instead. If you want to disable some resource set its value to \fBNone\fP in
\fBWEBSERVICE_RESOURCES\fP\&.
.SS Writing a web service resource
.sp
Web service resources are implemented using the Twisted Web API. See this
\fI\%Twisted Web guide\fP for more information on Twisted web and Twisted web
resources.
.sp
To write a web service resource you should subclass the \fBJsonResource\fP or
\fBJsonRpcResource\fP classes and implement the \fBrenderGET\fP method.
.INDENT 0.0
.TP
.B class scrapy.webservice.JsonResource
A subclass of \fI\%twisted.web.resource.Resource\fP that implements a JSON web
service resource. See
.INDENT 7.0
.TP
.B ws_name
The name by which the Scrapy web service will known this resource, and
also the path where this resource will listen. For example, assuming
Scrapy web service is listening on \fI\%http://localhost:6080/\fP and the
\fBws_name\fP is \fB\(aqresource1\(aq\fP the URL for that resource will be:
.INDENT 7.0
.INDENT 3.5
\fI\%http://localhost:6080/resource1/\fP
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.webservice.JsonRpcResource(crawler, target=None)
This is a subclass of \fBJsonResource\fP for implementing JSON\-RPC
resources. JSON\-RPC resources wrap Python (Scrapy) objects around a
JSON\-RPC API. The resource wrapped must be returned by the
\fBget_target()\fP method, which returns the target passed in the
constructor by default
.INDENT 7.0
.TP
.B get_target()
Return the object wrapped by this JSON\-RPC resource. By default, it
returns the object passed on the constructor.
.UNINDENT
.UNINDENT
.SS Examples of web service resources
.SS StatsResource (JSON\-RPC resource)
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.webservice import JsonRpcResource

class StatsResource(JsonRpcResource):

    ws_name = \(aqstats\(aq

    def __init__(self, crawler):
        JsonRpcResource.__init__(self, crawler, crawler.stats)

.ft P
.fi
.UNINDENT
.UNINDENT
.SS EngineStatusResource (JSON resource)
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.webservice import JsonResource
from scrapy.utils.engine import get_engine_status

class EngineStatusResource(JsonResource):

    ws_name = \(aqenginestatus\(aq

    def __init__(self, crawler, spider_name=None):
        JsonResource.__init__(self, crawler)
        self._spider_name = spider_name
        self.isLeaf = spider_name is not None

    def render_GET(self, txrequest):
        status = get_engine_status(self.crawler.engine)
        if self._spider_name is None:
            return status
        for sp, st in status[\(aqspiders\(aq].items():
            if sp.name == self._spider_name:
                return st

    def getChild(self, name, txrequest):
        return EngineStatusResource(name, self.crawler)

.ft P
.fi
.UNINDENT
.UNINDENT
.SS Example of web service client
.SS scrapy\-ws.py script
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
#!/usr/bin/env python
"""
Example script to control a Scrapy server using its JSON\-RPC web service.

It only provides a reduced functionality as its main purpose is to illustrate
how to write a web service client. Feel free to improve or write you own.

Also, keep in mind that the JSON\-RPC API is not stable. The recommended way for
controlling a Scrapy server is through the execution queue (see the "queue"
command).

"""

from __future__ import print_function
import sys, optparse, urllib, json
from urlparse import urljoin

from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError

def get_commands():
    return {
        \(aqhelp\(aq: cmd_help,
        \(aqstop\(aq: cmd_stop,
        \(aqlist\-available\(aq: cmd_list_available,
        \(aqlist\-running\(aq: cmd_list_running,
        \(aqlist\-resources\(aq: cmd_list_resources,
        \(aqget\-global\-stats\(aq: cmd_get_global_stats,
        \(aqget\-spider\-stats\(aq: cmd_get_spider_stats,
    }

def cmd_help(args, opts):
    """help \- list available commands"""
    print("Available commands:")
    for _, func in sorted(get_commands().items()):
        print("  ", func.__doc__)

def cmd_stop(args, opts):
    """stop <spider> \- stop a running spider"""
    jsonrpc_call(opts, \(aqcrawler/engine\(aq, \(aqclose_spider\(aq, args[0])

def cmd_list_running(args, opts):
    """list\-running \- list running spiders"""
    for x in json_get(opts, \(aqcrawler/engine/open_spiders\(aq):
        print(x)

def cmd_list_available(args, opts):
    """list\-available \- list name of available spiders"""
    for x in jsonrpc_call(opts, \(aqcrawler/spiders\(aq, \(aqlist\(aq):
        print(x)

def cmd_list_resources(args, opts):
    """list\-resources \- list available web service resources"""
    for x in json_get(opts, \(aq\(aq)[\(aqresources\(aq]:
        print(x)

def cmd_get_spider_stats(args, opts):
    """get\-spider\-stats <spider> \- get stats of a running spider"""
    stats = jsonrpc_call(opts, \(aqstats\(aq, \(aqget_stats\(aq, args[0])
    for name, value in stats.items():
        print("%\-40s %s" % (name, value))

def cmd_get_global_stats(args, opts):
    """get\-global\-stats \- get global stats"""
    stats = jsonrpc_call(opts, \(aqstats\(aq, \(aqget_stats\(aq)
    for name, value in stats.items():
        print("%\-40s %s" % (name, value))

def get_wsurl(opts, path):
    return urljoin("http://%s:%s/"% (opts.host, opts.port), path)

def jsonrpc_call(opts, path, method, *args, **kwargs):
    url = get_wsurl(opts, path)
    return jsonrpc_client_call(url, method, *args, **kwargs)

def json_get(opts, path):
    url = get_wsurl(opts, path)
    return json.loads(urllib.urlopen(url).read())

def parse_opts():
    usage = "%prog [options] <command> [arg] ..."
    description = "Scrapy web service control script. Use \(aq%prog help\(aq " \e
        "to see the list of available commands."
    op = optparse.OptionParser(usage=usage, description=description)
    op.add_option("\-H", dest="host", default="localhost", \e
        help="Scrapy host to connect to")
    op.add_option("\-P", dest="port", type="int", default=6080, \e
        help="Scrapy port to connect to")
    opts, args = op.parse_args()
    if not args:
        op.print_help()
        sys.exit(2)
    cmdname, cmdargs, opts = args[0], args[1:], opts
    commands = get_commands()
    if cmdname not in commands:
        sys.stderr.write("Unknown command: %s\en\en" % cmdname)
        cmd_help(None, None)
        sys.exit(1)
    return commands[cmdname], cmdargs, opts

def main():
    cmd, args, opts = parse_opts()
    try:
        cmd(args, opts)
    except IndexError:
        print(cmd.__doc__)
    except JsonRpcError as e:
        print(str(e))
        if e.data:
            print("Server Traceback below:")
            print(e.data)


if __name__ == \(aq__main__\(aq:
    main()

.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \fBtopics/logging\fP
Understand the simple logging facility provided by Scrapy.
.TP
.B \fBtopics/stats\fP
Collect statistics about your scraping crawler.
.TP
.B \fBtopics/email\fP
Send email notifications when certain events occur.
.TP
.B \fBtopics/telnetconsole\fP
Inspect a running crawler using a built\-in Python console.
.TP
.B \fBtopics/webservice\fP
Monitor and control a crawler using a web service.
.UNINDENT
.SH SOLVING SPECIFIC PROBLEMS
.SS Frequently Asked Questions
.SS How does Scrapy compare to BeautifulSoup or lxml?
.sp
\fI\%BeautifulSoup\fP and \fI\%lxml\fP are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.
.sp
Scrapy provides a built\-in mechanism for extracting data (called
\fIselectors\fP) but you can easily use \fI\%BeautifulSoup\fP
(or \fI\%lxml\fP) instead, if you feel more comfortable working with them. After
all, they\(aqre just parsing libraries which can be imported and used from any
Python code.
.sp
In other words, comparing \fI\%BeautifulSoup\fP (or \fI\%lxml\fP) to Scrapy is like
comparing \fI\%jinja2\fP to \fI\%Django\fP\&.
.SS What Python versions does Scrapy support?
.sp
Scrapy is supported under Python 2.7 only.
Python 2.6 support was dropped starting at Scrapy 0.20.
.SS Does Scrapy work with Python 3?
.sp
No, but there are plans to support Python 3.3+.
At the moment, Scrapy works with Python 2.7.
.sp
\fBSEE ALSO:\fP
.INDENT 0.0
.INDENT 3.5
\fIfaq\-python\-versions\fP\&.
.UNINDENT
.UNINDENT
.SS Did Scrapy "steal" X from Django?
.sp
Probably, but we don\(aqt like that word. We think \fI\%Django\fP is a great open source
project and an example to follow, so we\(aqve used it as an inspiration for
Scrapy.
.sp
We believe that, if something is already done well, there\(aqs no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.
.sp
We\(aqd be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!
.SS Does Scrapy work with HTTP proxies?
.sp
Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
\fBHttpProxyMiddleware\fP\&.
.SS How can I scrape an item with attributes in different pages?
.sp
See \fItopics\-request\-response\-ref\-request\-callback\-arguments\fP\&.
.SS Scrapy crashes with: ImportError: No module named win32api
.sp
You need to install \fI\%pywin32\fP because of \fI\%this Twisted bug\fP\&.
.SS How can I simulate a user login in my spider?
.sp
See \fItopics\-request\-response\-ref\-request\-userlogin\fP\&.
.SS Does Scrapy crawl in breadth\-first or depth\-first order?
.sp
By default, Scrapy uses a \fI\%LIFO\fP queue for storing pending requests, which
basically means that it crawls in \fI\%DFO order\fP\&. This order is more convenient
in most cases. If you do want to crawl in true \fI\%BFO order\fP, you can do it by
setting the following settings:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = \(aqscrapy.squeue.PickleFifoDiskQueue\(aq
SCHEDULER_MEMORY_QUEUE = \(aqscrapy.squeue.FifoMemoryQueue\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS My Scrapy crawler has memory leaks. What can I do?
.sp
See \fItopics\-leaks\fP\&.
.sp
Also, Python has a builtin memory leak issue which is described in
\fItopics\-leaks\-without\-leaks\fP\&.
.SS How can I make Scrapy consume less memory?
.sp
See previous question.
.SS Can I use Basic HTTP Authentication in my spiders?
.sp
Yes, see \fBHttpAuthMiddleware\fP\&.
.SS Why does Scrapy download pages in English instead of my native language?
.sp
Try changing the default \fI\%Accept\-Language\fP request header by overriding the
\fBDEFAULT_REQUEST_HEADERS\fP setting.
.SS Where can I find some example Scrapy projects?
.sp
See \fIintro\-examples\fP\&.
.SS Can I run a spider without creating a project?
.sp
Yes. You can use the \fBrunspider\fP command. For example, if you have a
spider written in a \fBmy_spider.py\fP file you can run it with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy runspider my_spider.py
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
See \fBrunspider\fP command for more info.
.SS I get "Filtered offsite request" messages. How can I fix them?
.sp
Those messages (logged with \fBDEBUG\fP level) don\(aqt necessarily mean there is a
problem, so you may not need to fix them.
.sp
Those message are thrown by the Offsite Spider Middleware, which is a spider
middleware (enabled by default) whose purpose is to filter out requests to
domains outside the ones covered by the spider.
.sp
For more info see:
\fBOffsiteMiddleware\fP\&.
.SS What is the recommended way to deploy a Scrapy crawler in production?
.sp
See \fItopics\-scrapyd\fP\&.
.SS Can I use JSON for large exports?
.sp
It\(aqll depend on how large your output is. See \fIthis warning\fP in \fBJsonItemExporter\fP
documentation.
.SS Can I return (Twisted) deferreds from signal handlers?
.sp
Some signals support returning deferreds from their handlers, others don\(aqt. See
the \fItopics\-signals\-ref\fP to know which ones.
.SS What does the response status code 999 means?
.sp
999 is a custom reponse status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of \fB2\fP (or
higher) in your spider:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MySpider(CrawlSpider):

    name = \(aqmyspider\(aq

    download_delay = 2

    # [ ... rest of the spider code ... ]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Or by setting a global download delay in your project with the
\fBDOWNLOAD_DELAY\fP setting.
.SS Can I call \fBpdb.set_trace()\fP from my spiders to debug them?
.sp
Yes, but you can also use the Scrapy shell which allows you too quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old \fBpdb.set_trace()\fP\&.
.sp
For more info see \fItopics\-shell\-inspect\-response\fP\&.
.SS Simplest way to dump all my scraped items into a JSON/CSV/XML file?
.sp
To dump into a JSON file:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl myspider \-o items.json \-t json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To dump into a CSV file:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl myspider \-o items.csv \-t csv
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To dump into a XML file:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl myspider \-o items.xml \-t xml
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For more information see \fItopics\-feed\-exports\fP
.SS What\(aqs this huge cryptic \fB__VIEWSTATE\fP parameter used in some forms?
.sp
The \fB__VIEWSTATE\fP parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see \fI\%this page\fP\&. Also, here\(aqs an \fI\%example spider\fP
which scrapes one of these sites.
.SS What\(aqs the best way to parse big XML/CSV data feeds?
.sp
Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.
.sp
In order to avoid parsing all the entire feed at once in memory, you can use
the functions \fBxmliter\fP and \fBcsviter\fP from \fBscrapy.utils.iterators\fP
module. In fact, this is what the feed spiders (see \fItopics\-spiders\fP) use
under the cover.
.SS Does Scrapy manage cookies automatically?
.sp
Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.
.sp
For more info see \fItopics\-request\-response\fP and \fIcookies\-mw\fP\&.
.SS How can I see the cookies being sent and received from Scrapy?
.sp
Enable the \fBCOOKIES_DEBUG\fP setting.
.SS How can I instruct a spider to stop itself?
.sp
Raise the \fBCloseSpider\fP exception from a callback. For
more info see: \fBCloseSpider\fP\&.
.SS How can I prevent my Scrapy bot from getting banned?
.sp
See \fIbans\fP\&.
.SS Should I use spider arguments or settings to configure my spider?
.sp
Both \fIspider arguments\fP and \fIsettings\fP
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don\(aqt change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).
.sp
To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.
.SS I\(aqm scraping a XML document and my XPath selector doesn\(aqt return any items
.sp
You may need to remove namespaces. See \fIremoving\-namespaces\fP\&.
.SS I\(aqm getting an error: "cannot import name crawler"
.sp
This is caused by Scrapy changes due to the singletons removal. The error is
most likely raised by a module (extension, middleware, pipeline or spider) in
your Scrapy project that imports \fBcrawler\fP from \fBscrapy.project\fP\&. For
example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.project import crawler

class SomeExtension(object):
    def __init__(self):
        self.crawler = crawler
        # ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This way to access the crawler object is deprecated, the code should be ported
to use \fBfrom_crawler\fP class method, for example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class SomeExtension(object):

    @classmethod
    def from_crawler(cls, crawler):
        o = cls()
        o.crawler = crawler
        return o
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Scrapy command line tool has some backwards compatibility in place to support
the old import mechanism (with a deprecation warning), but this mechanism may
not work if you use Scrapy differently (for example, as a library).
.SS Debugging Spiders
.sp
This document explains the most common techniques for debugging spiders.
Consider the following scrapy spider below:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MySpider(Spider):
    name = \(aqmyspider\(aq
    start_urls = (
        \(aqhttp://example.com/page1\(aq,
        \(aqhttp://example.com/page2\(aq,
        )

    def parse(self, response):
        # collect \(gaitem_urls\(ga
        for item_url in item_urls:
            yield Request(url=item_url, callback=self.parse_item)

    def parse_item(self, response):
        item = MyItem()
        # populate \(gaitem\(ga fields
        yield Request(url=item_details_url, meta={\(aqitem\(aq: item},
            callback=self.parse_details)

    def parse_details(self, response):
        item = response.meta[\(aqitem\(aq]
        # populate more \(gaitem\(ga fields
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the \fBmeta\fP functionality of \fBRequest\fP to pass a
partially populated item.
.SS Parse Command
.sp
The most basic way of checking the output of your spider is to use the
\fBparse\fP command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.
.sp
In order to see the item scraped from a specific url:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy parse \-\-spider=myspider \-c parse_item \-d 2 <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 2 <<<
# Scraped Items  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[{\(aqurl\(aq: <item_url>}]

# Requests  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Using the \fB\-\-verbose\fP or \fB\-v\fP option we can see the status at each depth level:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy parse \-\-spider=myspider \-c parse_item \-d 2 \-v <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> DEPTH LEVEL: 1 <<<
# Scraped Items  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[]

# Requests  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[<GET item_details_url>]


>>> DEPTH LEVEL: 2 <<<
# Scraped Items  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[{\(aqurl\(aq: <item_url>}]

# Requests  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
[]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Checking items scraped from a single start_url, can also be easily achieved
using:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ scrapy parse \-\-spider=myspider \-d 3 \(aqhttp://example.com/page1\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Scrapy Shell
.sp
While the \fBparse\fP command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
\fBparse_details\fP sometimes receives no item?
.sp
Fortunately, the \fBshell\fP is your bread and butter in this case (see
\fItopics\-shell\-inspect\-response\fP):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.shell import inspect_response

def parse_details(self, response):
    item = response.meta.get(\(aqitem\(aq, None)
    if item:
        # populate more \(gaitem\(ga fields
        return item
    else:
        inspect_response(response, self)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
See also: \fItopics\-shell\-inspect\-response\fP\&.
.SS Open in browser
.sp
Sometimes you just want to see how a certain response looks in a browser, you
can use the \fBopen_in_browser\fP function for that. Here is an example of how
you would use it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if "item name" not in response.body:
        open_in_browser(response)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBopen_in_browser\fP will open a browser with the response received by Scrapy at
that point, adjusting the \fI\%base tag\fP so that images and styles are displayed
properly.
.SS Logging
.sp
Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import log

def parse_details(self, response):
    item = response.meta.get(\(aqitem\(aq, None)
    if item:
        # populate more \(gaitem\(ga fields
        return item
    else:
        self.log(\(aqNo item received for %s\(aq % response.url,
            level=log.WARNING)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For more information, check the \fItopics\-logging\fP section.
.SS Spiders Contracts
.sp
New in version 0.15.

.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
This is a new feature (introduced in Scrapy 0.15) and may be subject
to minor functionality/API updates. Check the \fIrelease notes\fP to
be notified of updates.
.UNINDENT
.UNINDENT
.sp
Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.
.sp
This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an \fB@\fP and included in the docstring. See the
following example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse(self, response):
    """ This function parses a sample response. Some contracts are mingled
    with this docstring.

    @url http://www.amazon.com/s?field\-keywords=selfish+gene
    @returns items 1 16
    @returns requests 0 0
    @scrapes Title Author Year Price
    """
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This callback is tested using three built\-in contracts:
.INDENT 0.0
.TP
.B class scrapy.contracts.default.UrlContract
This contract (\fB@url\fP) sets the sample url used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
@url url
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contracts.default.ReturnsContract
This contract (\fB@returns\fP) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
@returns item(s)|request(s) [min [max]]
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B class scrapy.contracts.default.ScrapesContract
This contract (\fB@scrapes\fP) checks that all the items returned by the
callback have the specified fields:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
@scrapes field_1 field_2 ...
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.sp
Use the \fBcheck\fP command to run the contract checks.
.SS Custom Contracts
.sp
If you find you need more power than the built\-in scrapy contracts you can
create and load your own contracts in the project by using the
\fBSPIDER_CONTRACTS\fP setting:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
SPIDER_CONTRACTS = {
    \(aqmyproject.contracts.ResponseCheck\(aq: 10,
    \(aqmyproject.contracts.ItemValidate\(aq: 10,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Each contract must inherit from \fBscrapy.contracts.Contract\fP and can
override three methods:
.INDENT 0.0
.TP
.B class scrapy.contracts.Contract(method, *args)
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBmethod\fP (\fIfunction\fP) \-\- callback function to which the contract is associated
.IP \(bu 2
\fBargs\fP (\fIlist\fP) \-\- list of arguments passed into the docstring (whitespace
separated)
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B adjust_request_args(args)
This receives a \fBdict\fP as an argument containing default arguments
for \fBRequest\fP object. Must return the same or a
modified version of it.
.UNINDENT
.INDENT 7.0
.TP
.B pre_process(response)
This allows hooking in various checks on the response received from the
sample request, before it\(aqs being passed to the callback.
.UNINDENT
.INDENT 7.0
.TP
.B post_process(output)
This allows processing the output of the callback. Iterators are
converted listified before being passed to this hook.
.UNINDENT
.UNINDENT
.sp
Here is a demo contract which checks the presence of a custom header in the
response received. Raise \fBscrapy.exceptions.ContractFail\fP in order to
get the failures pretty printed:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail

class HasHeaderContract(Contract):
    """ Demo contract which checks the presence of a custom header
        @has_header X\-CustomHeader
    """

    name = \(aqhas_header\(aq

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                raise ContractFail(\(aqX\-CustomHeader not present\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Common Practices
.sp
This section documents common practices when using Scrapy. These are things
that cover many topics and don\(aqt often fall into any other specific section.
.SS Run Scrapy from a script
.sp
You can use the \fIAPI\fP to run Scrapy from a script, instead of
the typical way of running Scrapy via \fBscrapy crawl\fP\&.
.sp
Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need run it inside the Twisted reactor.
.sp
Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by connecting a handler to the
\fBsignals.spider_closed\fP signal.
.sp
What follows is a working example of how to do that, using the \fI\%testspiders\fP
project as example.
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log, signals
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

spider = FollowAllSpider(domain=\(aqscrapinghub.com\(aq)
settings = get_project_settings()
crawler = Crawler(settings)
crawler.signals.connect(reactor.stop, signal=signals.spider_closed)
crawler.configure()
crawler.crawl(spider)
crawler.start()
log.start()
reactor.run() # the script will block here until the spider_closed signal was sent
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 0.0
.INDENT 3.5
\fI\%Twisted Reactor Overview\fP\&.
.UNINDENT
.UNINDENT
.SS Running multiple spiders in the same process
.sp
By default, Scrapy runs a single spider per process when you run \fBscrapy
crawl\fP\&. However, Scrapy supports running multiple spiders per process using
the \fIinternal API\fP\&.
.sp
Here is an example, using the \fI\%testspiders\fP project:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from twisted.internet import reactor
from scrapy.crawler import Crawler
from scrapy import log
from testspiders.spiders.followall import FollowAllSpider
from scrapy.utils.project import get_project_settings

def setup_crawler(domain):
    spider = FollowAllSpider(domain=domain)
    settings = get_project_settings()
    crawler = Crawler(settings)
    crawler.configure()
    crawler.crawl(spider)
    crawler.start()

for domain in [\(aqscrapinghub.com\(aq, \(aqinsophia.com\(aq]:
    setup_crawler(domain)
log.start()
reactor.run()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 0.0
.INDENT 3.5
\fIrun\-from\-script\fP\&.
.UNINDENT
.UNINDENT
.SS Distributed crawls
.sp
Scrapy doesn\(aqt provide any built\-in facility for running crawls in a distribute
(multi\-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.
.sp
If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.
.sp
If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:
.sp
First, you prepare the list of urls to crawl and put them into separate
files/urls:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
http://somedomain.com/urls\-to\-crawl/spider1/part1.list
http://somedomain.com/urls\-to\-crawl/spider1/part2.list
http://somedomain.com/urls\-to\-crawl/spider1/part3.list
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument \fBpart\fP with the number of the partition to
crawl:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
curl http://scrapy1.mycompany.com:6800/schedule.json \-d project=myproject \-d spider=spider1 \-d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json \-d project=myproject \-d spider=spider1 \-d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json \-d project=myproject \-d spider=spider1 \-d part=3
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Avoiding getting banned
.sp
Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting \fI\%commercial support\fP if in doubt.
.sp
Here are some tips to keep in mind when dealing with these kind of sites:
.INDENT 0.0
.IP \(bu 2
rotate your user agent from a pool of well\-known ones from browsers (google
around to get a list of them)
.IP \(bu 2
disable cookies (see \fBCOOKIES_ENABLED\fP) as some sites may use
cookies to spot bot behaviour
.IP \(bu 2
use download delays (2 or higher). See \fBDOWNLOAD_DELAY\fP setting.
.IP \(bu 2
if possible, use \fI\%Google cache\fP to fetch pages, instead of hitting the sites
directly
.IP \(bu 2
use a pool of rotating IPs. For example, the free \fI\%Tor project\fP or paid
services like \fI\%ProxyMesh\fP
.IP \(bu 2
use a highly distributed downloader that circumvents bans internally, so you
can just focus on parsing clean pages. One example of such downloaders is
\fI\%Crawlera\fP
.UNINDENT
.sp
If you are still unable to prevent your bot getting banned, consider contacting
\fI\%commercial support\fP\&.
.SS Dynamic Creation of Item Classes
.sp
For applications in which the structure of item class is to be determined by
user input, or other changing conditions, you can dynamically create item
classes instead of manually coding them.
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import DictItem, Field

def create_item_class(class_name,field_list):
    field_dict = {}
    for field_name in field_list:
        field_dict[field_name] = Field()

    return type(class_name,DictItem,field_dict)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Broad Crawls
.sp
Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).
.sp
In addition to this "focused crawl", there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called "broad crawls" and is the typical crawlers employed by search
engines.
.sp
These are some common properties often found in broad crawls:
.INDENT 0.0
.IP \(bu 2
they crawl many domains (often, unbounded) instead of a specific set of sites
.IP \(bu 2
they don\(aqt necessarily crawl domains to completion, because it would
impractical (or impossible) to do so, and instead limit the crawl by time or
number of pages crawled
.IP \(bu 2
they are simpler in logic (as opposed to very complex spiders with many
extraction rules) because data is often post\-processed in a separate stage
.IP \(bu 2
they crawl many domains concurrently, which allows them to achieve faster
crawl speeds by not being limited by any particular site constraint (each site
is crawled slowly to respect politeness, but many sites are crawled in
parallel)
.UNINDENT
.sp
As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarize some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.
.SS Increase concurrency
.sp
Concurrency is the number of requests that are processed in parallel. There is
a global limit and a per\-domain limit.
.sp
The default global concurrency limit in Scrapy is not suitable for crawling
many different  domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU you crawler will have available. A
good starting point is \fB100\fP, but the best way to find out is by doing some
trials and identifying at what concurrency your Scrapy process gets CPU
bounded. For optimum performance, You should pick a concurrency where CPU usage
is at 80\-90%.
.sp
To increase the global concurrency use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
CONCURRENT_REQUESTS = 100
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Reduce log level
.sp
When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
\fBINFO\fP log level. In order to save CPU (and log storage requirements) you
should not use \fBDEBUG\fP log level when preforming large broad crawls in
production. Using \fBDEBUG\fP level when developing your (broad) crawler may fine
though.
.sp
To set the log level use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
LOG_LEVEL = \(aqINFO\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Disable cookies
.sp
Disable cookies unless you \fIreally\fP need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.
.sp
To disable cookies use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
COOKIES_ENABLED = False
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Disable retries
.sp
Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.
.sp
To disable retries use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
RETRY_ENABLED = False
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Reduce download timeout
.sp
Unless you are crawling from a very slow connection (which shouldn\(aqt be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.
.sp
To reduce the download timeout use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
DOWNLOAD_TIMEOUT = 15
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Disable redirects
.sp
Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it\(aqs common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.
.sp
To disable redirects use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
REDIRECT_ENABLED = False
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using Firefox for scraping
.sp
Here is a list of tips and advice on using Firefox for scraping, along with a
list of useful Firefox add\-ons to ease the scraping process.
.SS Caveats with inspecting the live browser DOM
.sp
Since Firefox add\-ons operate on a live browser DOM, what you\(aqll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing Javascript code.  Firefox,
in particular, is known for adding \fB<tbody>\fP elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won\(aqt be able to
extract any data if you use \fB<tbody\fP in your XPath expressions.
.sp
Therefore, you should keep in mind the following things when working with
Firefox and XPath:
.INDENT 0.0
.IP \(bu 2
Disable Firefox Javascript while inspecting the DOM looking for XPaths to be
used in Scrapy
.IP \(bu 2
Never use full XPath paths, use relative and clever ones based on attributes
(such as \fBid\fP, \fBclass\fP, \fBwidth\fP, etc) or any identifying features like
\fBcontains(@href, \(aqimage\(aq)\fP\&.
.IP \(bu 2
Never include \fB<tbody>\fP elements in your XPath expressions unless you
really know what you\(aqre doing
.UNINDENT
.SS Useful Firefox add\-ons for scraping
.SS Firebug
.sp
\fI\%Firebug\fP is a widely known tool among web developers and it\(aqs also very
useful for scraping. In particular, its \fI\%Inspect Element\fP feature comes very
handy when you need to construct the XPaths for extracting data because it
allows you to view the HTML code of each page element while moving your mouse
over it.
.sp
See \fItopics\-firebug\fP for a detailed guide on how to use Firebug with
Scrapy.
.SS XPather
.sp
\fI\%XPather\fP allows you to test XPath expressions directly on the pages.
.SS XPath Checker
.sp
\fI\%XPath Checker\fP is another Firefox add\-on for testing XPaths on your pages.
.SS Tamper Data
.sp
\fI\%Tamper Data\fP is a Firefox add\-on which allows you to view and modify the HTTP
request headers sent by Firefox. Firebug also allows to view HTTP headers, but
not to modify them.
.SS Firecookie
.sp
\fI\%Firecookie\fP makes it easier to view and manage cookies. You can use this
extension to create a new cookie, delete existing cookies, see a list of cookies
for the current site, manage cookies permissions and a lot more.
.SS Using Firebug for scraping
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
Google Directory, the example website used in this guide is no longer
available as it \fI\%has been shut down by Google\fP\&. The concepts in this guide
are still valid though. If you want to update this guide to use a new
(working) site, your contribution will be more than welcome!. See \fItopics\-contributing\fP
for information on how to do so.
.UNINDENT
.UNINDENT
.SS Introduction
.sp
This document explains how to use \fI\%Firebug\fP (a Firefox add\-on) to make the
scraping process easier and more fun. For other useful Firefox add\-ons see
\fItopics\-firefox\-addons\fP\&. There are some caveats with using Firefox add\-ons
to inspect pages, see \fItopics\-firefox\-livedom\fP\&.
.sp
In this example, we\(aqll show how to use \fI\%Firebug\fP to scrape data from the
\fI\%Google Directory\fP, which contains the same data as the \fI\%Open Directory
Project\fP used in the \fItutorial\fP but with a different
face.
.sp
Firebug comes with a very useful feature called \fI\%Inspect Element\fP which allows
you to inspect the HTML code of the different page elements just by hovering
your mouse over them. Otherwise you would have to search for the tags manually
through the HTML body which can be a very tedious task.
.sp
In the following screenshot you can see the \fI\%Inspect Element\fP tool in action.
[image: Inspecting elements with Firebug]
[image]
.sp
At first sight, we can see that the directory is divided in categories, which
are also divided in subcategories.
.sp
However, it seems that there are more subcategories than the ones being shown
in this page, so we\(aqll keep looking:
[image: Inspecting elements with Firebug]
[image]
.sp
As expected, the subcategories contain links to other subcategories, and also
links to actual websites, which is the purpose of the directory.
.SS Getting links to follow
.sp
By looking at the category URLs we can see they share a pattern:
.INDENT 0.0
.INDENT 3.5
\fI\%http://directory.google.com/Category/Subcategory/Another_Subcategory\fP
.UNINDENT
.UNINDENT
.sp
Once we know that, we are able to construct a regular expression to follow
those links. For example, the following one:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
directory\e.google\e.com/[A\-Z][a\-zA\-Z_/]+$
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
So, based on that regular expression we can create the first crawling rule:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Rule(SgmlLinkExtractor(allow=\(aqdirectory.google.com/[A\-Z][a\-zA\-Z_/]+$\(aq, ),
    \(aqparse_category\(aq,
    follow=True,
),
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBRule\fP object instructs
\fBCrawlSpider\fP based spiders how to follow the
category links. \fBparse_category\fP will be a method of the spider which will
process and extract data from those pages.
.sp
This is how the spider would look so far:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule

class GoogleDirectorySpider(CrawlSpider):
    name = \(aqdirectory.google.com\(aq
    allowed_domains = [\(aqdirectory.google.com\(aq]
    start_urls = [\(aqhttp://directory.google.com/\(aq]

    rules = (
        Rule(SgmlLinkExtractor(allow=\(aqdirectory\e.google\e.com/[A\-Z][a\-zA\-Z_/]+$\(aq),
            \(aqparse_category\(aq, follow=True,
        ),
    )

    def parse_category(self, response):
        # write the category page data extraction code here
        pass
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Extracting the data
.sp
Now we\(aqre going to write the code to extract data from those pages.
.sp
With the help of Firebug, we\(aqll take a look at some page containing links to
websites (say \fI\%http://directory.google.com/Top/Arts/Awards/\fP) and find out how we can
extract those links using \fISelectors\fP\&. We\(aqll also
use the \fIScrapy shell\fP to test those XPath\(aqs and make sure
they work as we expect.
[image: Inspecting elements with Firebug]
[image]
.sp
As you can see, the page markup is not very descriptive: the elements don\(aqt
contain \fBid\fP, \fBclass\fP or any attribute that clearly identifies them, so
we\(aq\(aqll use the ranking bars as a reference point to select the data to extract
when we construct our XPaths.
.sp
After using FireBug, we can see that each link is inside a \fBtd\fP tag, which is
itself inside a \fBtr\fP tag that also contains the link\(aqs ranking bar (in
another \fBtd\fP).
.sp
So we can select the ranking bar, then find its parent (the \fBtr\fP), and then
finally, the link\(aqs \fBtd\fP (which contains the data we want to scrape).
.sp
This results in the following XPath:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
//td[descendant::a[contains(@href, "#pagerank")]]/following\-sibling::td//a
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
It\(aqs important to use the \fIScrapy shell\fP to test these
complex XPath expressions and make sure they work as expected.
.sp
Basically, that expression will look for the ranking bar\(aqs \fBtd\fP element, and
then select any \fBtd\fP element who has a descendant \fBa\fP element whose
\fBhref\fP attribute contains the string \fB#pagerank\fP"
.sp
Of course, this is not the only XPath, and maybe not the simpler one to select
that data. Another approach could be, for example, to find any \fBfont\fP tags
that have that grey colour of the links,
.sp
Finally, we can write our \fBparse_category()\fP method:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_category(self, response):
    sel = Selector(response)

    # The path to website links in directory page
    links = sel.xpath(\(aq//td[descendant::a[contains(@href, "#pagerank")]]/following\-sibling::td/font\(aq)

    for link in links:
        item = DirectoryItem()
        item[\(aqname\(aq] = link.xpath(\(aqa/text()\(aq).extract()
        item[\(aqurl\(aq] = link.xpath(\(aqa/@href\(aq).extract()
        item[\(aqdescription\(aq] = link.xpath(\(aqfont[2]/text()\(aq).extract()
        yield item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Be aware that you may find some elements which appear in Firebug but
not in the original HTML, such as the typical case of \fB<tbody>\fP
elements.
.sp
or tags which Therefer   in page HTML
sources may on Firebug inspects the live DOM
.SS Debugging memory leaks
.sp
In Scrapy, objects such as Requests, Responses and Items have a finite
lifetime: they are created, used for a while, and finally destroyed.
.sp
From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it\(aqs time to process
it. For more info see \fItopics\-architecture\fP\&.
.sp
As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a "memory leak".
.sp
To help debugging memory leaks, Scrapy provides a built\-in mechanism for
tracking objects references called \fItrackref\fP,
and you can also use a third\-party library called \fIGuppy\fP for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the \fITelnet Console\fP\&.
.SS Common causes of memory leaks
.sp
It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
\fBmeta\fP attribute or the request callback function)
and that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of memory leaks
in Scrapy projects, and a quite difficult one to debug for newcomers.
.sp
In big projects, the spiders are typically written by different people and some
of those spiders could be "leaking" and thus affecting the rest of the other
(well\-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.
.sp
At the same time, it\(aqs hard to avoid the reasons that cause these leaks
without restricting the power of the framework, so we have decided not to
restrict the functionally but provide useful tools for debugging these leaks,
which quite often consist in an answer to the question: \fIwhich spider is leaking?\fP\&.
.sp
The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, if you\(aqre allocating resources on
\fBspider_opened\fP but not releasing them on \fBspider_closed\fP\&.
.SS Debugging memory leaks with \fBtrackref\fP
.sp
\fBtrackref\fP is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Requests,
Responses, Item and Selector objects.
.sp
You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the \fBprefs()\fP function which is an
alias to the \fBprint_live_refs()\fP function:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
telnet localhost 6023

>>> prefs()
Live References

ExampleSpider                       1   oldest: 15s ago
HtmlResponse                       10   oldest: 1s ago
Selector                            2   oldest: 0s ago
FormRequest                       878   oldest: 7s ago
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As you can see, that report also shows the "age" of the oldest object in each
class.
.sp
If you do have leaks, chances are you can figure out which spider is leaking by
looking at the oldest request or response. You can get the oldest object of
each class using the \fBget_oldest()\fP function like
this (from the telnet console).
.SS Which objects are tracked?
.sp
The objects tracked by \fBtrackrefs\fP are all from these classes (and all its
subclasses):
.INDENT 0.0
.IP \(bu 2
\fBscrapy.http.Request\fP
.IP \(bu 2
\fBscrapy.http.Response\fP
.IP \(bu 2
\fBscrapy.item.Item\fP
.IP \(bu 2
\fBscrapy.selector.Selector\fP
.IP \(bu 2
\fBscrapy.spider.Spider\fP
.UNINDENT
.SS A real example
.sp
Let\(aqs see a concrete example of an hypothetical case of memory leaks.
.sp
Suppose we have some spider with a line similar to this one:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
return Request("http://www.somenastyspider.com/product.php?pid=%d" % product_id,
    callback=self.parse, meta={referer: response}")
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests\(aq one, and that would definitely
cause memory leaks.
.sp
Let\(aqs see how we can discover which one is the nasty spider (without knowing it
a\-priori, of course) by using the \fBtrackref\fP tool.
.sp
After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> prefs()
Live References

SomenastySpider                     1   oldest: 15s ago
HtmlResponse                     3890   oldest: 265s ago
Selector                            2   oldest: 0s ago
Request                          3878   oldest: 250s ago
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The fact that there are so many live responses (and that they\(aqre so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. So let\(aqs check the oldest response:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.utils.trackref import get_oldest
>>> r = get_oldest(\(aqHtmlResponse\(aq)
>>> r.url
\(aqhttp://www.somenastyspider.com/product.php?pid=123\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
There it is. By looking at the URL of the oldest response we can see it belongs
to the \fBsomenastyspider.com\fP spider. We can now go and check the code of that
spider to discover the nasty line that is generating the leaks (passing
response references inside requests).
.sp
If you want to iterate over all objects, instead of getting the oldest one, you
can use the \fBiter_all()\fP function:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.utils.trackref import iter_all
>>> [r.url for r in iter_all(\(aqHtmlResponse\(aq)]
[\(aqhttp://www.somenastyspider.com/product.php?pid=123\(aq,
 \(aqhttp://www.somenastyspider.com/product.php?pid=584\(aq,
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Too many spiders?
.sp
If your project has too many spiders, the output of \fBprefs()\fP can be
difficult to read. For this reason, that function has a \fBignore\fP argument
which can be used to ignore a particular class (and all its subclases). For
example, using:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> from scrapy.spider import Spider
>>> prefs(ignore=Spider)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Won\(aqt show any live references to spiders.
.SS scrapy.utils.trackref module
.sp
Here are the functions available in the \fBtrackref\fP module.
.INDENT 0.0
.TP
.B class scrapy.utils.trackref.object_ref
Inherit from this class (instead of object) if you want to track live
instances with the \fBtrackref\fP module.
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.utils.trackref.print_live_refs(class_name, ignore=NoneType)
Print a report of live references, grouped by class name.
.INDENT 7.0
.TP
.B Parameters
\fBignore\fP (\fIclass or classes tuple\fP) \-\- if given, all objects from the specified class (or tuple of
classes) will be ignored.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.utils.trackref.get_oldest(class_name)
Return the oldest object alive with the given class name, or \fBNone\fP if
none is found. Use \fBprint_live_refs()\fP first to get a list of all
tracked live objects per class name.
.UNINDENT
.INDENT 0.0
.TP
.B scrapy.utils.trackref.iter_all(class_name)
Return an iterator over all objects alive with the given class name, or
\fBNone\fP if none is found. Use \fBprint_live_refs()\fP first to get a list
of all tracked live objects per class name.
.UNINDENT
.SS Debugging memory leaks with Guppy
.sp
\fBtrackref\fP provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks (Requests, Responses, Items, and Selectors). However, there are
other cases where the memory leaks could come from other (more or less obscure)
objects. If this is your case, and you can\(aqt find your leaks using \fBtrackref\fP,
you still have another resource: the \fI\%Guppy library\fP\&.
.sp
If you use \fBsetuptools\fP, you can install Guppy with the following command:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
easy_install guppy
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The telnet console also comes with a built\-in shortcut (\fBhpy\fP) for accessing
Guppy heap objects. Here\(aqs an example to view all Python objects available in
the heap using Guppy:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> x = hpy.heap()
>>> x.bytype
Partition of a set of 297033 objects. Total size = 52587824 bytes.
 Index  Count   %     Size   % Cumulative  % Type
     0  22307   8 16423880  31  16423880  31 dict
     1 122285  41 12441544  24  28865424  55 str
     2  68346  23  5966696  11  34832120  66 tuple
     3    227   0  5836528  11  40668648  77 unicode
     4   2461   1  2222272   4  42890920  82 type
     5  16870   6  2024400   4  44915320  85 function
     6  13949   5  1673880   3  46589200  89 types.CodeType
     7  13422   5  1653104   3  48242304  92 list
     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern
     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers
<1676 more rows. Type e.g. \(aq_.more\(aq to view.>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You can see that most space is used by dicts. Then, if you want to see from
which attribute those dicts are referenced, you could do:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> x.bytype[0].byvia
Partition of a set of 22307 objects. Total size = 16423880 bytes.
 Index  Count   %     Size   % Cumulative  % Referred Via:
     0  10982  49  9416336  57   9416336  57 \(aq.__dict__\(aq
     1   1820   8  2681504  16  12097840  74 \(aq.__dict__\(aq, \(aq.func_globals\(aq
     2   3097  14  1122904   7  13220744  80
     3    990   4   277200   2  13497944  82 "[\(aqcookies\(aq]"
     4    987   4   276360   2  13774304  84 "[\(aqcache\(aq]"
     5    985   4   275800   2  14050104  86 "[\(aqmeta\(aq]"
     6    897   4   251160   2  14301264  87 \(aq[2]\(aq
     7      1   0   196888   1  14498152  88 "[\(aqmoduleDict\(aq]", "[\(aqmodules\(aq]"
     8    672   3   188160   1  14686312  89 "[\(aqcb_kwargs\(aq]"
     9     27   0   155016   1  14841328  90 \(aq[1]\(aq
<333 more rows. Type e.g. \(aq_.more\(aq to view.>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As you can see, the Guppy module is very powerful but also requires some deep
knowledge about Python internals. For more info about Guppy, refer to the
\fI\%Guppy documentation\fP\&.
.SS Leaks without leaks
.sp
Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:
.INDENT 0.0
.IP \(bu 2
\fI\%Python Memory Management\fP
.IP \(bu 2
\fI\%Python Memory Management Part 2\fP
.IP \(bu 2
\fI\%Python Memory Management Part 3\fP
.UNINDENT
.sp
The improvements proposed by Evan Jones, which are detailed in \fI\%this paper\fP,
got merged in Python 2.5, but this only reduces the problem, it doesn\(aqt fix it
completely. To quote the paper:
.INDENT 0.0
.INDENT 3.5
\fIUnfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a large
issue. An application could have many megabytes of free memory, scattered
throughout all the arenas, but it will be unable to free any of it. This is
a problem experienced by all memory allocators. The only way to solve it is
to move to a compacting garbage collector, which is able to move objects in
memory. This would require significant changes to the Python interpreter.\fP
.UNINDENT
.UNINDENT
.sp
This problem will be fixed in future Scrapy releases, where we plan to adopt a
new process model and run spiders in a pool of recyclable sub\-processes.
.SS Downloading Item Images
.sp
Scrapy provides an \fBitem pipeline\fP for downloading
images attached to a particular item, for example, when you scrape products and
also want to download their images locally.
.sp
This pipeline, called the Images Pipeline and implemented in the
\fBImagesPipeline\fP class, provides a convenient way for
downloading and storing images locally with some additional features:
.INDENT 0.0
.IP \(bu 2
Convert all downloaded images to a common format (JPG) and mode (RGB)
.IP \(bu 2
Avoid re\-downloading images which were downloaded recently
.IP \(bu 2
Thumbnail generation
.IP \(bu 2
Check images width/height to make sure they meet a minimum constraint
.UNINDENT
.sp
This pipeline also keeps an internal queue of those images which are currently
being scheduled for download, and connects those items that arrive containing
the same image, to that queue. This avoids downloading the same image more than
once when it\(aqs shared by several items.
.sp
\fI\%Pillow\fP is used for thumbnailing and normalizing images to JPEG/RGB format,
so you need to install this library in order to use the images pipeline.
\fI\%Python Imaging Library\fP (PIL) should also work in most cases, but it
is known to cause troubles in some setups, so we recommend to use \fI\%Pillow\fP
instead of \fI\%PIL\fP\&.
.SS Using the Images Pipeline
.sp
The typical workflow, when using the \fBImagesPipeline\fP goes like
this:
.INDENT 0.0
.IP 1. 3
In a Spider, you scrape an item and put the URLs of its images into a
\fBimage_urls\fP field.
.IP 2. 3
The item is returned from the spider and goes to the item pipeline.
.IP 3. 3
When the item reaches the \fBImagesPipeline\fP, the URLs in the
\fBimage_urls\fP field are scheduled for download using the standard
Scrapy scheduler and downloader (which means the scheduler and downloader
middlewares are reused), but with a higher priority, processing them before other
pages are scraped. The item remains "locked" at that particular pipeline stage
until the images have finish downloading (or fail for some reason).
.IP 4. 3
When the images are downloaded another field (\fBimages\fP) will be populated
with the results. This field will contain a list of dicts with information
about the images downloaded, such as the downloaded path, the original
scraped url (taken from the \fBimage_urls\fP field) , and the image checksum.
The images in the list of the \fBimages\fP field will retain the same order of
the original \fBimage_urls\fP field. If some image failed downloading, an
error will be logged and the image won\(aqt be present in the \fBimages\fP field.
.UNINDENT
.SS Usage example
.sp
In order to use the image pipeline you just need to \fIenable it\fP and define an item with the \fBimage_urls\fP and
\fBimages\fP fields:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item

class MyItem(Item):

    # ... other item fields ...
    image_urls = Field()
    images = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
If you need something more complex and want to override the custom images
pipeline behaviour, see \fItopics\-images\-override\fP\&.
.SS Enabling your Images Pipeline
.sp
To enable your images pipeline you must first add it to your project
\fBITEM_PIPELINES\fP setting:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ITEM_PIPELINES = {\(aqscrapy.contrib.pipeline.images.ImagesPipeline\(aq: 1}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And set the \fBIMAGES_STORE\fP setting to a valid directory that will be
used for storing the downloaded images. Otherwise the pipeline will remain
disabled, even if you include it in the \fBITEM_PIPELINES\fP setting.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
IMAGES_STORE = \(aq/path/to/valid/dir\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Images Storage
.sp
File system is currently the only officially supported storage, but there is
also (undocumented) support for \fI\%Amazon S3\fP\&.
.SS File system storage
.sp
The images are stored in files (one per image), using a \fI\%SHA1 hash\fP of their
URLs for the file names.
.sp
For example, the following image URL:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
http://www.example.com/image.jpg
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Whose \fISHA1 hash\fP is:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
3afec3b4765f8f0a07b78f98c07b83f013567a0a
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Will be downloaded and stored in the following file:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<IMAGES_STORE>/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Where:
.INDENT 0.0
.IP \(bu 2
\fB<IMAGES_STORE>\fP is the directory defined in \fBIMAGES_STORE\fP setting
.IP \(bu 2
\fBfull\fP is a sub\-directory to separate full images from thumbnails (if
used). For more info see \fItopics\-images\-thumbnails\fP\&.
.UNINDENT
.SS Additional features
.SS Image expiration
.sp
The Image Pipeline avoids downloading images that were downloaded recently. To
adjust this retention delay use the \fBIMAGES_EXPIRES\fP setting, which
specifies the delay in number of days:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
# 90 days of delay for image expiration
IMAGES_EXPIRES = 90
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Thumbnail generation
.sp
The Images Pipeline can automatically create thumbnails of the downloaded
images.
.sp
In order use this feature, you must set \fBIMAGES_THUMBS\fP to a dictionary
where the keys are the thumbnail names and the values are their dimensions.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
IMAGES_THUMBS = {
    \(aqsmall\(aq: (50, 50),
    \(aqbig\(aq: (270, 270),
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Where:
.INDENT 0.0
.IP \(bu 2
\fB<size_name>\fP is the one specified in the \fBIMAGES_THUMBS\fP
dictionary keys (\fBsmall\fP, \fBbig\fP, etc)
.IP \(bu 2
\fB<image_id>\fP is the \fI\%SHA1 hash\fP of the image url
.UNINDENT
.sp
Example of image files stored using \fBsmall\fP and \fBbig\fP thumbnail names:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<IMAGES_STORE>/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
<IMAGES_STORE>/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
<IMAGES_STORE>/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The first one is the full image, as downloaded from the site.
.SS Filtering out small images
.sp
You can drop images which are too small, by specifying the minimum allowed size
in the \fBIMAGES_MIN_HEIGHT\fP and \fBIMAGES_MIN_WIDTH\fP settings.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Note: these size constraints don\(aqt affect thumbnail generation at all.
.sp
By default, there are no size constraints, so all images are processed.
.SS Implementing your custom Images Pipeline
.sp
Here are the methods that you should override in your custom Images Pipeline:
.INDENT 0.0
.TP
.B class scrapy.contrib.pipeline.images.ImagesPipeline
.INDENT 7.0
.TP
.B get_media_requests(item, info)
As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you must override the
\fBget_media_requests()\fP method and return a Request for each
image URL:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
def get_media_requests(self, item, info):
    for image_url in item[\(aqimage_urls\(aq]:
        yield Request(image_url)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
\fBitem_completed()\fP method, as a list of 2\-element tuples.
Each tuple will contain \fB(success, image_info_or_failure)\fP where:
.INDENT 7.0
.IP \(bu 2
\fBsuccess\fP is a boolean which is \fBTrue\fP if the image was downloaded
successfully or \fBFalse\fP if it failed for some reason
.IP \(bu 2
\fBimage_info_or_error\fP is a dict containing the following keys (if success
is \fBTrue\fP) or a \fI\%Twisted Failure\fP if there was a problem.
.INDENT 2.0
.IP \(bu 2
\fBurl\fP \- the url where the image was downloaded from. This is the url of
the request returned from the \fBget_media_requests()\fP
method.
.IP \(bu 2
\fBpath\fP \- the path (relative to \fBIMAGES_STORE\fP) where the image
was stored
.IP \(bu 2
\fBchecksum\fP \- a \fI\%MD5 hash\fP of the image contents
.UNINDENT
.UNINDENT
.sp
The list of tuples received by \fBitem_completed()\fP is
guaranteed to retain the same order of the requests returned from the
\fBget_media_requests()\fP method.
.sp
Here\(aqs a typical value of the \fBresults\fP argument:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
[(True,
  {\(aqchecksum\(aq: \(aq2b00042f7481c7b056c4b410d28f33cf\(aq,
   \(aqpath\(aq: \(aqfull/7d97e98f8af710c7e7fe703abc8f639e0ee507c4.jpg\(aq,
   \(aqurl\(aq: \(aqhttp://www.example.com/images/product1.jpg\(aq}),
 (True,
  {\(aqchecksum\(aq: \(aqb9628c4ab9b595f72f280b90c4fd093d\(aq,
   \(aqpath\(aq: \(aqfull/1ca5879492b8fd606df1964ea3c1e2f4520f076f.jpg\(aq,
   \(aqurl\(aq: \(aqhttp://www.example.com/images/product2.jpg\(aq}),
 (False,
  Failure(...))]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
By default the \fBget_media_requests()\fP method returns \fBNone\fP which
means there are no images to download for the item.
.UNINDENT
.INDENT 7.0
.TP
.B item_completed(results, items, info)
The \fBImagesPipeline.item_completed()\fP method called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).
.sp
The \fBitem_completed()\fP method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.
.sp
Here is an example of the \fBitem_completed()\fP method where we
store the downloaded image paths (passed in results) in the \fBimage_paths\fP
item field, and we drop the item if it doesn\(aqt contain any images:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    image_paths = [x[\(aqpath\(aq] for ok, x in results if ok]
    if not image_paths:
        raise DropItem("Item contains no images")
    item[\(aqimage_paths\(aq] = image_paths
    return item
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
By default, the \fBitem_completed()\fP method returns the item.
.UNINDENT
.UNINDENT
.SS Custom Images pipeline example
.sp
Here is a full example of the Images Pipeline whose methods are examplified
above:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.pipeline.images import ImagesPipeline
from scrapy.exceptions import DropItem
from scrapy.http import Request

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item[\(aqimage_urls\(aq]:
            yield Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x[\(aqpath\(aq] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        item[\(aqimage_paths\(aq] = image_paths
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Ubuntu packages
.sp
New in version 0.10.

.sp
\fI\%Scrapinghub\fP publishes apt\-gettable packages which are generally fresher than
those in Ubuntu, and more stable too since they\(aqre continuously built from
\fI\%Github repo\fP (master & stable branches) and so they contain the latest bug
fixes.
.sp
To use the packages, just add the following line to your
\fB/etc/apt/sources.list\fP, and then run \fBaptitude update\fP and
\fBapt\-get install scrapy\-0.18\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
deb http://archive.scrapy.org/ubuntu DISTRO main
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Replacing \fBDISTRO\fP with the name of your Ubuntu release, which you can get
with command:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
lsb_release \-cs
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Supported Ubuntu releases are: \fBprecise\fP, \fBquantal\fP, \fBraring\fP\&.
.sp
For Ubuntu Raring (13.04):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
deb http://archive.scrapy.org/ubuntu raring main
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For Ubuntu Quantal (12.10):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
deb http://archive.scrapy.org/ubuntu quantal main
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For Ubuntu Precise (12.04):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
deb http://archive.scrapy.org/ubuntu precise main
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBWARNING:\fP
.INDENT 0.0
.INDENT 3.5
Please note that these packages are updated frequently, and so if
you find you can\(aqt download the packages, try updating your apt package
lists first, e.g., with \fBapt\-get update\fP or \fBaptitude update\fP\&.
.UNINDENT
.UNINDENT
.sp
The public GPG key used to sign these packages can be imported into you APT
keyring as follows:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
curl \-s http://archive.scrapy.org/ubuntu/archive.key | sudo apt\-key add \-
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Scrapyd
.sp
Scrapyd has been moved into a separate project.
.sp
Its documentation is now hosted at:
.INDENT 0.0
.INDENT 3.5
\fI\%http://scrapyd.readthedocs.org/\fP
.UNINDENT
.UNINDENT
.SS AutoThrottle extension
.sp
This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.
.SS Design goals
.INDENT 0.0
.IP 1. 3
be nicer to sites instead of using default download delay of zero
.IP 2. 3
automatically adjust scrapy to the optimum crawling speed, so the user
doesn\(aqt have to tune the download delays and concurrent requests to find the
optimum one. the user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.
.UNINDENT
.SS How it works
.sp
In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.
.sp
Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.
.SS Throttling algorithm
.sp
This adjusts download delays and concurrency based on the following rules:
.INDENT 0.0
.IP 1. 3
spiders always start with one concurrent request and a download delay of
\fBAUTOTHROTTLE_START_DELAY\fP
.IP 2. 3
when a response is received, the download delay is adjusted to the
average of previous download delay and the latency of the response.
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will never set a download delay
lower than \fBDOWNLOAD_DELAY\fP or a concurrency higher than \fBCONCURRENT_REQUESTS_PER_DOMAIN\fP (or \fBCONCURRENT_REQUESTS_PER_IP\fP, depending on which one you use).
.UNINDENT
.UNINDENT
.SS Settings
.sp
The settings used to control the AutoThrottle extension are:
.INDENT 0.0
.IP \(bu 2
\fBAUTOTHROTTLE_ENABLED\fP
.IP \(bu 2
\fBAUTOTHROTTLE_START_DELAY\fP
.IP \(bu 2
\fBAUTOTHROTTLE_MAX_DELAY\fP
.IP \(bu 2
\fBAUTOTHROTTLE_DEBUG\fP
.IP \(bu 2
\fBCONCURRENT_REQUESTS_PER_DOMAIN\fP
.IP \(bu 2
\fBCONCURRENT_REQUESTS_PER_IP\fP
.IP \(bu 2
\fBDOWNLOAD_DELAY\fP
.UNINDENT
.sp
For more information see \fIautothrottle\-algorithm\fP\&.
.SS AUTOTHROTTLE_ENABLED
.sp
Default: \fBFalse\fP
.sp
Enables the AutoThrottle extension.
.SS AUTOTHROTTLE_START_DELAY
.sp
Default: \fB5.0\fP
.sp
The initial download delay (in seconds).
.SS AUTOTHROTTLE_MAX_DELAY
.sp
Default: \fB60.0\fP
.sp
The maximum download delay (in seconds) to be set in case of high latencies.
.SS AUTOTHROTTLE_DEBUG
.sp
Default: \fBFalse\fP
.sp
Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.
.SS Benchmarking
.sp
New in version 0.17.

.sp
Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.
.sp
To run it use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy bench
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You should see an output like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
2013\-05\-16 13:08:46\-0300 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)
2013\-05\-16 13:08:47\-0300 [follow] INFO: Spider opened
2013\-05\-16 13:08:47\-0300 [follow] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:48\-0300 [follow] INFO: Crawled 74 pages (at 4440 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:49\-0300 [follow] INFO: Crawled 143 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:50\-0300 [follow] INFO: Crawled 210 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:51\-0300 [follow] INFO: Crawled 274 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:52\-0300 [follow] INFO: Crawled 343 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:53\-0300 [follow] INFO: Crawled 410 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:54\-0300 [follow] INFO: Crawled 474 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:55\-0300 [follow] INFO: Crawled 538 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:56\-0300 [follow] INFO: Crawled 602 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:57\-0300 [follow] INFO: Closing spider (closespider_timeout)
2013\-05\-16 13:08:57\-0300 [follow] INFO: Crawled 666 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013\-05\-16 13:08:57\-0300 [follow] INFO: Dumping Scrapy stats:
    {\(aqdownloader/request_bytes\(aq: 231508,
     \(aqdownloader/request_count\(aq: 682,
     \(aqdownloader/request_method_count/GET\(aq: 682,
     \(aqdownloader/response_bytes\(aq: 1172802,
     \(aqdownloader/response_count\(aq: 682,
     \(aqdownloader/response_status_count/200\(aq: 682,
     \(aqfinish_reason\(aq: \(aqclosespider_timeout\(aq,
     \(aqfinish_time\(aq: datetime.datetime(2013, 5, 16, 16, 8, 57, 985539),
     \(aqlog_count/INFO\(aq: 14,
     \(aqrequest_depth_max\(aq: 34,
     \(aqresponse_received_count\(aq: 682,
     \(aqscheduler/dequeued\(aq: 682,
     \(aqscheduler/dequeued/memory\(aq: 682,
     \(aqscheduler/enqueued\(aq: 12767,
     \(aqscheduler/enqueued/memory\(aq: 12767,
     \(aqstart_time\(aq: datetime.datetime(2013, 5, 16, 16, 8, 47, 676539)}
2013\-05\-16 13:08:57\-0300 [follow] INFO: Spider closed (closespider_timeout)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
That tells you that Scrapy is able to crawl about 3900 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it\(aqs written.
.sp
In the future, more cases will be added to the benchmarking suite to cover
other common scenarios.
.SS Jobs: pausing and resuming crawls
.sp
Sometimes, for big sites, it\(aqs desirable to pause crawls and be able to resume
them later.
.sp
Scrapy supports this functionality out of the box by providing the following
facilities:
.INDENT 0.0
.IP \(bu 2
a scheduler that persists scheduled requests on disk
.IP \(bu 2
a duplicates filter that persists visited requests on disk
.IP \(bu 2
an extension that keeps some spider state (key/value pairs) persistent
between batches
.UNINDENT
.SS Job directory
.sp
To enable persistence support you just need to define a \fIjob directory\fP through
the \fBJOBDIR\fP setting. This directory will be for storing all required data to
keep the state of a single job (ie. a spider run).  It\(aqs important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it\(aqs meant to be used for storing the state of
a \fIsingle\fP job.
.SS How to use it
.sp
To start a spider with persistence supported enabled, run it like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl somespider \-s JOBDIR=crawls/somespider\-1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then, you can stop the spider safely at any time (by pressing Ctrl\-C or sending
a signal), and resume it later by issuing the same command:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl somespider \-s JOBDIR=crawls/somespider\-1
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Keeping persistent state between batches
.sp
Sometimes you\(aqll want to keep some persistent spider state between pause/resume
batches. You can use the \fBspider.state\fP attribute for that, which should be a
dict. There\(aqs a built\-in extension that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.
.sp
Here\(aqs an example of a callback that uses the spider state (other spider code
is omitted for brevity):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_item(self, response):
    # parse item here
    self.state[\(aqitems_count\(aq] = self.state.get(\(aqitems_count\(aq, 0) + 1
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Persistence gotchas
.sp
There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:
.SS Cookies expiration
.sp
Cookies may expire. So, if you don\(aqt resume your spider quickly the requests
scheduled may no longer work. This won\(aqt be an issue if you spider doesn\(aqt rely
on cookies.
.SS Request serialization
.sp
Requests must be serializable by the \fIpickle\fP module, in order for persistence
to work, so you should make sure that your requests are serializable.
.sp
The most common issue here is to use \fBlambda\fP functions on request callbacks that
can\(aqt be persisted.
.sp
So, for example, this won\(aqt work:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def some_callback(self, response):
    somearg = \(aqtest\(aq
    return Request(\(aqhttp://www.example.com\(aq, callback=lambda r: self.other_callback(r, somearg))

def other_callback(self, response, somearg):
    print "the argument passed is:", somearg
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
But this will:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def some_callback(self, response):
    somearg = \(aqtest\(aq
    return Request(\(aqhttp://www.example.com\(aq, meta={\(aqsomearg\(aq: somearg})

def other_callback(self, response):
    somearg = response.meta[\(aqsomearg\(aq]
    print "the argument passed is:", somearg
.ft P
.fi
.UNINDENT
.UNINDENT
.SS DjangoItem
.sp
\fBDjangoItem\fP is a class of item that gets its fields definition from a
Django model, you simply create a \fBDjangoItem\fP and specify what Django
model it relates to.
.sp
Besides of getting the model fields defined on your item, \fBDjangoItem\fP
provides a method to create and populate a Django model instance with the item
data.
.SS Using DjangoItem
.sp
\fBDjangoItem\fP works much like ModelForms in Django, you create a subclass
and define its \fBdjango_model\fP attribute to be a valid Django model. With this
you will get an item with a field for each Django model field.
.sp
In addition, you can define fields that aren\(aqt present in the model and even
override fields that are present in the model defining them in the item.
.sp
Let\(aqs see some examples:
.sp
Creating a Django model for the examples:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from django.db import models

class Person(models.Model):
    name = models.CharField(max_length=255)
    age = models.IntegerField()
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Defining a basic \fBDjangoItem\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.djangoitem import DjangoItem

class PersonItem(DjangoItem):
    django_model = Person
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBDjangoItem\fP work just like \fBItem\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> p = PersonItem()
>>> p[\(aqname\(aq] = \(aqJohn\(aq
>>> p[\(aqage\(aq] = \(aq22\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To obtain the Django model from the item, we call the extra method
\fBsave()\fP of the \fBDjangoItem\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> person = p.save()
>>> person.name
\(aqJohn\(aq
>>> person.age
\(aq22\(aq
>>> person.id
1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The model is already saved when we call \fBsave()\fP, we
can prevent this by calling it with \fBcommit=False\fP\&. We can use
\fBcommit=False\fP in \fBsave()\fP method to obtain an unsaved model:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> person = p.save(commit=False)
>>> person.name
\(aqJohn\(aq
>>> person.age
\(aq22\(aq
>>> person.id
None
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As said before, we can add other fields to the item:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class PersonItem(DjangoItem):
    django_model = Person
    sex = Field()
.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> p = PersonItem()
>>> p[\(aqname\(aq] = \(aqJohn\(aq
>>> p[\(aqage\(aq] = \(aq22\(aq
>>> p[\(aqsex\(aq] = \(aqM\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
fields added to the item won\(aqt be taken into account when doing a \fBsave()\fP
.UNINDENT
.UNINDENT
.sp
And we can override the fields of the model with your own:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class PersonItem(DjangoItem):
    django_model = Person
    name = Field(default=\(aqNo Name\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This is useful to provide properties to the field, like a default or any other
property that your project uses.
.SS DjangoItem caveats
.sp
DjangoItem is a rather convenient way to integrate Scrapy projects with Django
models, but bear in mind that Django ORM may not scale well if you scrape a lot
of items (ie. millions) with Scrapy. This is because a relational backend is
often not a good choice for a write intensive application (such as a web
crawler), specially if the database is highly normalized and with many indices.
.SS Django settings set up
.sp
To use the Django models outside the Django application you need to set up the
\fBDJANGO_SETTINGS_MODULE\fP environment variable and \-\-in most cases\-\- modify
the \fBPYTHONPATH\fP environment variable to be able to import the settings
module.
.sp
There are many ways to do this depending on your use case and preferences.
Below is detailed one of the simplest ways to do it.
.sp
Suppose your Django project is named \fBmysite\fP, is located in the path
\fB/home/projects/mysite\fP and you have created an app \fBmyapp\fP with the model
\fBPerson\fP\&. That means your directory structure is something like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
/home/projects/mysite
├── manage.py
├── myapp
│\ \  ├── __init__.py
│\ \  ├── models.py
│\ \  ├── tests.py
│\ \  └── views.py
└── mysite
    ├── __init__.py
    ├── settings.py
    ├── urls.py
    └── wsgi.py
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Then you need to add \fB/home/projects/mysite\fP to the \fBPYTHONPATH\fP
environment variable and set up the environment variable
\fBDJANGO_SETTINGS_MODULE\fP to \fBmysite.settings\fP\&. That can be done in your
Scrapy\(aqs settings file by adding the lines below:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import sys
sys.path.append(\(aq/home/projects/mysite\(aq)

import os
os.environ[\(aqDJANGO_SETTINGS_MODULE\(aq] = \(aqmysite.settings\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Notice that we modify the \fBsys.path\fP variable instead the \fBPYTHONPATH\fP
environment variable as we are already within the python runtime. If everything
is right, you should be able to start the \fBscrapy shell\fP command and import
the model \fBPerson\fP (i.e. \fBfrom myapp.models import Person\fP).
.INDENT 0.0
.TP
.B \fBfaq\fP
Get answers to most frequently asked questions.
.TP
.B \fBtopics/debug\fP
Learn how to debug common problems of your scrapy spider.
.TP
.B \fBtopics/contracts\fP
Learn how to use contracts for testing your spiders.
.TP
.B \fBtopics/practices\fP
Get familiar with some Scrapy common practices.
.TP
.B \fBtopics/broad\-crawls\fP
Tune Scrapy for crawling a lot domains in parallel.
.TP
.B \fBtopics/firefox\fP
Learn how to scrape with Firefox and some useful add\-ons.
.TP
.B \fBtopics/firebug\fP
Learn how to scrape efficiently using Firebug.
.TP
.B \fBtopics/leaks\fP
Learn how to find and get rid of memory leaks in your crawler.
.TP
.B \fBtopics/images\fP
Download static images associated with your scraped items.
.TP
.B \fBtopics/ubuntu\fP
Install latest Scrapy packages easily on Ubuntu
.TP
.B \fBtopics/scrapyd\fP
Deploying your Scrapy project in production.
.TP
.B \fBtopics/autothrottle\fP
Adjust crawl rate dynamically based on load.
.TP
.B \fBtopics/benchmarking\fP
Check how Scrapy performs on your hardware.
.TP
.B \fBtopics/jobs\fP
Learn how to pause and resume crawls for large spiders.
.TP
.B \fBtopics/djangoitem\fP
Write scraped items using Django models.
.UNINDENT
.SH EXTENDING SCRAPY
.SS Architecture overview
.sp
This document describes the architecture of Scrapy and how its components
interact.
.SS Overview
.sp
The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the green arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.
[image: Scrapy architecture]
[image]
.SS Components
.SS Scrapy Engine
.sp
The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the Data
Flow section below for more details.
.SS Scheduler
.sp
The Scheduler receives requests from the engine and enqueues them for feeding
them later (also to the engine) when the engine requests them.
.SS Downloader
.sp
The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.
.SS Spiders
.sp
Spiders are custom classes written by Scrapy users to parse responses and
extract items (aka scraped items) from them or additional URLs (requests) to
follow. Each spider is able to handle a specific domain (or group of domains).
For more information see \fItopics\-spiders\fP\&.
.SS Item Pipeline
.sp
The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see \fItopics\-item\-pipeline\fP\&.
.SS Downloader middlewares
.sp
Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine. They provide
a convenient mechanism for extending Scrapy functionality by plugging custom
code. For more information see \fItopics\-downloader\-middleware\fP\&.
.SS Spider middlewares
.sp
Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests). They provide a convenient mechanism for extending Scrapy
functionality by plugging custom code. For more information see
\fItopics\-spider\-middleware\fP\&.
.SS Data flow
.sp
The data flow in Scrapy is controlled by the execution engine, and goes like
this:
.INDENT 0.0
.IP 1. 3
The Engine opens a domain, locates the Spider that handles that domain, and
asks the spider for the first URLs to crawl.
.IP 2. 3
The Engine gets the first URLs to crawl from the Spider and schedules them
in the Scheduler, as Requests.
.IP 3. 3
The Engine asks the Scheduler for the next URLs to crawl.
.IP 4. 3
The Scheduler returns the next URLs to crawl to the Engine and the Engine
sends them to the Downloader, passing through the Downloader Middleware
(request direction).
.IP 5. 3
Once the page finishes downloading the Downloader generates a Response (with
that page) and sends it to the Engine, passing through the Downloader
Middleware (response direction).
.IP 6. 3
The Engine receives the Response from the Downloader and sends it to the
Spider for processing, passing through the Spider Middleware (input direction).
.IP 7. 3
The Spider processes the Response and returns scraped Items and new Requests
(to follow) to the Engine.
.IP 8. 3
The Engine sends scraped Items (returned by the Spider) to the Item Pipeline
and Requests (returned by spider) to the Scheduler
.IP 9. 3
The process repeats (from step 2) until there are no more requests from the
Scheduler, and the Engine closes the domain.
.UNINDENT
.SS Event\-driven networking
.sp
Scrapy is written with \fI\%Twisted\fP, a popular event\-driven networking framework
for Python. Thus, it\(aqs implemented using a non\-blocking (aka asynchronous) code
for concurrency.
.sp
For more information about asynchronous programming and Twisted see these
links:
.INDENT 0.0
.IP \(bu 2
\fI\%Asynchronous Programming with Twisted\fP
.IP \(bu 2
\fI\%Twisted \- hello, asynchronous programming\fP
.UNINDENT
.SS Downloader Middleware
.sp
The downloader middleware is a framework of hooks into Scrapy\(aqs
request/response processing.  It\(aqs a light, low\-level system for globally
altering Scrapy\(aqs requests and responses.
.SS Activating a downloader middleware
.sp
To activate a downloader middleware component, add it to the
\fBDOWNLOADER_MIDDLEWARES\fP setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.
.sp
Here\(aqs an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
DOWNLOADER_MIDDLEWARES = {
    \(aqmyproject.middlewares.CustomDownloaderMiddleware\(aq: 543,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBDOWNLOADER_MIDDLEWARES\fP setting is merged with the
\fBDOWNLOADER_MIDDLEWARES_BASE\fP setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the downloader.
.sp
To decide which order to assign to your middleware see the
\fBDOWNLOADER_MIDDLEWARES_BASE\fP setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.
.sp
If you want to disable a built\-in middleware (the ones defined in
\fBDOWNLOADER_MIDDLEWARES_BASE\fP and enabled by default) you must define it
in your project\(aqs \fBDOWNLOADER_MIDDLEWARES\fP setting and assign \fINone\fP
as its value.  For example, if you want to disable the off\-site middleware:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
DOWNLOADER_MIDDLEWARES = {
    \(aqmyproject.middlewares.CustomDownloaderMiddleware\(aq: 543,
    \(aqscrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware\(aq: None,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.
.SS Writing your own downloader middleware
.sp
Writing your own downloader middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.DownloaderMiddleware
.INDENT 7.0
.TP
.B process_request(request, spider)
This method is called for each request that goes through the download
middleware.
.sp
\fBprocess_request()\fP should either: return \fBNone\fP, return a
\fBResponse\fP object, return a \fBRequest\fP
object, or raise \fBIgnoreRequest\fP\&.
.sp
If it returns \fBNone\fP, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).
.sp
If it returns a \fBResponse\fP object, Scrapy won\(aqt bother
calling \fIany\fP other \fBprocess_request()\fP or \fBprocess_exception()\fP methods,
or the appropriate download function; it\(aqll return that response. The \fBprocess_response()\fP
methods of installed middleware is always called on every response.
.sp
If it returns a \fBRequest\fP object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.
.sp
If it raises an \fBIgnoreRequest\fP exception, the
\fBprocess_exception()\fP methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(\fBRequest.errback\fP) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBrequest\fP (\fBRequest\fP object) \-\- the request being processed
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which this request is intended
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B process_response(request, response, spider)
\fBprocess_response()\fP should either: return a \fBResponse\fP
object, return a \fBRequest\fP object or
raise a \fBIgnoreRequest\fP exception.
.sp
If it returns a \fBResponse\fP (it could be the same given
response, or a brand\-new one), that response will continue to be processed
with the \fBprocess_response()\fP of the next middleware in the chain.
.sp
If it returns a \fBRequest\fP object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from \fBprocess_request()\fP\&.
.sp
If it raises an \fBIgnoreRequest\fP exception, the errback
function of the request (\fBRequest.errback\fP) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBrequest\fP (is a \fBRequest\fP object) \-\- the request that originated the response
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response being processed
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which this response is intended
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B process_exception(request, exception, spider)
Scrapy calls \fBprocess_exception()\fP when a download handler
or a \fBprocess_request()\fP (from a downloader middleware) raises an
exception (including an \fBIgnoreRequest\fP exception)
.sp
\fBprocess_exception()\fP should return: either \fBNone\fP,
a \fBResponse\fP object, or a \fBRequest\fP object.
.sp
If it returns \fBNone\fP, Scrapy will continue processing this exception,
executing any other \fBprocess_exception()\fP methods of installed middleware,
until no middleware is left and the default exception handling kicks in.
.sp
If it returns a \fBResponse\fP object, the \fBprocess_response()\fP
method chain of installed middleware is started, and Scrapy won\(aqt bother calling
any other \fBprocess_exception()\fP methods of middleware.
.sp
If it returns a \fBRequest\fP object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
\fBprocess_exception()\fP methods of the middleware the same as returning a
response would.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBrequest\fP (is a \fBRequest\fP object) \-\- the request that generated the exception
.IP \(bu 2
\fBexception\fP (an \fBException\fP object) \-\- the raised exception
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which this request is intended
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Built\-in downloader middleware reference
.sp
This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the \fIdownloader middleware usage guide\fP\&.
.sp
For a list of the components enabled by default (and their orders) see the
\fBDOWNLOADER_MIDDLEWARES_BASE\fP setting.
.SS CookiesMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware
This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.
.UNINDENT
.sp
The following settings can be used to configure the cookie middleware:
.INDENT 0.0
.IP \(bu 2
\fBCOOKIES_ENABLED\fP
.IP \(bu 2
\fBCOOKIES_DEBUG\fP
.UNINDENT
.SS Multiple cookie sessions per spider
.sp
New in version 0.15.

.sp
There is support for keeping multiple cookie sessions per spider by using the
\fBcookiejar\fP Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
for i, url in enumerate(urls):
    yield Request("http://www.example.com", meta={\(aqcookiejar\(aq: i},
        callback=self.parse_page)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Keep in mind that the \fBcookiejar\fP meta key is not "sticky". You need to keep
passing it along on subsequent requests. For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_page(self, response):
    # do some processing
    return Request("http://www.example.com/otherpage",
        meta={\(aqcookiejar\(aq: response.meta[\(aqcookiejar\(aq]},
        callback=self.parse_other_page)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS COOKIES_ENABLED
.sp
Default: \fBTrue\fP
.sp
Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.
.SS COOKIES_DEBUG
.sp
Default: \fBFalse\fP
.sp
If enabled, Scrapy will log all cookies sent in requests (ie. \fBCookie\fP
header) and all cookies received in responses (ie. \fBSet\-Cookie\fP header).
.sp
Here\(aqs an example of a log with \fBCOOKIES_DEBUG\fP enabled:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
2011\-04\-06 14:35:10\-0300 [diningcity] INFO: Spider opened
2011\-04\-06 14:35:10\-0300 [diningcity] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>
        Cookie: clientlanguage_nl=en_EN
2011\-04\-06 14:35:14\-0300 [diningcity] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>
        Set\-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set\-Cookie: ip_isocode=US
        Set\-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07\-Apr\-2011 21:21:34 GMT; Path=/
2011\-04\-06 14:49:50\-0300 [diningcity] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)
[...]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS DefaultHeadersMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware
This middleware sets all default requests headers specified in the
\fBDEFAULT_REQUEST_HEADERS\fP setting.
.UNINDENT
.SS DownloadTimeoutMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware
This middleware sets the download timeout for requests specified in the
\fBDOWNLOAD_TIMEOUT\fP setting.
.UNINDENT
.SS HttpAuthMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware
This middleware authenticates all requests generated from certain spiders
using \fI\%Basic access authentication\fP (aka. HTTP auth).
.sp
To enable HTTP authentication from certain spiders, set the \fBhttp_user\fP
and \fBhttp_pass\fP attributes of those spiders.
.sp
Example:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
class SomeIntranetSiteSpider(CrawlSpider):

    http_user = \(aqsomeuser\(aq
    http_pass = \(aqsomepass\(aq
    name = \(aqintranet.example.com\(aq

    # .. rest of the spider code omitted ...
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS HttpCacheMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware
This middleware provides low\-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.
.sp
Scrapy ships with two HTTP cache storage backends:
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fIhttpcache\-storage\-dbm\fP
.IP \(bu 2
\fIhttpcache\-storage\-fs\fP
.UNINDENT
.UNINDENT
.UNINDENT
.sp
You can change the HTTP cache storage backend with the \fBHTTPCACHE_STORAGE\fP
setting. Or you can also implement your own storage backend.
.sp
Scrapy ships with two HTTP cache policies:
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fIhttpcache\-policy\-rfc2616\fP
.IP \(bu 2
\fIhttpcache\-policy\-dummy\fP
.UNINDENT
.UNINDENT
.UNINDENT
.sp
You can change the HTTP cache policy with the \fBHTTPCACHE_POLICY\fP
setting. Or you can also implement your own policy.
.UNINDENT
.SS Dummy policy (default)
.sp
This policy has no awareness of any HTTP Cache\-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.
.sp
The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
"replay" a spider run \fIexactly as it ran before\fP\&.
.sp
In order to use this policy, set:
.INDENT 0.0
.IP \(bu 2
\fBHTTPCACHE_POLICY\fP to \fBscrapy.contrib.httpcache.DummyPolicy\fP
.UNINDENT
.SS RFC2616 policy
.sp
This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache\-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).
.sp
what is implemented:
.INDENT 0.0
.IP \(bu 2
Do not attempt to store responses/requests with \fIno\-store\fP cache\-control directive set
.IP \(bu 2
Do not serve responses from cache if \fIno\-cache\fP cache\-control directive is set even for fresh responses
.IP \(bu 2
Compute freshness lifetime from \fImax\-age\fP cache\-control directive
.IP \(bu 2
Compute freshness lifetime from \fIExpires\fP response header
.IP \(bu 2
Compute freshness lifetime from \fILast\-Modified\fP response header (heuristic used by Firefox)
.IP \(bu 2
Compute current age from \fIAge\fP response header
.IP \(bu 2
Compute current age from \fIDate\fP header
.IP \(bu 2
Revalidate stale responses based on \fILast\-Modified\fP response header
.IP \(bu 2
Revalidate stale responses based on \fIETag\fP response header
.IP \(bu 2
Set \fIDate\fP header for any received response missing it
.UNINDENT
.sp
what is missing:
.INDENT 0.0
.IP \(bu 2
\fIPragma: no\-cache\fP support \fI\%http://www.mnot.net/cache_docs/#PRAGMA\fP
.IP \(bu 2
\fIVary\fP header support \fI\%http://www.w3.org/Protocols/rfc2616/rfc2616\-sec13.html#sec13.6\fP
.IP \(bu 2
Invalidation after updates or deletes \fI\%http://www.w3.org/Protocols/rfc2616/rfc2616\-sec13.html#sec13.10\fP
.IP \(bu 2
\&... probably others ..
.UNINDENT
.sp
In order to use this policy, set:
.INDENT 0.0
.IP \(bu 2
\fBHTTPCACHE_POLICY\fP to \fBscrapy.contrib.httpcache.RFC2616Policy\fP
.UNINDENT
.SS DBM storage backend (default)
.sp
New in version 0.13.

.sp
A \fI\%DBM\fP storage backend is available for the HTTP cache middleware.
.sp
By default, it uses the \fI\%anydbm\fP module, but you can change it with the
\fBHTTPCACHE_DBM_MODULE\fP setting.
.sp
In order to use this storage backend, set:
.INDENT 0.0
.IP \(bu 2
\fBHTTPCACHE_STORAGE\fP to \fBscrapy.contrib.httpcache.DbmCacheStorage\fP
.UNINDENT
.SS Filesystem storage backend
.sp
A file system storage backend is also available for the HTTP cache middleware.
.sp
In order to use this storage backend, set:
.INDENT 0.0
.IP \(bu 2
\fBHTTPCACHE_STORAGE\fP to \fBscrapy.contrib.httpcache.FilesystemCacheStorage\fP
.UNINDENT
.sp
Each request/response pair is stored in a different directory containing
the following files:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBrequest_body\fP \- the plain request body
.IP \(bu 2
\fBrequest_headers\fP \- the request headers (in raw HTTP format)
.IP \(bu 2
\fBresponse_body\fP \- the plain response body
.IP \(bu 2
\fBresponse_headers\fP \- the request headers (in raw HTTP format)
.IP \(bu 2
\fBmeta\fP \- some metadata of this cache resource in Python \fBrepr()\fP format
(grep\-friendly format)
.IP \(bu 2
\fBpickled_meta\fP \- the same metadata in \fBmeta\fP but pickled for more
efficient deserialization
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The directory name is made from the request fingerprint (see
\fBscrapy.utils.request.fingerprint\fP), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
.ft P
.fi
.UNINDENT
.UNINDENT
.SS HTTPCache middleware settings
.sp
The \fBHttpCacheMiddleware\fP can be configured through the following
settings:
.SS HTTPCACHE_ENABLED
.sp
New in version 0.11.

.sp
Default: \fBFalse\fP
.sp
Whether the HTTP cache will be enabled.
.sp
Changed in version 0.11: Before 0.11, \fBHTTPCACHE_DIR\fP was used to enable cache.

.SS HTTPCACHE_EXPIRATION_SECS
.sp
Default: \fB0\fP
.sp
Expiration time for cached requests, in seconds.
.sp
Cached requests older than this time will be re\-downloaded. If zero, cached
requests will never expire.
.sp
Changed in version 0.11: Before 0.11, zero meant cached requests always expire.

.SS HTTPCACHE_DIR
.sp
Default: \fB\(aqhttpcache\(aq\fP
.sp
The directory to use for storing the (low\-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: \fItopics\-project\-structure\fP\&.
.SS HTTPCACHE_IGNORE_HTTP_CODES
.sp
New in version 0.10.

.sp
Default: \fB[]\fP
.sp
Don\(aqt cache response with these HTTP codes.
.SS HTTPCACHE_IGNORE_MISSING
.sp
Default: \fBFalse\fP
.sp
If enabled, requests not found in the cache will be ignored instead of downloaded.
.SS HTTPCACHE_IGNORE_SCHEMES
.sp
New in version 0.10.

.sp
Default: \fB[\(aqfile\(aq]\fP
.sp
Don\(aqt cache responses with these URI schemes.
.SS HTTPCACHE_STORAGE
.sp
Default: \fB\(aqscrapy.contrib.httpcache.DbmCacheStorage\(aq\fP
.sp
The class which implements the cache storage backend.
.SS HTTPCACHE_DBM_MODULE
.sp
New in version 0.13.

.sp
Default: \fB\(aqanydbm\(aq\fP
.sp
The database module to use in the \fIDBM storage backend\fP\&. This setting is specific to the DBM backend.
.SS HTTPCACHE_POLICY
.sp
New in version 0.18.

.sp
Default: \fB\(aqscrapy.contrib.httpcache.DummyPolicy\(aq\fP
.sp
The class which implements the cache policy.
.SS HttpCompressionMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware
This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.
.UNINDENT
.SS HttpCompressionMiddleware Settings
.SS COMPRESSION_ENABLED
.sp
Default: \fBTrue\fP
.sp
Whether the Compression middleware will be enabled.
.SS ChunkedTransferMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware
This middleware adds support for \fI\%chunked transfer encoding\fP
.UNINDENT
.SS HttpProxyMiddleware
.sp
New in version 0.8.

.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware
This middleware sets the HTTP proxy to use for requests, by setting the
\fBproxy\fP meta value to \fBRequest\fP objects.
.sp
Like the Python standard library modules \fI\%urllib\fP and \fI\%urllib2\fP, it obeys
the following environment variables:
.INDENT 7.0
.IP \(bu 2
\fBhttp_proxy\fP
.IP \(bu 2
\fBhttps_proxy\fP
.IP \(bu 2
\fBno_proxy\fP
.UNINDENT
.UNINDENT
.SS RedirectMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware
This middleware handles redirection of requests based on response status.
.UNINDENT
.sp
The urls which the request goes through (while being redirected) can be found
in the \fBredirect_urls\fP \fBRequest.meta\fP key.
.sp
The \fBRedirectMiddleware\fP can be configured through the following
settings (see the settings documentation for more info):
.INDENT 0.0
.IP \(bu 2
\fBREDIRECT_ENABLED\fP
.IP \(bu 2
\fBREDIRECT_MAX_TIMES\fP
.UNINDENT
.sp
If \fBRequest.meta\fP contains the
\fBdont_redirect\fP key, the request will be ignored by this middleware.
.SS RedirectMiddleware settings
.SS REDIRECT_ENABLED
.sp
New in version 0.13.

.sp
Default: \fBTrue\fP
.sp
Whether the Redirect middleware will be enabled.
.SS REDIRECT_MAX_TIMES
.sp
Default: \fB20\fP
.sp
The maximum number of redirections that will be follow for a single request.
.SS MetaRefreshMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware
This middleware handles redirection of requests based on meta\-refresh html tag.
.UNINDENT
.sp
The \fBMetaRefreshMiddleware\fP can be configured through the following
settings (see the settings documentation for more info):
.INDENT 0.0
.IP \(bu 2
\fBMETAREFRESH_ENABLED\fP
.IP \(bu 2
\fBMETAREFRESH_MAXDELAY\fP
.UNINDENT
.sp
This middleware obey \fBREDIRECT_MAX_TIMES\fP setting, \fBdont_redirect\fP
and \fBredirect_urls\fP request meta keys as described for \fBRedirectMiddleware\fP
.SS MetaRefreshMiddleware settings
.SS METAREFRESH_ENABLED
.sp
New in version 0.17.

.sp
Default: \fBTrue\fP
.sp
Whether the Meta Refresh middleware will be enabled.
.SS REDIRECT_MAX_METAREFRESH_DELAY
.sp
Default: \fB100\fP
.sp
The maximum meta\-refresh delay (in seconds) to follow the redirection.
.SS RetryMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.retry.RetryMiddleware
A middlware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.
.UNINDENT
.sp
Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.
.sp
The \fBRetryMiddleware\fP can be configured through the following
settings (see the settings documentation for more info):
.INDENT 0.0
.IP \(bu 2
\fBRETRY_ENABLED\fP
.IP \(bu 2
\fBRETRY_TIMES\fP
.IP \(bu 2
\fBRETRY_HTTP_CODES\fP
.UNINDENT
.sp
About HTTP errors to consider:
.sp
You may want to remove 400 from \fBRETRY_HTTP_CODES\fP, if you stick to the
HTTP protocol. It\(aqs included by default because it\(aqs a common code used
to indicate server overload, which would be something we want to retry.
.sp
If \fBRequest.meta\fP contains the \fBdont_retry\fP
key, the request will be ignored by this middleware.
.SS RetryMiddleware Settings
.SS RETRY_ENABLED
.sp
New in version 0.13.

.sp
Default: \fBTrue\fP
.sp
Whether the Retry middleware will be enabled.
.SS RETRY_TIMES
.sp
Default: \fB2\fP
.sp
Maximum number of times to retry, in addition to the first download.
.SS RETRY_HTTP_CODES
.sp
Default: \fB[500, 502, 503, 504, 400, 408]\fP
.sp
Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.
.SS RobotsTxtMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware
This middleware filters out requests forbidden by the robots.txt exclusion
standard.
.sp
To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the \fBROBOTSTXT_OBEY\fP setting is enabled.
.sp
\fBWARNING:\fP
.INDENT 7.0
.INDENT 3.5
Keep in mind that, if you crawl using multiple concurrent
requests per domain, Scrapy could still  download some forbidden pages
if they were requested before the robots.txt file was downloaded. This
is a known limitation of the current robots.txt middleware and will
be fixed in the future.
.UNINDENT
.UNINDENT
.UNINDENT
.SS DownloaderStats
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.stats.DownloaderStats
Middleware that stores stats of all requests, responses and exceptions that
pass through it.
.sp
To use this middleware you must enable the \fBDOWNLOADER_STATS\fP
setting.
.UNINDENT
.SS UserAgentMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware
Middleware that allows spiders to override the default user agent.
.sp
In order for a spider to override the default user agent, its \fIuser_agent\fP
attribute must be set.
.UNINDENT
.SS Spider Middleware
.sp
The spider middleware is a framework of hooks into Scrapy\(aqs spider processing
mechanism where you can plug custom functionality to process the requests that
are sent to \fItopics\-spiders\fP for processing and to process the responses
and items that are generated from spiders.
.SS Activating a spider middleware
.sp
To activate a spider middleware component, add it to the
\fBSPIDER_MIDDLEWARES\fP setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.
.sp
Here\(aqs an example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
SPIDER_MIDDLEWARES = {
    \(aqmyproject.middlewares.CustomSpiderMiddleware\(aq: 543,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBSPIDER_MIDDLEWARES\fP setting is merged with the
\fBSPIDER_MIDDLEWARES_BASE\fP setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider.
.sp
To decide which order to assign to your middleware see the
\fBSPIDER_MIDDLEWARES_BASE\fP setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.
.sp
If you want to disable a builtin middleware (the ones defined in
\fBSPIDER_MIDDLEWARES_BASE\fP, and enabled by default) you must define it
in your project \fBSPIDER_MIDDLEWARES\fP setting and assign \fINone\fP as its
value.  For example, if you want to disable the off\-site middleware:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
SPIDER_MIDDLEWARES = {
    \(aqmyproject.middlewares.CustomSpiderMiddleware\(aq: 543,
    \(aqscrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware\(aq: None,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.
.SS Writing your own spider middleware
.sp
Writing your own spider middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.SpiderMiddleware
.INDENT 7.0
.TP
.B process_spider_input(response, spider)
This method is called for each response that goes through the spider
middleware and into the spider, for processing.
.sp
\fBprocess_spider_input()\fP should return \fBNone\fP or raise an
exception.
.sp
If it returns \fBNone\fP, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.
.sp
If it raises an exception, Scrapy won\(aqt bother calling any other spider
middleware \fBprocess_spider_input()\fP and will call the request
errback.  The output of the errback is chained back in the other
direction for \fBprocess_spider_output()\fP to process it, or
\fBprocess_spider_exception()\fP if it raised an exception.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response being processed
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which this response is intended
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B process_spider_output(response, result, spider)
This method is called with the results returned from the Spider, after
it has processed the response.
.sp
\fBprocess_spider_output()\fP must return an iterable of
\fBRequest\fP or \fBItem\fP objects.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (class:\fI~scrapy.http.Response\fP object) \-\- the response which generated this output from the
spider
.IP \(bu 2
\fBresult\fP (an iterable of \fBRequest\fP or
\fBItem\fP objects) \-\- the result returned by the spider
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider whose result is being processed
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B process_spider_exception(response, exception, spider)
This method is called when when a spider or \fBprocess_spider_input()\fP
method (from other spider middleware) raises an exception.
.sp
\fBprocess_spider_exception()\fP should return either \fBNone\fP or an
iterable of \fBResponse\fP or
\fBItem\fP objects.
.sp
If it returns \fBNone\fP, Scrapy will continue processing this exception,
executing any other \fBprocess_spider_exception()\fP in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it\(aqs logged and discarded).
.sp
If it returns an iterable the \fBprocess_spider_output()\fP pipeline
kicks in, and no other \fBprocess_spider_exception()\fP will be called.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response being processed when the exception was
raised
.IP \(bu 2
\fBexception\fP (\fI\%Exception\fP object) \-\- the exception raised
.IP \(bu 2
\fBspider\fP (\fBscrapy.spider.Spider\fP object) \-\- the spider which raised the exception
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B process_start_requests(start_requests, spider)
New in version 0.15.

.sp
This method is called with the start requests of the spider, and works
similarly to the \fBprocess_spider_output()\fP method, except that it
doesn\(aqt have a response associated and must return only requests (not
items).
.sp
It receives an iterable (in the \fBstart_requests\fP parameter) and must
return another iterable of \fBRequest\fP objects.
.sp
\fBNOTE:\fP
.INDENT 7.0
.INDENT 3.5
When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all \fBstart_requests\fP iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBstart_requests\fP (an iterable of \fBRequest\fP) \-\- the start requests
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider to whom the start requests belong
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Built\-in spider middleware reference
.sp
This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the \fIspider middleware usage guide\fP\&.
.sp
For a list of the components enabled by default (and their orders) see the
\fBSPIDER_MIDDLEWARES_BASE\fP setting.
.SS DepthMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.depth.DepthMiddleware
DepthMiddleware is a scrape middleware used for tracking the depth of each
Request inside the site being scraped. It can be used to limit the maximum
depth to scrape or things like that.
.sp
The \fBDepthMiddleware\fP can be configured through the following
settings (see the settings documentation for more info):
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBDEPTH_LIMIT\fP \- The maximum depth that will be allowed to
crawl for any site. If zero, no limit will be imposed.
.IP \(bu 2
\fBDEPTH_STATS\fP \- Whether to collect depth stats.
.IP \(bu 2
\fBDEPTH_PRIORITY\fP \- Whether to prioritize the requests based on
their depth.
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS HttpErrorMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware
Filter out unsuccessful (erroneous) HTTP responses so that spiders don\(aqt
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.
.UNINDENT
.sp
According to the \fI\%HTTP standard\fP, successful responses are those whose
status codes are in the 200\-300 range.
.sp
If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
\fBhandle_httpstatus_list\fP spider attribute or
\fBHTTPERROR_ALLOWED_CODES\fP setting.
.sp
For example, if you want your spider to handle 404 responses you can do
this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MySpider(CrawlSpider):
    handle_httpstatus_list = [404]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBhandle_httpstatus_list\fP key of \fBRequest.meta\fP can also be used to specify which response codes to
allow on a per\-request basis.
.sp
Keep in mind, however, that it\(aqs usually a bad idea to handle non\-200
responses, unless you really know what you\(aqre doing.
.sp
For more information see: \fI\%HTTP Status Code Definitions\fP\&.
.SS HttpErrorMiddleware settings
.SS HTTPERROR_ALLOWED_CODES
.sp
Default: \fB[]\fP
.sp
Pass all responses with non\-200 status codes contained in this list.
.SS HTTPERROR_ALLOW_ALL
.sp
Default: \fBFalse\fP
.sp
Pass all responses, regardless of its status code.
.SS OffsiteMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware
Filters out Requests for URLs outside the domains covered by the spider.
.sp
This middleware filters out every request whose host names aren\(aqt in the
spider\(aqs \fBallowed_domains\fP attribute.
.sp
When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
DEBUG: Filtered offsite request to \(aqwww.othersite.com\(aq: <GET http://www.othersite.com/some/page.html>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for \fBwww.othersite.com\fP is filtered, no log message will be
printed. But if a request for \fBsomeothersite.com\fP is filtered, a message
will be printed (but only for the first request filtered).
.sp
If the spider doesn\(aqt define an
\fBallowed_domains\fP attribute, or the
attribute is empty, the offsite middleware will allow all requests.
.sp
If the request has the \fBdont_filter\fP attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.
.UNINDENT
.SS RefererMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.referer.RefererMiddleware
Populates Request \fBReferer\fP header, based on the URL of the Response which
generated it.
.UNINDENT
.SS RefererMiddleware settings
.SS REFERER_ENABLED
.sp
New in version 0.15.

.sp
Default: \fBTrue\fP
.sp
Whether to enable referer middleware.
.SS UrlLengthMiddleware
.INDENT 0.0
.TP
.B class scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware
Filters out requests with URLs longer than URLLENGTH_LIMIT
.sp
The \fBUrlLengthMiddleware\fP can be configured through the following
settings (see the settings documentation for more info):
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
\fBURLLENGTH_LIMIT\fP \- The maximum URL length to allow for crawled URLs.
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Extensions
.sp
The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.
.sp
Extensions are just regular classes that are instantiated at Scrapy startup,
when extensions are initialized.
.SS Extension settings
.sp
Extensions use the \fIScrapy settings\fP to manage their
settings, just like any other Scrapy code.
.sp
It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, an
hypothetic extension to handle \fI\%Google Sitemaps\fP would use settings like
\fIGOOGLESITEMAP_ENABLED\fP, \fIGOOGLESITEMAP_DEPTH\fP, and so on.
.SS Loading & activating extensions
.sp
Extensions are loaded and activated at startup by instantiating a single
instance of the extension class. Therefore, all the extension initialization
code must be performed in the class constructor (\fB__init__\fP method).
.sp
To make an extension available, add it to the \fBEXTENSIONS\fP setting in
your Scrapy settings. In \fBEXTENSIONS\fP, each extension is represented
by a string: the full Python path to the extension\(aqs class name. For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
EXTENSIONS = {
    \(aqscrapy.contrib.corestats.CoreStats\(aq: 500,
    \(aqscrapy.webservice.WebService\(aq: 500,
    \(aqscrapy.telnet.TelnetConsole\(aq: 500,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
As you can see, the \fBEXTENSIONS\fP setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension \fIloading\fP order. Extensions orders are not as important as middleware
orders though, and they are typically irrelevant, ie. it doesn\(aqt matter in
which order the extensions are loaded because they don\(aqt depend on each other
[1].
.sp
However, this feature can be exploited if you need to add an extension which
depends on other extensions already loaded.
.sp
[1] This is is why the \fBEXTENSIONS_BASE\fP setting in Scrapy (which
contains all built\-in extensions enabled by default) defines all the extensions
with the same order (\fB500\fP).
.SS Available, enabled and disabled extensions
.sp
Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the \fBHTTPCACHE_ENABLED\fP setting is set.
.SS Disabling an extension
.sp
In order to disable an extension that comes enabled by default (ie. those
included in the \fBEXTENSIONS_BASE\fP setting) you must set its order to
\fBNone\fP\&. For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
EXTENSIONS = {
    \(aqscrapy.contrib.corestats.CoreStats\(aq: None,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Writing your own extension
.sp
Writing your own extension is easy. Each extension is a single Python class
which doesn\(aqt need to implement any particular method.
.sp
The main entry point for a Scrapy extension (this also includes middlewares and
pipelines) is the \fBfrom_crawler\fP class method which receives a
\fBCrawler\fP instance which is the main object controlling the Scrapy crawler.
Through that object you can access settings, signals, stats, and also control
the crawler behaviour, if your extension needs to such thing.
.sp
Typically, extensions connect to \fIsignals\fP and perform
tasks triggered by them.
.sp
Finally, if the \fBfrom_crawler\fP method raises the
\fBNotConfigured\fP exception, the extension will be
disabled. Otherwise, the extension will be enabled.
.SS Sample extension
.sp
Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:
.INDENT 0.0
.IP \(bu 2
a spider is opened
.IP \(bu 2
a spider is closed
.IP \(bu 2
a specific number of items are scraped
.UNINDENT
.sp
The extension will be enabled through the \fBMYEXT_ENABLED\fP setting and the
number of items will be specified through the \fBMYEXT_ITEMCOUNT\fP setting.
.sp
Here is the code of such extension:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import signals
from scrapy.exceptions import NotConfigured

class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise
        # NotConfigured otherwise
        if not crawler.settings.getbool(\(aqMYEXT_ENABLED\(aq):
            raise NotConfigured

        # get the number of items from settings
        item_count = crawler.settings.getint(\(aqMYEXT_ITEMCOUNT\(aq, 1000)

        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        spider.log("opened spider %s" % spider.name)

    def spider_closed(self, spider):
        spider.log("closed spider %s" % spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped == self.item_count:
            spider.log("scraped %d items, resetting counter" % self.items_scraped)
            self.item_count = 0
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Built\-in extensions reference
.SS General purpose extensions
.SS Log Stats extension
.INDENT 0.0
.TP
.B class scrapy.contrib.logstats.LogStats
.UNINDENT
.sp
Log basic stats like crawled pages and scraped items.
.SS Core Stats extension
.INDENT 0.0
.TP
.B class scrapy.contrib.corestats.CoreStats
.UNINDENT
.sp
Enable the collection of core statistics, provided the stats collection is
enabled (see \fItopics\-stats\fP).
.SS Web service extension
.INDENT 0.0
.TP
.B class scrapy.webservice.WebService
.UNINDENT
.sp
See \fItopics\-webservice\fP\&.
.SS Telnet console extension
.INDENT 0.0
.TP
.B class scrapy.telnet.TelnetConsole
.UNINDENT
.sp
Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.
.sp
The telnet console must be enabled by the \fBTELNETCONSOLE_ENABLED\fP
setting, and the server will listen in the port specified in
\fBTELNETCONSOLE_PORT\fP\&.
.SS Memory usage extension
.INDENT 0.0
.TP
.B class scrapy.contrib.memusage.MemoryUsage
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
This extension does not work in Windows.
.UNINDENT
.UNINDENT
.sp
Monitors the memory used by the Scrapy process that runs the spider and:
.sp
1, sends a notification e\-mail when it exceeds a certain value
2. closes the spider when it exceeds a certain value
.sp
The notification e\-mails can be triggered when a certain warning value is
reached (\fBMEMUSAGE_WARNING_MB\fP) and when the maximum value is reached
(\fBMEMUSAGE_LIMIT_MB\fP) which will also cause the spider to be closed
and the Scrapy process to be terminated.
.sp
This extension is enabled by the \fBMEMUSAGE_ENABLED\fP setting and
can be configured with the following settings:
.INDENT 0.0
.IP \(bu 2
\fBMEMUSAGE_LIMIT_MB\fP
.IP \(bu 2
\fBMEMUSAGE_WARNING_MB\fP
.IP \(bu 2
\fBMEMUSAGE_NOTIFY_MAIL\fP
.IP \(bu 2
\fBMEMUSAGE_REPORT\fP
.UNINDENT
.SS Memory debugger extension
.INDENT 0.0
.TP
.B class scrapy.contrib.memdebug.MemoryDebugger
.UNINDENT
.sp
An extension for debugging memory usage. It collects information about:
.INDENT 0.0
.IP \(bu 2
objects uncollected by the Python garbage collector
.IP \(bu 2
objects left alive that shouldn\(aqt. For more info, see \fItopics\-leaks\-trackrefs\fP
.UNINDENT
.sp
To enable this extension, turn on the \fBMEMDEBUG_ENABLED\fP setting. The
info will be stored in the stats.
.SS Close spider extension
.INDENT 0.0
.TP
.B class scrapy.contrib.closespider.CloseSpider
.UNINDENT
.sp
Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.
.sp
The conditions for closing a spider can be configured through the following
settings:
.INDENT 0.0
.IP \(bu 2
\fBCLOSESPIDER_TIMEOUT\fP
.IP \(bu 2
\fBCLOSESPIDER_ITEMCOUNT\fP
.IP \(bu 2
\fBCLOSESPIDER_PAGECOUNT\fP
.IP \(bu 2
\fBCLOSESPIDER_ERRORCOUNT\fP
.UNINDENT
.SS CLOSESPIDER_TIMEOUT
.sp
Default: \fB0\fP
.sp
An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason \fBclosespider_timeout\fP\&. If zero (or non set), spiders won\(aqt be closed by
timeout.
.SS CLOSESPIDER_ITEMCOUNT
.sp
Default: \fB0\fP
.sp
An integer which specifies a number of items. If the spider scrapes more than
that amount if items and those items are passed by the item pipeline, the
spider will be closed with the reason \fBclosespider_itemcount\fP\&. If zero (or
non set), spiders won\(aqt be closed by number of passed items.
.SS CLOSESPIDER_PAGECOUNT
.sp
New in version 0.11.

.sp
Default: \fB0\fP
.sp
An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
\fBclosespider_pagecount\fP\&. If zero (or non set), spiders won\(aqt be closed by
number of crawled responses.
.SS CLOSESPIDER_ERRORCOUNT
.sp
New in version 0.11.

.sp
Default: \fB0\fP
.sp
An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason \fBclosespider_errorcount\fP\&. If zero (or non
set), spiders won\(aqt be closed by number of errors.
.SS StatsMailer extension
.INDENT 0.0
.TP
.B class scrapy.contrib.statsmailer.StatsMailer
.UNINDENT
.sp
This simple extension can be used to send a notification e\-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the \fBSTATSMAILER_RCPTS\fP
setting.
.SS Debugging extensions
.SS Stack trace dump extension
.INDENT 0.0
.TP
.B class scrapy.contrib.debug.StackTraceDump
.UNINDENT
.sp
Dumps information about the running process when a \fI\%SIGQUIT\fP or \fI\%SIGUSR2\fP
signal is received. The information dumped is the following:
.INDENT 0.0
.IP 1. 3
engine status (using \fBscrapy.utils.engine.get_engine_status()\fP)
.IP 2. 3
live references (see \fItopics\-leaks\-trackrefs\fP)
.IP 3. 3
stack trace of all threads
.UNINDENT
.sp
After the stack trace and engine status is dumped, the Scrapy process continues
running normally.
.sp
This extension only works on POSIX\-compliant platforms (ie. not Windows),
because the \fI\%SIGQUIT\fP and \fI\%SIGUSR2\fP signals are not available on Windows.
.sp
There are at least two ways to send Scrapy the \fI\%SIGQUIT\fP signal:
.INDENT 0.0
.IP 1. 3
By pressing Ctrl\-while a Scrapy process is running (Linux only?)
.IP 2. 3
By running this command (assuming \fB<pid>\fP is the process id of the Scrapy
process):
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
kill \-QUIT <pid>
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS Debugger extension
.INDENT 0.0
.TP
.B class scrapy.contrib.debug.Debugger
.UNINDENT
.sp
Invokes a \fI\%Python debugger\fP inside a running Scrapy process when a \fI\%SIGUSR2\fP
signal is received. After the debugger is exited, the Scrapy process continues
running normally.
.sp
For more info see \fIDebugging in Python\fP\&.
.sp
This extension only works on POSIX\-compliant platforms (ie. not Windows).
.SS Core API
.sp
New in version 0.15.

.sp
This section documents the Scrapy core API, and it\(aqs intended for developers of
extensions and middlewares.
.SS Crawler API
.sp
The main entry point to Scrapy API is the \fBCrawler\fP
object, passed to extensions through the \fBfrom_crawler\fP class method. This
object provides access to all Scrapy core components, and it\(aqs the only way for
extensions to access them and hook their functionality into Scrapy.
.sp
The Extension Manager is responsible for loading and keeping track of installed
extensions and it\(aqs configured through the \fBEXTENSIONS\fP setting which
contains a dictionary of all available extensions and their order similar to
how you \fIconfigure the downloader middlewares\fP\&.
.INDENT 0.0
.TP
.B class scrapy.crawler.Crawler(settings)
The Crawler object must be instantiated with a
\fBscrapy.settings.Settings\fP object.
.INDENT 7.0
.TP
.B settings
The settings manager of this crawler.
.sp
This is used by extensions & middlewares to access the Scrapy settings
of this crawler.
.sp
For an introduction on Scrapy settings see \fItopics\-settings\fP\&.
.sp
For the API see \fBSettings\fP class.
.UNINDENT
.INDENT 7.0
.TP
.B signals
The signals manager of this crawler.
.sp
This is used by extensions & middlewares to hook themselves into Scrapy
functionality.
.sp
For an introduction on signals see \fItopics\-signals\fP\&.
.sp
For the API see \fBSignalManager\fP class.
.UNINDENT
.INDENT 7.0
.TP
.B stats
The stats collector of this crawler.
.sp
This is used from extensions & middlewares to record stats of their
behaviour, or access stats collected by other extensions.
.sp
For an introduction on stats collection see \fItopics\-stats\fP\&.
.sp
For the API see \fBStatsCollector\fP class.
.UNINDENT
.INDENT 7.0
.TP
.B extensions
The extension manager that keeps track of enabled extensions.
.sp
Most extensions won\(aqt need to access this attribute.
.sp
For an introduction on extensions and a list of available extensions on
Scrapy see \fItopics\-extensions\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B spiders
The spider manager which takes care of loading and instantiating
spiders.
.sp
Most extensions won\(aqt need to access this attribute.
.UNINDENT
.INDENT 7.0
.TP
.B engine
The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.
.sp
Some extension may want to access the Scrapy engine, to modify inspect
or modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.
.UNINDENT
.INDENT 7.0
.TP
.B configure()
Configure the crawler.
.sp
This loads extensions, middlewares and spiders, leaving the crawler
ready to be started. It also configures the execution engine.
.UNINDENT
.INDENT 7.0
.TP
.B start()
Start the crawler. This calls \fBconfigure()\fP if it hasn\(aqt been called yet.
Returns a deferred that is fired when the crawl is finished.
.UNINDENT
.UNINDENT
.SS Settings API
.INDENT 0.0
.TP
.B class scrapy.settings.Settings
This object that provides access to Scrapy settings.
.INDENT 7.0
.TP
.B overrides
Global overrides are the ones that take most precedence, and are usually
populated by command\-line options.
.sp
Overrides should be populated \fIbefore\fP configuring the Crawler object
(through the \fBconfigure()\fP method),
otherwise they won\(aqt have any effect. You don\(aqt typically need to worry
about overrides unless you are implementing your own Scrapy command.
.UNINDENT
.INDENT 7.0
.TP
.B get(name, default=None)
Get a setting value without affecting its original type.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBname\fP (\fIstring\fP) \-\- the setting name
.IP \(bu 2
\fBdefault\fP (\fIany\fP) \-\- the value to return if no setting is found
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B getbool(name, default=False)
Get a setting value as a boolean. For example, both \fB1\fP and \fB\(aq1\(aq\fP, and
\fBTrue\fP return \fBTrue\fP, while \fB0\fP, \fB\(aq0\(aq\fP, \fBFalse\fP and \fBNone\fP
return \fBFalse\(ga\(ga\fP
.sp
For example, settings populated through environment variables set to \fB\(aq0\(aq\fP
will return \fBFalse\fP when using this method.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBname\fP (\fIstring\fP) \-\- the setting name
.IP \(bu 2
\fBdefault\fP (\fIany\fP) \-\- the value to return if no setting is found
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B getint(name, default=0)
Get a setting value as an int
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBname\fP (\fIstring\fP) \-\- the setting name
.IP \(bu 2
\fBdefault\fP (\fIany\fP) \-\- the value to return if no setting is found
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B getfloat(name, default=0.0)
Get a setting value as a float
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBname\fP (\fIstring\fP) \-\- the setting name
.IP \(bu 2
\fBdefault\fP (\fIany\fP) \-\- the value to return if no setting is found
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B getlist(name, default=None)
Get a setting value as a list. If the setting original type is a list it
will be returned verbatim. If it\(aqs a string it will be split by ",".
.sp
For example, settings populated through environment variables set to
\fB\(aqone,two\(aq\fP will return a list [\(aqone\(aq, \(aqtwo\(aq] when using this method.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBname\fP (\fIstring\fP) \-\- the setting name
.IP \(bu 2
\fBdefault\fP (\fIany\fP) \-\- the value to return if no setting is found
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Signals API
.INDENT 0.0
.TP
.B class scrapy.signalmanager.SignalManager
.INDENT 7.0
.TP
.B connect(receiver, signal)
Connect a receiver function to a signal.
.sp
The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the \fItopics\-signals\fP
section.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBreceiver\fP (\fIcallable\fP) \-\- the function to be connected
.IP \(bu 2
\fBsignal\fP (\fIobject\fP) \-\- the signal to connect to
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B send_catch_log(signal, **kwargs)
Send a signal, catch exceptions and log them.
.sp
The keyword arguments are passed to the signal handlers (connected
through the \fBconnect()\fP method).
.UNINDENT
.INDENT 7.0
.TP
.B send_catch_log_deferred(signal, **kwargs)
Like \fBsend_catch_log()\fP but supports returning \fI\%deferreds\fP from
signal handlers.
.sp
Returns a \fI\%deferred\fP that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.
.sp
The keyword arguments are passed to the signal handlers (connected
through the \fBconnect()\fP method).
.UNINDENT
.INDENT 7.0
.TP
.B disconnect(receiver, signal)
Disconnect a receiver function from a signal. This has the opposite
effect of the \fBconnect()\fP method, and the arguments are the same.
.UNINDENT
.INDENT 7.0
.TP
.B disconnect_all(signal)
Disconnect all receivers from the given signal.
.INDENT 7.0
.TP
.B Parameters
\fBsignal\fP (\fIobject\fP) \-\- the signal to disconnect from
.UNINDENT
.UNINDENT
.UNINDENT
.SS Stats Collector API
.sp
There are several Stats Collectors available under the
\fBscrapy.statscol\fP module and they all implement the Stats
Collector API defined by the \fBStatsCollector\fP
class (which they all inherit from).
.INDENT 0.0
.TP
.B class scrapy.statscol.StatsCollector
.INDENT 7.0
.TP
.B get_value(key, default=None)
Return the value for the given stats key or default if it doesn\(aqt exist.
.UNINDENT
.INDENT 7.0
.TP
.B get_stats()
Get all stats from the currently running spider as a dict.
.UNINDENT
.INDENT 7.0
.TP
.B set_value(key, value)
Set the given value for the given stats key.
.UNINDENT
.INDENT 7.0
.TP
.B set_stats(stats)
Override the current stats with the dict passed in \fBstats\fP argument.
.UNINDENT
.INDENT 7.0
.TP
.B inc_value(key, count=1, start=0)
Increment the value of the given stats key, by the given count,
assuming the start value given (when it\(aqs not set).
.UNINDENT
.INDENT 7.0
.TP
.B max_value(key, value)
Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.
.UNINDENT
.INDENT 7.0
.TP
.B min_value(key, value)
Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.
.UNINDENT
.INDENT 7.0
.TP
.B clear_stats()
Clear all stats.
.UNINDENT
.sp
The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:
.INDENT 7.0
.TP
.B open_spider(spider)
Open the given spider for stats collection.
.UNINDENT
.INDENT 7.0
.TP
.B close_spider(spider)
Close the given spider. After this is called, no more specific stats
can be accessed or collected.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \fBtopics/architecture\fP
Understand the Scrapy architecture.
.TP
.B \fBtopics/downloader\-middleware\fP
Customize how pages get requested and downloaded.
.TP
.B \fBtopics/spider\-middleware\fP
Customize the input and output of your spiders.
.TP
.B \fBtopics/extensions\fP
Extend Scrapy with your custom functionality
.TP
.B \fBtopics/api\fP
Use it on extensions and middlewares to extend Scrapy functionality
.UNINDENT
.SH REFERENCE
.SS Requests and Responses
.sp
Scrapy uses \fBRequest\fP and \fBResponse\fP objects for crawling web
sites.
.sp
Typically, \fBRequest\fP objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a \fBResponse\fP object which travels back to the spider that
issued the request.
.sp
Both \fBRequest\fP and \fBResponse\fP classes have subclasses which add
functionality not required in the base classes. These are described
below in \fItopics\-request\-response\-ref\-request\-subclasses\fP and
\fItopics\-request\-response\-ref\-response\-subclasses\fP\&.
.SS Request objects
.INDENT 0.0
.TP
.B class scrapy.http.Request(url[, callback, method=\(aqGET\(aq, headers, body, cookies, meta, encoding=\(aqutf\-8\(aq, priority=0, dont_filter=False, errback])
A \fBRequest\fP object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a \fBResponse\fP\&.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBurl\fP (\fIstring\fP) \-\- the URL of this request
.IP \(bu 2
\fBcallback\fP (\fIcallable\fP) \-\- the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see \fItopics\-request\-response\-ref\-request\-callback\-arguments\fP below.
If a Request doesn\(aqt specify a callback, the spider\(aqs
\fBparse()\fP method will be used.
Note that if exceptions are raised during processing, errback is called instead.
.IP \(bu 2
\fBmethod\fP (\fIstring\fP) \-\- the HTTP method of this request. Defaults to \fB\(aqGET\(aq\fP\&.
.IP \(bu 2
\fBmeta\fP (\fIdict\fP) \-\- the initial values for the \fBRequest.meta\fP attribute. If
given, the dict passed in this parameter will be shallow copied.
.IP \(bu 2
\fBbody\fP (\fIstr or unicode\fP) \-\- the request body. If a \fBunicode\fP is passed, then it\(aqs encoded to
\fBstr\fP using the \fIencoding\fP passed (which defaults to \fButf\-8\fP). If
\fBbody\fP is not given,, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a \fBstr\fP (never
\fBunicode\fP or \fBNone\fP).
.IP \(bu 2
\fBheaders\fP (\fIdict\fP) \-\- the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi\-valued headers). If
\fBNone\fP is passed as value, the HTTP header will not be sent at all.
.IP \(bu 2
\fBcookies\fP (\fIdict or list\fP) \-\- 
.sp
the request cookies. These can be sent in two forms.
.INDENT 2.0
.IP 1. 3
Using a dict:
.INDENT 2.0
.INDENT 3.5
.sp
.nf
.ft C
request_with_cookies = Request(url="http://www.example.com",
                               cookies={\(aqcurrency\(aq: \(aqUSD\(aq, \(aqcountry\(aq: \(aqUY\(aq})
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 2. 3
Using a list of dicts:
.INDENT 2.0
.INDENT 3.5
.sp
.nf
.ft C
request_with_cookies = Request(url="http://www.example.com",
                               cookies=[{\(aqname\(aq: \(aqcurrency\(aq,
                                        \(aqvalue\(aq: \(aqUSD\(aq,
                                        \(aqdomain\(aq: \(aqexample.com\(aq,
                                        \(aqpath\(aq: \(aq/currency\(aq}])
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The latter form allows for customizing the \fBdomain\fP and \fBpath\fP
attributes of the cookie. These is only useful if the cookies are saved
for later requests.
.sp
When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That\(aqs
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the \fBdont_merge_cookies\fP key in the
\fBRequest.meta\fP\&.
.sp
Example of request without merging cookies:
.INDENT 2.0
.INDENT 3.5
.sp
.nf
.ft C
request_with_cookies = Request(url="http://www.example.com",
                               cookies={\(aqcurrency\(aq: \(aqUSD\(aq, \(aqcountry\(aq: \(aqUY\(aq},
                               meta={\(aqdont_merge_cookies\(aq: True})
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For more info see \fIcookies\-mw\fP\&.

.IP \(bu 2
\fBencoding\fP (\fIstring\fP) \-\- the encoding of this request (defaults to \fB\(aqutf\-8\(aq\fP).
This encoding will be used to percent\-encode the URL and to convert the
body to \fBstr\fP (if given as \fBunicode\fP).
.IP \(bu 2
\fBpriority\fP (\fIint\fP) \-\- the priority of this request (defaults to \fB0\fP).
The priority is used by the scheduler to define the order used to process
requests.
.IP \(bu 2
\fBdont_filter\fP (\fIboolean\fP) \-\- indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to \fBFalse\fP\&.
.IP \(bu 2
\fBerrback\fP (\fIcallable\fP) \-\- a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a \fI\%Twisted Failure\fP instance
as first parameter.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B url
A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.
.sp
This attribute is read\-only. To change the URL of a Request use
\fBreplace()\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B method
A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: \fB"GET"\fP, \fB"POST"\fP, \fB"PUT"\fP, etc
.UNINDENT
.INDENT 7.0
.TP
.B headers
A dictionary\-like object which contains the request headers.
.UNINDENT
.INDENT 7.0
.TP
.B body
A str that contains the request body.
.sp
This attribute is read\-only. To change the body of a Request use
\fBreplace()\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B meta
A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.
.sp
See \fItopics\-request\-meta\fP for a list of special meta keys
recognized by Scrapy.
.sp
This dict is \fI\%shallow copied\fP when the request is cloned using the
\fBcopy()\fP or \fBreplace()\fP methods, and can also be accessed, in your
spider, from the \fBresponse.meta\fP attribute.
.UNINDENT
.INDENT 7.0
.TP
.B copy()
Return a new Request which is a copy of this Request. See also:
\fItopics\-request\-response\-ref\-request\-callback\-arguments\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B replace([url, method, headers, body, cookies, meta, encoding, dont_filter, callback, errback])
Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute \fBRequest.meta\fP is copied by default (unless a new value
is given in the \fBmeta\fP argument). See also
\fItopics\-request\-response\-ref\-request\-callback\-arguments\fP\&.
.UNINDENT
.UNINDENT
.SS Passing additional data to callback functions
.sp
The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded \fBResponse\fP object as its first argument.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_page1(self, response):
    return Request("http://www.example.com/some_page.html",
                      callback=self.parse_page2)

def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.log("Visited %s" % response.url)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the \fBRequest.meta\fP attribute for that.
.sp
Here\(aqs an example of how to pass an item using this mechanism, to populate
different fields from different pages:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_page1(self, response):
    item = MyItem()
    item[\(aqmain_url\(aq] = response.url
    request = Request("http://www.example.com/some_page.html",
                      callback=self.parse_page2)
    request.meta[\(aqitem\(aq] = item
    return request

def parse_page2(self, response):
    item = response.meta[\(aqitem\(aq]
    item[\(aqother_url\(aq] = response.url
    return item
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Request.meta special keys
.sp
The \fBRequest.meta\fP attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built\-in extensions.
.sp
Those are:
.INDENT 0.0
.IP \(bu 2
\fBdont_redirect\fP
.IP \(bu 2
\fBdont_retry\fP
.IP \(bu 2
\fBhandle_httpstatus_list\fP
.IP \(bu 2
\fBdont_merge_cookies\fP (see \fBcookies\fP parameter of \fBRequest\fP constructor)
.IP \(bu 2
\fBcookiejar\fP
.IP \(bu 2
\fBredirect_urls\fP
.IP \(bu 2
\fBbindaddress\fP
.UNINDENT
.SS bindaddress
.sp
The IP of the outgoing IP address to use for the performing the request.
.SS Request subclasses
.sp
Here is the list of built\-in \fBRequest\fP subclasses. You can also subclass
it to implement your own custom functionality.
.SS FormRequest objects
.sp
The FormRequest class extends the base \fBRequest\fP with functionality for
dealing with HTML forms. It uses \fI\%lxml.html forms\fP  to pre\-populate form
fields with form data from \fBResponse\fP objects.
.INDENT 0.0
.TP
.B class scrapy.http.FormRequest(url[, formdata, \&...])
The \fBFormRequest\fP class adds a new argument to the constructor. The
remaining arguments are the same as for the \fBRequest\fP class and are
not documented here.
.INDENT 7.0
.TP
.B Parameters
\fBformdata\fP (\fIdict or iterable of tuples\fP) \-\- is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url\-encoded and assigned to the
body of the request.
.UNINDENT
.sp
The \fBFormRequest\fP objects support the following class method in
addition to the standard \fBRequest\fP methods:
.INDENT 7.0
.TP
.B classmethod from_response(response[, formname=None, formnumber=0, formdata=None, formxpath=None, dont_click=False, \&...])
Returns a new \fBFormRequest\fP object with its form field values
pre\-populated with those found in the HTML \fB<form>\fP element contained
in the given response. For an example see
\fItopics\-request\-response\-ref\-request\-userlogin\fP\&.
.sp
The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a \fB<input type="submit">\fP\&.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default \fBfrom_response()\fP behaviour may not be the
most appropriate. To disable this behaviour you can set the
\fBdont_click\fP argument to \fBTrue\fP\&. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
\fBclickdata\fP argument.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response containing a HTML form which will be used
to pre\-populate the form fields
.IP \(bu 2
\fBformname\fP (\fIstring\fP) \-\- if given, the form with name attribute set to this value will be used.
.IP \(bu 2
\fBformxpath\fP (\fIstring\fP) \-\- if given, the first form that matches the xpath will be used.
.IP \(bu 2
\fBformnumber\fP (\fIinteger\fP) \-\- the number of form to use, when the response contains
multiple forms. The first one (and also the default) is \fB0\fP\&.
.IP \(bu 2
\fBformdata\fP (\fIdict\fP) \-\- fields to override in the form data. If a field was
already present in the response \fB<form>\fP element, its value is
overridden by the one passed in this parameter.
.IP \(bu 2
\fBdont_click\fP (\fIboolean\fP) \-\- If True, the form data will be submitted without
clicking in any element.
.UNINDENT
.UNINDENT
.sp
The other parameters of this class method are passed directly to the
\fBFormRequest\fP constructor.
.sp
New in version 0.10.3: The \fBformname\fP parameter.

.sp
New in version 0.17: The \fBformxpath\fP parameter.

.UNINDENT
.UNINDENT
.SS Request usage examples
.SS Using FormRequest to send data via HTTP POST
.sp
If you want to simulate a HTML Form POST in your spider and send a couple of
key\-value fields, you can return a \fBFormRequest\fP object (from your
spider) like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
return [FormRequest(url="http://www.example.com/post/action",
                    formdata={\(aqname\(aq: \(aqJohn Doe\(aq, \(aqage\(aq: \(aq27\(aq},
                    callback=self.after_post)]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Using FormRequest.from_response() to simulate a user login
.sp
It is usual for web sites to provide pre\-populated form fields through \fB<input
type="hidden">\fP elements, such as session related data or authentication
tokens (for login pages). When scraping, you\(aqll want these fields to be
automatically pre\-populated and only override a couple of them, such as the
user name and password. You can use the \fBFormRequest.from_response()\fP
method for this job. Here\(aqs an example spider which uses it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class LoginSpider(Spider):
    name = \(aqexample.com\(aq
    start_urls = [\(aqhttp://www.example.com/users/login.php\(aq]

    def parse(self, response):
        return [FormRequest.from_response(response,
                    formdata={\(aqusername\(aq: \(aqjohn\(aq, \(aqpassword\(aq: \(aqsecret\(aq},
                    callback=self.after_login)]

    def after_login(self, response):
        # check login succeed before going on
        if "authentication failed" in response.body:
            self.log("Login failed", level=log.ERROR)
            return

        # continue scraping with authenticated session...
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Response objects
.INDENT 0.0
.TP
.B class scrapy.http.Response(url[, status=200, headers, body, flags])
A \fBResponse\fP object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBurl\fP (\fIstring\fP) \-\- the URL of this response
.IP \(bu 2
\fBheaders\fP (\fIdict\fP) \-\- the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi\-valued headers).
.IP \(bu 2
\fBstatus\fP (\fIinteger\fP) \-\- the HTTP status of the response. Defaults to \fB200\fP\&.
.IP \(bu 2
\fBbody\fP (\fIstr\fP) \-\- the response body. It must be str, not unicode, unless you\(aqre
using a encoding\-aware \fIResponse subclass\fP, such as
\fBTextResponse\fP\&.
.IP \(bu 2
\fBmeta\fP (\fIdict\fP) \-\- the initial values for the \fBResponse.meta\fP attribute. If
given, the dict will be shallow copied.
.IP \(bu 2
\fBflags\fP (\fIlist\fP) \-\- is a list containing the initial values for the
\fBResponse.flags\fP attribute. If given, the list will be shallow
copied.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B url
A string containing the URL of the response.
.sp
This attribute is read\-only. To change the URL of a Response use
\fBreplace()\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B status
An integer representing the HTTP status of the response. Example: \fB200\fP,
\fB404\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B headers
A dictionary\-like object which contains the response headers.
.UNINDENT
.INDENT 7.0
.TP
.B body
A str containing the body of this Response. Keep in mind that Reponse.body
is always a str. If you want the unicode version use
\fBTextResponse.body_as_unicode()\fP (only available in
\fBTextResponse\fP and subclasses).
.sp
This attribute is read\-only. To change the body of a Response use
\fBreplace()\fP\&.
.UNINDENT
.INDENT 7.0
.TP
.B request
The \fBRequest\fP object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all \fIDownloader Middlewares\fP\&.
In particular, this means that:
.INDENT 7.0
.IP \(bu 2
HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).
.IP \(bu 2
Response.request.url doesn\(aqt always equal Response.url
.IP \(bu 2
This attribute is only available in the spider code, and in the
\fISpider Middlewares\fP, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the \fBresponse_downloaded\fP signal.
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B meta
A shortcut to the \fBRequest.meta\fP attribute of the
\fBResponse.request\fP object (ie. \fBself.request.meta\fP).
.sp
Unlike the \fBResponse.request\fP attribute, the \fBResponse.meta\fP
attribute is propagated along redirects and retries, so you will get
the original \fBRequest.meta\fP sent from your spider.
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBRequest.meta\fP attribute
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B flags
A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: \fI\(aqcached\(aq\fP, \fI\(aqredirected\fP\(aq, etc. And
they\(aqre shown on the string representation of the Response (\fI__str__\fP
method) which is used by the engine for logging.
.UNINDENT
.INDENT 7.0
.TP
.B copy()
Returns a new Response which is a copy of this Response.
.UNINDENT
.INDENT 7.0
.TP
.B replace([url, status, headers, body, request, flags, cls])
Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute \fBResponse.meta\fP is copied by default.
.UNINDENT
.UNINDENT
.SS Response subclasses
.sp
Here is the list of available built\-in Response subclasses. You can also
subclass the Response class to implement your own functionality.
.SS TextResponse objects
.INDENT 0.0
.TP
.B class scrapy.http.TextResponse(url[, encoding[, \&...]])
\fBTextResponse\fP objects adds encoding capabilities to the base
\fBResponse\fP class, which is meant to be used only for binary data,
such as images, sounds or any media file.
.sp
\fBTextResponse\fP objects support a new constructor argument, in
addition to the base \fBResponse\fP objects. The remaining functionality
is the same as for the \fBResponse\fP class and is not documented here.
.INDENT 7.0
.TP
.B Parameters
\fBencoding\fP (\fIstring\fP) \-\- is a string which contains the encoding to use for this
response. If you create a \fBTextResponse\fP object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If \fBencoding\fP is \fBNone\fP (default value), the
encoding will be looked up in the response headers and body instead.
.UNINDENT
.sp
\fBTextResponse\fP objects support the following attributes in addition
to the standard \fBResponse\fP ones:
.INDENT 7.0
.TP
.B encoding
A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:
.INDENT 7.0
.IP 1. 3
the encoding passed in the constructor \fIencoding\fP argument
.IP 2. 3
the encoding declared in the Content\-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.
.IP 3. 3
the encoding declared in the response body. The TextResponse class
doesn\(aqt provide any special functionality for this. However, the
\fBHtmlResponse\fP and \fBXmlResponse\fP classes do.
.IP 4. 3
the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.
.UNINDENT
.UNINDENT
.sp
\fBTextResponse\fP objects support the following methods in addition to
the standard \fBResponse\fP ones:
.INDENT 7.0
.TP
.B body_as_unicode()
Returns the body of the response as unicode. This is equivalent to:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
response.body.decode(response.encoding)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
But \fBnot\fP equivalent to:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
unicode(response.body)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Since, in the latter case, you would be using you system default encoding
(typically \fIascii\fP) to convert the body to unicode, instead of the response
encoding.
.UNINDENT
.UNINDENT
.SS HtmlResponse objects
.INDENT 0.0
.TP
.B class scrapy.http.HtmlResponse(url[, \&...])
The \fBHtmlResponse\fP class is a subclass of \fBTextResponse\fP
which adds encoding auto\-discovering support by looking into the HTML \fI\%meta
http\-equiv\fP attribute.  See \fBTextResponse.encoding\fP\&.
.UNINDENT
.SS XmlResponse objects
.INDENT 0.0
.TP
.B class scrapy.http.XmlResponse(url[, \&...])
The \fBXmlResponse\fP class is a subclass of \fBTextResponse\fP which
adds encoding auto\-discovering support by looking into the XML declaration
line.  See \fBTextResponse.encoding\fP\&.
.UNINDENT
.SS Settings
.sp
The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.
.sp
The infrastructure of the settings provides a global namespace of key\-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.
.sp
The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).
.sp
For a list of available built\-in settings see: \fItopics\-settings\-ref\fP\&.
.SS Designating the settings
.sp
When you use Scrapy, you have to tell it which settings you\(aqre using. You can
do this by using an environment variable, \fBSCRAPY_SETTINGS_MODULE\fP\&.
.sp
The value of \fBSCRAPY_SETTINGS_MODULE\fP should be in Python path syntax, e.g.
\fBmyproject.settings\fP\&. Note that the settings module should be on the
Python \fI\%import search path\fP\&.
.SS Populating the settings
.sp
Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP 1. 3
Global overrides (most precedence)
.IP 2. 3
Project settings module
.IP 3. 3
Default settings per\-command
.IP 4. 3
Default global settings (less precedence)
.UNINDENT
.UNINDENT
.UNINDENT
.sp
These mechanisms are described in more detail below.
.SS 1. Global overrides
.sp
Global overrides are the ones that take most precedence, and are usually
populated by command\-line options. You can also override one (or more) settings
from command line using the \fB\-s\fP (or \fB\-\-set\fP) command line option.
.sp
For more information see the \fBoverrides\fP
Settings attribute.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy crawl myspider \-s LOG_FILE=scrapy.log
.ft P
.fi
.UNINDENT
.UNINDENT
.SS 2. Project settings module
.sp
The project settings module is the standard configuration file for your Scrapy
project.  It\(aqs where most of your custom settings will be populated. For
example:: \fBmyproject.settings\fP\&.
.SS 3. Default settings per\-command
.sp
Each \fBScrapy tool\fP command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the \fBdefault_settings\fP attribute of the command
class.
.SS 4. Default global settings
.sp
The global defaults are located in the \fBscrapy.settings.default_settings\fP
module and documented in the \fItopics\-settings\-ref\fP section.
.SS How to access settings
.sp
Settings can be accessed through the \fBscrapy.crawler.Crawler.settings\fP
attribute of the Crawler that is passed to \fBfrom_crawler\fP method in
extensions and middlewares:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class MyExtension(object):

    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        if settings[\(aqLOG_ENABLED\(aq]:
            print "log is enabled!"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
In other words, settings can be accessed like a dict, but it\(aqs usually preferred
to extract the setting in the format you need it to avoid type errors. In order
to do that you\(aqll have to use one of the methods provided the
\fBSettings\fP API.
.SS Rationale for setting names
.sp
Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
\fBROBOTSTXT_ENABLED\fP, \fBROBOTSTXT_OBEY\fP, \fBROBOTSTXT_CACHEDIR\fP, etc.
.SS Built\-in settings reference
.sp
Here\(aqs a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.
.sp
The scope, where available, shows where the setting is being used, if it\(aqs tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.
.SS AWS_ACCESS_KEY_ID
.sp
Default: \fBNone\fP
.sp
The AWS access key used by code that requires access to \fI\%Amazon Web services\fP,
such as the \fIS3 feed storage backend\fP\&.
.SS AWS_SECRET_ACCESS_KEY
.sp
Default: \fBNone\fP
.sp
The AWS secret key used by code that requires access to \fI\%Amazon Web services\fP,
such as the \fIS3 feed storage backend\fP\&.
.SS BOT_NAME
.sp
Default: \fB\(aqscrapybot\(aq\fP
.sp
The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User\-Agent by default, and
also for logging.
.sp
It\(aqs automatically populated with your project name when you create your
project with the \fBstartproject\fP command.
.SS CONCURRENT_ITEMS
.sp
Default: \fB100\fP
.sp
Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the \fIItem Pipeline\fP).
.SS CONCURRENT_REQUESTS
.sp
Default: \fB16\fP
.sp
The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.
.SS CONCURRENT_REQUESTS_PER_DOMAIN
.sp
Default: \fB8\fP
.sp
The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.
.SS CONCURRENT_REQUESTS_PER_IP
.sp
Default: \fB0\fP
.sp
The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non\-zero, the
\fBCONCURRENT_REQUESTS_PER_DOMAIN\fP setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.
.SS DEFAULT_ITEM_CLASS
.sp
Default: \fB\(aqscrapy.item.Item\(aq\fP
.sp
The default class that will be used for instantiating items in the \fIthe
Scrapy shell\fP\&.
.SS DEFAULT_REQUEST_HEADERS
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqAccept\(aq: \(aqtext/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\(aq,
    \(aqAccept\-Language\(aq: \(aqen\(aq,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The default headers used for Scrapy HTTP Requests. They\(aqre populated in the
\fBDefaultHeadersMiddleware\fP\&.
.SS DEPTH_LIMIT
.sp
Default: \fB0\fP
.sp
The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.
.SS DEPTH_PRIORITY
.sp
Default: \fB0\fP
.sp
An integer that is used to adjust the request priority based on its depth.
.sp
If zero, no priority adjustment is made from depth.
.SS DEPTH_STATS
.sp
Default: \fBTrue\fP
.sp
Whether to collect maximum depth stats.
.SS DEPTH_STATS_VERBOSE
.sp
Default: \fBFalse\fP
.sp
Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.
.SS DNSCACHE_ENABLED
.sp
Default: \fBTrue\fP
.sp
Whether to enable DNS in\-memory cache.
.SS DOWNLOADER_DEBUG
.sp
Default: \fBFalse\fP
.sp
Whether to enable the Downloader debugging mode.
.SS DOWNLOADER_MIDDLEWARES
.sp
Default:: \fB{}\fP
.sp
A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see \fItopics\-downloader\-middleware\-setting\fP\&.
.SS DOWNLOADER_MIDDLEWARES_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqscrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware\(aq: 100,
    \(aqscrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware\(aq: 300,
    \(aqscrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware\(aq: 350,
    \(aqscrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware\(aq: 400,
    \(aqscrapy.contrib.downloadermiddleware.retry.RetryMiddleware\(aq: 500,
    \(aqscrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware\(aq: 550,
    \(aqscrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware\(aq: 580,
    \(aqscrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware\(aq: 590,
    \(aqscrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware\(aq: 600,
    \(aqscrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware\(aq: 700,
    \(aqscrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware\(aq: 750,
    \(aqscrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware\(aq: 830,
    \(aqscrapy.contrib.downloadermiddleware.stats.DownloaderStats\(aq: 850,
    \(aqscrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\(aq: 900,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the downloader middlewares enabled by default in Scrapy. You
should never modify this setting in your project, modify
\fBDOWNLOADER_MIDDLEWARES\fP instead.  For more info see
\fItopics\-downloader\-middleware\-setting\fP\&.
.SS DOWNLOADER_STATS
.sp
Default: \fBTrue\fP
.sp
Whether to enable downloader stats collection.
.SS DOWNLOAD_DELAY
.sp
Default: \fB0\fP
.sp
The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same spider. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
DOWNLOAD_DELAY = 0.25    # 250 ms of delay
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This setting is also affected by the \fBRANDOMIZE_DOWNLOAD_DELAY\fP
setting (which is enabled by default). By default, Scrapy doesn\(aqt wait a fixed
amount of time between requests, but uses a random interval between 0.5 and 1.5
* \fBDOWNLOAD_DELAY\fP\&.
.sp
You can also change this setting per spider.
.SS DOWNLOAD_HANDLERS
.sp
Default: \fB{}\fP
.sp
A dict containing the request downloader handlers enabled in your project.
See \fIDOWNLOAD_HANDLERS_BASE\fP for example format.
.SS DOWNLOAD_HANDLERS_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqfile\(aq: \(aqscrapy.core.downloader.handlers.file.FileDownloadHandler\(aq,
    \(aqhttp\(aq: \(aqscrapy.core.downloader.handlers.http.HttpDownloadHandler\(aq,
    \(aqhttps\(aq: \(aqscrapy.core.downloader.handlers.http.HttpDownloadHandler\(aq,
    \(aqs3\(aq: \(aqscrapy.core.downloader.handlers.s3.S3DownloadHandler\(aq,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
\fBDOWNLOAD_HANDLERS\fP instead.
.SS DOWNLOAD_TIMEOUT
.sp
Default: \fB180\fP
.sp
The amount of time (in secs) that the downloader will wait before timing out.
.SS DUPEFILTER_CLASS
.sp
Default: \fB\(aqscrapy.dupefilter.RFPDupeFilter\(aq\fP
.sp
The class used to detect and filter duplicate requests.
.sp
The default (\fBRFPDupeFilter\fP) filters based on request fingerprint using
the \fBscrapy.utils.request.request_fingerprint\fP function.
.SS EDITOR
.sp
Default: \fIdepends on the environment\fP
.sp
The editor to use for editing spiders with the \fBedit\fP command. It
defaults to the \fBEDITOR\fP environment variable, if set. Otherwise, it defaults
to \fBvi\fP (on Unix systems) or the IDLE editor (on Windows).
.SS EXTENSIONS
.sp
Default:: \fB{}\fP
.sp
A dict containing the extensions enabled in your project, and their orders.
.SS EXTENSIONS_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqscrapy.contrib.corestats.CoreStats\(aq: 0,
    \(aqscrapy.webservice.WebService\(aq: 0,
    \(aqscrapy.telnet.TelnetConsole\(aq: 0,
    \(aqscrapy.contrib.memusage.MemoryUsage\(aq: 0,
    \(aqscrapy.contrib.memdebug.MemoryDebugger\(aq: 0,
    \(aqscrapy.contrib.closespider.CloseSpider\(aq: 0,
    \(aqscrapy.contrib.feedexport.FeedExporter\(aq: 0,
    \(aqscrapy.contrib.logstats.LogStats\(aq: 0,
    \(aqscrapy.contrib.spiderstate.SpiderState\(aq: 0,
    \(aqscrapy.contrib.throttle.AutoThrottle\(aq: 0,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The list of available extensions. Keep in mind that some of them need to
be enabled through a setting. By default, this setting contains all stable
built\-in extensions.
.sp
For more information See the \fIextensions user guide\fP
and the \fIlist of available extensions\fP\&.
.SS ITEM_PIPELINES
.sp
Default: \fB{}\fP
.sp
A dict containing the item pipelines to use, and their orders. The dict is
empty by default order values are arbitrary but it\(aqs customary to define them
in the 0\-1000 range.
.sp
Lists are supported in \fBITEM_PIPELINES\fP for backwards compatibility,
but they are deprecated.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ITEM_PIPELINES = {
    \(aqmybot.pipeline.validate.ValidateMyItem\(aq: 300,
    \(aqmybot.pipeline.validate.StoreMyItem\(aq: 800,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS ITEM_PIPELINES_BASE
.sp
Default: \fB{}\fP
.sp
A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify \fBITEM_PIPELINES\fP instead.
.SS LOG_ENABLED
.sp
Default: \fBTrue\fP
.sp
Whether to enable logging.
.SS LOG_ENCODING
.sp
Default: \fB\(aqutf\-8\(aq\fP
.sp
The encoding to use for logging.
.SS LOG_FILE
.sp
Default: \fBNone\fP
.sp
File name to use for logging output. If None, standard error will be used.
.SS LOG_LEVEL
.sp
Default: \fB\(aqDEBUG\(aq\fP
.sp
Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see \fItopics\-logging\fP\&.
.SS LOG_STDOUT
.sp
Default: \fBFalse\fP
.sp
If \fBTrue\fP, all standard output (and error) of your process will be redirected
to the log. For example if you \fBprint \(aqhello\(aq\fP it will appear in the Scrapy
log.
.SS MEMDEBUG_ENABLED
.sp
Default: \fBFalse\fP
.sp
Whether to enable memory debugging.
.SS MEMDEBUG_NOTIFY
.sp
Default: \fB[]\fP
.sp
When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
MEMDEBUG_NOTIFY = [\(aquser@example.com\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS MEMUSAGE_ENABLED
.sp
Default: \fBFalse\fP
.sp
Scope: \fBscrapy.contrib.memusage\fP
.sp
Whether to enable the memory usage extension that will shutdown the Scrapy
process when it exceeds a memory limit, and also notify by email when that
happened.
.sp
See \fItopics\-extensions\-ref\-memusage\fP\&.
.SS MEMUSAGE_LIMIT_MB
.sp
Default: \fB0\fP
.sp
Scope: \fBscrapy.contrib.memusage\fP
.sp
The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.
.sp
See \fItopics\-extensions\-ref\-memusage\fP\&.
.SS MEMUSAGE_NOTIFY_MAIL
.sp
Default: \fBFalse\fP
.sp
Scope: \fBscrapy.contrib.memusage\fP
.sp
A list of emails to notify if the memory limit has been reached.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
MEMUSAGE_NOTIFY_MAIL = [\(aquser@example.com\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
See \fItopics\-extensions\-ref\-memusage\fP\&.
.SS MEMUSAGE_REPORT
.sp
Default: \fBFalse\fP
.sp
Scope: \fBscrapy.contrib.memusage\fP
.sp
Whether to send a memory usage report after each spider has been closed.
.sp
See \fItopics\-extensions\-ref\-memusage\fP\&.
.SS MEMUSAGE_WARNING_MB
.sp
Default: \fB0\fP
.sp
Scope: \fBscrapy.contrib.memusage\fP
.sp
The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.
.SS NEWSPIDER_MODULE
.sp
Default: \fB\(aq\(aq\fP
.sp
Module where to create new spiders using the \fBgenspider\fP command.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
NEWSPIDER_MODULE = \(aqmybot.spiders_dev\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.SS RANDOMIZE_DOWNLOAD_DELAY
.sp
Default: \fBTrue\fP
.sp
If enabled, Scrapy will wait a random amount of time (between 0.5 and 1.5
* \fBDOWNLOAD_DELAY\fP) while fetching requests from the same
spider.
.sp
This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.
.sp
The randomization policy is the same used by \fI\%wget\fP \fB\-\-random\-wait\fP option.
.sp
If \fBDOWNLOAD_DELAY\fP is zero (default) this option has no effect.
.SS REDIRECT_MAX_TIMES
.sp
Default: \fB20\fP
.sp
Defines the maximum times a request can be redirected. After this maximum the
request\(aqs response is returned as is. We used Firefox default value for the
same task.
.SS REDIRECT_MAX_METAREFRESH_DELAY
.sp
Default: \fB100\fP
.sp
Some sites use meta\-refresh for redirecting to a session expired page, so we
restrict automatic redirection to a maximum delay (in seconds)
.SS REDIRECT_PRIORITY_ADJUST
.sp
Default: \fB+2\fP
.sp
Adjust redirect request priority relative to original request.
A negative priority adjust means more priority.
.SS ROBOTSTXT_OBEY
.sp
Default: \fBFalse\fP
.sp
Scope: \fBscrapy.contrib.downloadermiddleware.robotstxt\fP
.sp
If enabled, Scrapy will respect robots.txt policies. For more information see
\fItopics\-dlmw\-robots\fP
.SS SCHEDULER
.sp
Default: \fB\(aqscrapy.core.scheduler.Scheduler\(aq\fP
.sp
The scheduler to use for crawling.
.SS SPIDER_CONTRACTS
.sp
Default:: \fB{}\fP
.sp
A dict containing the scrapy contracts enabled in your project, used for
testing spiders. For more info see \fItopics\-contracts\fP\&.
.SS SPIDER_CONTRACTS_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqscrapy.contracts.default.UrlContract\(aq : 1,
    \(aqscrapy.contracts.default.ReturnsContract\(aq: 2,
    \(aqscrapy.contracts.default.ScrapesContract\(aq: 3,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify \fBSPIDER_CONTRACTS\fP
instead. For more info see \fItopics\-contracts\fP\&.
.SS SPIDER_MIDDLEWARES
.sp
Default:: \fB{}\fP
.sp
A dict containing the spider middlewares enabled in your project, and their
orders. For more info see \fItopics\-spider\-middleware\-setting\fP\&.
.SS SPIDER_MIDDLEWARES_BASE
.sp
Default:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    \(aqscrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware\(aq: 50,
    \(aqscrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware\(aq: 500,
    \(aqscrapy.contrib.spidermiddleware.referer.RefererMiddleware\(aq: 700,
    \(aqscrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware\(aq: 800,
    \(aqscrapy.contrib.spidermiddleware.depth.DepthMiddleware\(aq: 900,
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A dict containing the spider middlewares enabled by default in Scrapy. You
should never modify this setting in your project, modify
\fBSPIDER_MIDDLEWARES\fP instead. For more info see
\fItopics\-spider\-middleware\-setting\fP\&.
.SS SPIDER_MODULES
.sp
Default: \fB[]\fP
.sp
A list of modules where Scrapy will look for spiders.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
SPIDER_MODULES = [\(aqmybot.spiders_prod\(aq, \(aqmybot.spiders_dev\(aq]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS STATS_CLASS
.sp
Default: \fB\(aqscrapy.statscol.MemoryStatsCollector\(aq\fP
.sp
The class to use for collecting stats, who must implement the
\fItopics\-api\-stats\fP\&.
.SS STATS_DUMP
.sp
Default: \fBTrue\fP
.sp
Dump the \fIScrapy stats\fP (to the Scrapy log) once the spider
finishes.
.sp
For more info see: \fItopics\-stats\fP\&.
.SS STATSMAILER_RCPTS
.sp
Default: \fB[]\fP (empty list)
.sp
Send Scrapy stats after spiders finish scraping. See
\fBStatsMailer\fP for more info.
.SS TELNETCONSOLE_ENABLED
.sp
Default: \fBTrue\fP
.sp
A boolean which specifies if the \fItelnet console\fP
will be enabled (provided its extension is also enabled).
.SS TELNETCONSOLE_PORT
.sp
Default: \fB[6023, 6073]\fP
.sp
The port range to use for the telnet console. If set to \fBNone\fP or \fB0\fP, a
dynamically assigned port is used. For more info see
\fItopics\-telnetconsole\fP\&.
.SS TEMPLATES_DIR
.sp
Default: \fBtemplates\fP dir inside scrapy module
.sp
The directory where to look for templates when creating new projects with
\fBstartproject\fP command.
.SS URLLENGTH_LIMIT
.sp
Default: \fB2083\fP
.sp
Scope: \fBcontrib.spidermiddleware.urllength\fP
.sp
The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: \fI\%http://www.boutell.com/newfaq/misc/urllength.html\fP
.SS USER_AGENT
.sp
Default: \fB"Scrapy/VERSION (+http://scrapy.org)"\fP
.sp
The default User\-Agent to use when crawling, unless overridden.
.SS Signals
.sp
Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an \fIextension\fP, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.
.sp
Even though signals provide several arguments, the handlers that catch them
don\(aqt need to accept all of them \- the signal dispatching mechanism will only
deliver the arguments that the handler receives.
.sp
You can connect to signals (or send your own) through the
\fItopics\-api\-signals\fP\&.
.SS Deferred signal handlers
.sp
Some signals support returning \fI\%Twisted deferreds\fP from their handlers, see
the \fItopics\-signals\-ref\fP below to know which ones.
.SS Built\-in signals reference
.sp
Here\(aqs the list of Scrapy built\-in signals and their meaning.
.SS engine_started
.INDENT 0.0
.TP
.B scrapy.signals.engine_started()
Sent when the Scrapy engine has started crawling.
.sp
This signal supports returning deferreds from their handlers.
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
This signal may be fired \fIafter\fP the \fBspider_opened\fP signal,
depending on how the spider was started. So \fBdon\(aqt\fP rely on this signal
getting fired before \fBspider_opened\fP\&.
.UNINDENT
.UNINDENT
.SS engine_stopped
.INDENT 0.0
.TP
.B scrapy.signals.engine_stopped()
Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).
.sp
This signal supports returning deferreds from their handlers.
.UNINDENT
.SS item_scraped
.INDENT 0.0
.TP
.B scrapy.signals.item_scraped(item, response, spider)
Sent when an item has been scraped, after it has passed all the
\fItopics\-item\-pipeline\fP stages (without being dropped).
.sp
This signal supports returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBitem\fP (\fBItem\fP object) \-\- the item scraped
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response from where the item was scraped
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider which scraped the item
.UNINDENT
.UNINDENT
.UNINDENT
.SS item_dropped
.INDENT 0.0
.TP
.B scrapy.signals.item_dropped(item, spider, exception)
Sent after an item has been dropped from the \fItopics\-item\-pipeline\fP
when some stage raised a \fBDropItem\fP exception.
.sp
This signal supports returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBitem\fP (\fBItem\fP object) \-\- the item dropped from the \fItopics\-item\-pipeline\fP
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider which scraped the item
.IP \(bu 2
\fBexception\fP (\fBDropItem\fP exception) \-\- the exception (which must be a
\fBDropItem\fP subclass) which caused the item
to be dropped
.UNINDENT
.UNINDENT
.UNINDENT
.SS spider_closed
.INDENT 0.0
.TP
.B scrapy.signals.spider_closed(spider, reason)
Sent after a spider has been closed. This can be used to release per\-spider
resources reserved on \fBspider_opened\fP\&.
.sp
This signal supports returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider which has been closed
.IP \(bu 2
\fBreason\fP (\fIstr\fP) \-\- a string which describes the reason why the spider was closed. If
it was closed because the spider has completed scraping, the reason
is \fB\(aqfinished\(aq\fP\&. Otherwise, if the spider was manually closed by
calling the \fBclose_spider\fP engine method, then the reason is the one
passed in the \fBreason\fP argument of that method (which defaults to
\fB\(aqcancelled\(aq\fP). If the engine was shutdown (for example, by hitting
Ctrl\-C to stop it) the reason will be \fB\(aqshutdown\(aq\fP\&.
.UNINDENT
.UNINDENT
.UNINDENT
.SS spider_opened
.INDENT 0.0
.TP
.B scrapy.signals.spider_opened(spider)
Sent after a spider has been opened for crawling. This is typically used to
reserve per\-spider resources, but can be used for any task that needs to be
performed when a spider is opened.
.sp
This signal supports returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
\fBspider\fP (\fBSpider\fP object) \-\- the spider which has been opened
.UNINDENT
.UNINDENT
.SS spider_idle
.INDENT 0.0
.TP
.B scrapy.signals.spider_idle(spider)
Sent when a spider has gone idle, which means the spider has no further:
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
requests waiting to be downloaded
.IP \(bu 2
requests scheduled
.IP \(bu 2
items being processed in the item pipeline
.UNINDENT
.UNINDENT
.UNINDENT
.sp
If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the \fBspider_closed\fP signal is sent.
.sp
You can, for example, schedule some requests in your \fBspider_idle\fP
handler to prevent the spider from being closed.
.sp
This signal does not support returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
\fBspider\fP (\fBSpider\fP object) \-\- the spider which has gone idle
.UNINDENT
.UNINDENT
.SS spider_error
.INDENT 0.0
.TP
.B scrapy.signals.spider_error(failure, response, spider)
Sent when a spider callback generates an error (ie. raises an exception).
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfailure\fP (\fI\%Failure\fP object) \-\- the exception raised as a Twisted \fI\%Failure\fP object
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response being processed when the exception was raised
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider which raised the exception
.UNINDENT
.UNINDENT
.UNINDENT
.SS response_received
.INDENT 0.0
.TP
.B scrapy.signals.response_received(response, request, spider)
Sent when the engine receives a new \fBResponse\fP from the
downloader.
.sp
This signal does not support returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response received
.IP \(bu 2
\fBrequest\fP (\fBRequest\fP object) \-\- the request that generated the response
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which the response is intended
.UNINDENT
.UNINDENT
.UNINDENT
.SS response_downloaded
.INDENT 0.0
.TP
.B scrapy.signals.response_downloaded(response, request, spider)
Sent by the downloader right after a \fBHTTPResponse\fP is downloaded.
.sp
This signal does not support returning deferreds from their handlers.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBresponse\fP (\fBResponse\fP object) \-\- the response downloaded
.IP \(bu 2
\fBrequest\fP (\fBRequest\fP object) \-\- the request that generated the response
.IP \(bu 2
\fBspider\fP (\fBSpider\fP object) \-\- the spider for which the response is intended
.UNINDENT
.UNINDENT
.UNINDENT
.SS Exceptions
.SS Built\-in Exceptions reference
.sp
Here\(aqs a list of all exceptions included in Scrapy and their usage.
.SS DropItem
.INDENT 0.0
.TP
.B exception scrapy.exceptions.DropItem
.UNINDENT
.sp
The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see \fItopics\-item\-pipeline\fP\&.
.SS CloseSpider
.INDENT 0.0
.TP
.B exception scrapy.exceptions.CloseSpider(reason=\(aqcancelled\(aq)
This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:
.INDENT 7.0
.TP
.B Parameters
\fBreason\fP (\fIstr\fP) \-\- the reason for closing
.UNINDENT
.UNINDENT
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
def parse_page(self, response):
    if \(aqBandwidth exceeded\(aq in response.body:
        raise CloseSpider(\(aqbandwidth_exceeded\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS IgnoreRequest
.INDENT 0.0
.TP
.B exception scrapy.exceptions.IgnoreRequest
.UNINDENT
.sp
This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.
.SS NotConfigured
.INDENT 0.0
.TP
.B exception scrapy.exceptions.NotConfigured
.UNINDENT
.sp
This exception can be raised by some components to indicate that they will
remain disabled. Those components include:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
Extensions
.IP \(bu 2
Item pipelines
.IP \(bu 2
Downloader middlwares
.IP \(bu 2
Spider middlewares
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The exception must be raised in the component constructor.
.SS NotSupported
.INDENT 0.0
.TP
.B exception scrapy.exceptions.NotSupported
.UNINDENT
.sp
This exception is raised to indicate an unsupported feature.
.SS Item Exporters
.sp
Once you have scraped your Items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.
.sp
For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.
.SS Using Item Exporters
.sp
If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the \fItopics\-feed\-exports\fP\&. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.
.sp
In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in \fItopics\-exporters\-reference\fP\&. After you have
instantiated you exporter, you have to:
.sp
1. call the method \fBstart_exporting()\fP in order to
signal the beginning of the exporting process
.sp
2. call the \fBexport_item()\fP method for each item you want
to export
.sp
3. and finally call the \fBfinish_exporting()\fP to signal
the end of the exporting process
.sp
Here you can see an \fBItem Pipeline\fP which uses an Item
Exporter to export scraped items to different files, one per spider:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy import signals
from scrapy.contrib.exporter import XmlItemExporter

class XmlExportPipeline(object):

    def __init__(self):
        self.files = {}

     @classmethod
     def from_crawler(cls, crawler):
         pipeline = cls()
         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)
         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)
         return pipeline

    def spider_opened(self, spider):
        file = open(\(aq%s_products.xml\(aq % spider.name, \(aqw+b\(aq)
        self.files[spider] = file
        self.exporter = XmlItemExporter(file)
        self.exporter.start_exporting()

    def spider_closed(self, spider):
        self.exporter.finish_exporting()
        file = self.files.pop(spider)
        file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Serialization of item fields
.sp
By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.
.sp
However, you can customize how each field value is serialized \fIbefore it is
passed to the serialization library\fP\&.
.sp
There are two ways to customize how a field will be serialized, which are
described next.
.SS 1. Declaring a serializer in the field
.sp
You can declare a serializer in the \fIfield metadata\fP\&. The serializer must be a callable which receives a
value and returns its serialized form.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.item import Item, Field

def serialize_price(value):
   return \(aq$ %s\(aq % str(value)

class Product(Item):
    name = Field()
    price = Field(serializer=serialize_price)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS 2. Overriding the serialize_field() method
.sp
You can also override the \fBserialize()\fP method to
customize how your field value will be exported.
.sp
Make sure you call the base class \fBserialize()\fP method
after your custom code.
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from scrapy.contrib.exporter import XmlItemExporter

class ProductXmlExporter(XmlItemExporter):

    def serialize_field(self, field, name, value):
        if field == \(aqprice\(aq:
            return \(aq$ %s\(aq % str(value)
        return super(Product, self).serialize_field(field, name, value)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Built\-in Item Exporters reference
.sp
Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you\(aqre exporting these two items:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Item(name=\(aqColor TV\(aq, price=\(aq1200\(aq)
Item(name=\(aqDVD player\(aq, price=\(aq200\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS BaseItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.BaseItemExporter(fields_to_export=None, export_empty_fields=False, encoding=\(aqutf\-8\(aq)
This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.
.sp
These features can be configured through the constructor arguments which
populate their respective instance attributes: \fBfields_to_export\fP,
\fBexport_empty_fields\fP, \fBencoding\fP\&.
.INDENT 7.0
.TP
.B export_item(item)
Exports the given item. This method must be implemented in subclasses.
.UNINDENT
.INDENT 7.0
.TP
.B serialize_field(field, name, value)
Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.
.sp
By default, this method looks for a serializer \fIdeclared in the item
field\fP and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged except for \fBunicode\fP values which are encoded to
\fBstr\fP using the encoding declared in the \fBencoding\fP attribute.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfield\fP (\fBField\fP object) \-\- the field being serialized
.IP \(bu 2
\fBname\fP (\fIstr\fP) \-\- the name of the field being serialized
.IP \(bu 2
\fBvalue\fP \-\- the value being serialized
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B start_exporting()
Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
\fBXmlItemExporter\fP). You must call this method before exporting any
items.
.UNINDENT
.INDENT 7.0
.TP
.B finish_exporting()
Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
\fBXmlItemExporter\fP). You must always call this method after you
have no more items to export.
.UNINDENT
.INDENT 7.0
.TP
.B fields_to_export
A list with the name of the fields that will be exported, or None if you
want to export all fields. Defaults to None.
.sp
Some exporters (like \fBCsvItemExporter\fP) respect the order of the
fields defined in this attribute.
.UNINDENT
.INDENT 7.0
.TP
.B export_empty_fields
Whether to include empty/unpopulated item fields in the exported data.
Defaults to \fBFalse\fP\&. Some exporters (like \fBCsvItemExporter\fP)
ignore this attribute and always export all empty fields.
.UNINDENT
.INDENT 7.0
.TP
.B encoding
The encoding that will be used to encode unicode values. This only
affects unicode values (which are always serialized to str using this
encoding). Other value types are passed unchanged to the specific
serialization library.
.UNINDENT
.UNINDENT
.SS XmlItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.XmlItemExporter(file, item_element=\(aqitem\(aq, root_element=\(aqitems\(aq, **kwargs)
Exports Items in XML format to the specified file object.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.IP \(bu 2
\fBroot_element\fP (\fIstr\fP) \-\- The name of root element in the exported XML.
.IP \(bu 2
\fBitem_element\fP (\fIstr\fP) \-\- The name of each item element in the exported XML.
.UNINDENT
.UNINDENT
.sp
The additional keyword arguments of this constructor are passed to the
\fBBaseItemExporter\fP constructor.
.sp
A typical output of this exporter would be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
<?xml version="1.0" encoding="utf\-8"?>
<items>
  <item>
    <name>Color TV</name>
    <price>1200</price>
 </item>
  <item>
    <name>DVD player</name>
    <price>200</price>
 </item>
</items>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Unless overridden in the \fBserialize_field()\fP method, multi\-valued fields are
exported by serializing each value inside a \fB<value>\fP element. This is for
convenience, as multi\-valued fields are very common.
.sp
For example, the item:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
Item(name=[\(aqJohn\(aq, \(aqDoe\(aq], age=\(aq23\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Would be serialized as:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
<?xml version="1.0" encoding="utf\-8"?>
<items>
  <item>
    <name>
      <value>John</value>
      <value>Doe</value>
    </name>
    <age>23</age>
  </item>
</items>
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS CsvItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.CsvItemExporter(file, include_headers_line=True, join_multivalued=\(aq, \(aq, **kwargs)
Exports Items in CSV format to the given file\-like object. If the
\fBfields_to_export\fP attribute is set, it will be used to define the
CSV columns and their order. The \fBexport_empty_fields\fP attribute has
no effect on this exporter.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.IP \(bu 2
\fBinclude_headers_line\fP (\fIstr\fP) \-\- If enabled, makes the exporter output a header
line with the field names taken from
\fBBaseItemExporter.fields_to_export\fP or the first exported item fields.
.IP \(bu 2
\fBjoin_multivalued\fP \-\- The char (or chars) that will be used for joining
multi\-valued fields, if found.
.UNINDENT
.UNINDENT
.sp
The additional keyword arguments of this constructor are passed to the
\fBBaseItemExporter\fP constructor, and the leftover arguments to the
\fI\%csv.writer\fP constructor, so you can use any \fIcsv.writer\fP constructor
argument to customize this exporter.
.sp
A typical output of this exporter would be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
product,price
Color TV,1200
DVD player,200
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SS PickleItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.PickleItemExporter(file, protocol=0, **kwargs)
Exports Items in pickle format to the given file\-like object.
.INDENT 7.0
.TP
.B Parameters
.INDENT 7.0
.IP \(bu 2
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.IP \(bu 2
\fBprotocol\fP (\fIint\fP) \-\- The pickle protocol to use.
.UNINDENT
.UNINDENT
.sp
For more information, refer to the \fI\%pickle module documentation\fP\&.
.sp
The additional keyword arguments of this constructor are passed to the
\fBBaseItemExporter\fP constructor.
.sp
Pickle isn\(aqt a human readable format, so no output examples are provided.
.UNINDENT
.SS PprintItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.PprintItemExporter(file, **kwargs)
Exports Items in pretty print format to the specified file object.
.INDENT 7.0
.TP
.B Parameters
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.UNINDENT
.sp
The additional keyword arguments of this constructor are passed to the
\fBBaseItemExporter\fP constructor.
.sp
A typical output of this exporter would be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
{\(aqname\(aq: \(aqColor TV\(aq, \(aqprice\(aq: \(aq1200\(aq}
{\(aqname\(aq: \(aqDVD player\(aq, \(aqprice\(aq: \(aq200\(aq}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Longer lines (when present) are pretty\-formatted.
.UNINDENT
.SS JsonItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.JsonItemExporter(file, **kwargs)
Exports Items in JSON format to the specified file\-like object, writing all
objects as a list of objects. The additional constructor arguments are
passed to the \fBBaseItemExporter\fP constructor, and the leftover
arguments to the \fI\%JSONEncoder\fP constructor, so you can use any
\fI\%JSONEncoder\fP constructor argument to customize this exporter.
.INDENT 7.0
.TP
.B Parameters
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.UNINDENT
.sp
A typical output of this exporter would be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
[{"name": "Color TV", "price": "1200"},
{"name": "DVD player", "price": "200"}]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBWARNING:\fP
.INDENT 7.0
.INDENT 3.5
JSON is very simple and flexible serialization format, but it
doesn\(aqt scale well for large amounts of data since incremental (aka.
stream\-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream\-friendly format, consider using \fBJsonLinesItemExporter\fP
instead, or splitting the output in multiple chunks.
.UNINDENT
.UNINDENT
.UNINDENT
.SS JsonLinesItemExporter
.INDENT 0.0
.TP
.B class scrapy.contrib.exporter.JsonLinesItemExporter(file, **kwargs)
Exports Items in JSON format to the specified file\-like object, writing one
JSON\-encoded item per line. The additional constructor arguments are passed
to the \fBBaseItemExporter\fP constructor, and the leftover arguments to
the \fI\%JSONEncoder\fP constructor, so you can use any \fI\%JSONEncoder\fP
constructor argument to customize this exporter.
.INDENT 7.0
.TP
.B Parameters
\fBfile\fP \-\- the file\-like object to use for exporting the data.
.UNINDENT
.sp
A typical output of this exporter would be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
{"name": "Color TV", "price": "1200"}
{"name": "DVD player", "price": "200"}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Unlike the one produced by \fBJsonItemExporter\fP, the format produced by
this exporter is well suited for serializing large amounts of data.
.UNINDENT
.INDENT 0.0
.TP
.B \fBtopics/commands\fP
Learn about the command\-line tool and see all \fIavailable commands\fP\&.
.TP
.B \fBtopics/request\-response\fP
Understand the classes used to represent HTTP requests and responses.
.TP
.B \fBtopics/settings\fP
Learn how to configure Scrapy and see all \fIavailable settings\fP\&.
.TP
.B \fBtopics/signals\fP
See all available signals and how to work with them.
.TP
.B \fBtopics/exceptions\fP
See all available exceptions and their meaning.
.TP
.B \fBtopics/exporters\fP
Quickly export your scraped items to a file (XML, CSV, etc).
.UNINDENT
.SH ALL THE REST
.SS Release notes
.SS 0.21 (not released yet)
.INDENT 0.0
.IP \(bu 2
soon...
.UNINDENT
.SS 0.20.2 (released 2013\-12\-09)
.INDENT 0.0
.IP \(bu 2
Update CrawlSpider Template with Selector changes (\fI\%commit 6d1457d\fP)
.IP \(bu 2
fix method name in tutorial. closes GH\-480 (\fI\%commit b4fc359\fP
.UNINDENT
.SS 0.20.1 (released 2013\-11\-28)
.INDENT 0.0
.IP \(bu 2
include_package_data is required to build wheels from published sources (\fI\%commit 5ba1ad5\fP)
.IP \(bu 2
process_parallel was leaking the failures on its internal deferreds.  closes #458 (\fI\%commit 419a780\fP)
.UNINDENT
.SS 0.20.0 (released 2013\-11\-08)
.SS Enhancements
.INDENT 0.0
.IP \(bu 2
New Selector\(aqs API including CSS selectors (\fI\%issue 395\fP and \fI\%issue 426\fP),
.IP \(bu 2
Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)
.IP \(bu 2
\fBITEM_PIPELINES\fP is now defined as a dict (instead of a list)
.IP \(bu 2
Sitemap spider can fetch alternate URLs (\fI\%issue 360\fP)
.IP \(bu 2
\fISelector.remove_namespaces()\fP now remove namespaces from element\(aqs attributes. (\fI\%issue 416\fP)
.IP \(bu 2
Paved the road for Python 3.3+ (\fI\%issue 435\fP, \fI\%issue 436\fP, \fI\%issue 431\fP, \fI\%issue 452\fP)
.IP \(bu 2
New item exporter using native python types with nesting support (\fI\%issue 366\fP)
.IP \(bu 2
Tune HTTP1.1 pool size so it matches concurrency defined by settings (\fI\%commit b43b5f575\fP)
.IP \(bu 2
scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (\fI\%issue 327\fP)
.IP \(bu 2
New FilesPipeline with functionality factored out from ImagesPipeline (\fI\%issue 370\fP, \fI\%issue 409\fP)
.IP \(bu 2
Recommend Pillow instead of PIL for image handling (\fI\%issue 317\fP)
.IP \(bu 2
Added debian packages for Ubuntu quantal and raring (\fI\%commit 86230c0\fP)
.IP \(bu 2
Mock server (used for tests) can listen for HTTPS requests (\fI\%issue 410\fP)
.IP \(bu 2
Remove multi spider support from multiple core components
(\fI\%issue 422\fP, \fI\%issue 421\fP, \fI\%issue 420\fP, \fI\%issue 419\fP, \fI\%issue 423\fP, \fI\%issue 418\fP)
.IP \(bu 2
Travis\-CI now tests Scrapy changes against development versions of \fIw3lib\fP and \fIqueuelib\fP python packages.
.IP \(bu 2
Add pypy 2.1 to continous integration tests (\fI\%commit ecfa7431\fP)
.IP \(bu 2
Pylinted, pep8 and removed old\-style exceptions from source (\fI\%issue 430\fP, \fI\%issue 432\fP)
.IP \(bu 2
Use importlib for parametric imports (\fI\%issue 445\fP)
.IP \(bu 2
Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (\fI\%issue 372\fP)
.IP \(bu 2
Bugfix crawling shutdown on SIGINT (\fI\%issue 450\fP)
.IP \(bu 2
Do not submit \fIreset\fP type inputs in FormRequest.from_response (\fI\%commit b326b87\fP)
.IP \(bu 2
Do not silence download errors when request errback raises an exception (\fI\%commit 684cfc0\fP)
.UNINDENT
.SS Bugfixes
.INDENT 0.0
.IP \(bu 2
Fix tests under Django 1.6 (\fI\%commit b6bed44c\fP)
.IP \(bu 2
Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler
.IP \(bu 2
Fix inconsistencies among Twisted releases (\fI\%issue 406\fP)
.IP \(bu 2
Fix scrapy shell bugs (\fI\%issue 418\fP, \fI\%issue 407\fP)
.IP \(bu 2
Fix invalid variable name in setup.py (\fI\%issue 429\fP)
.IP \(bu 2
Fix tutorial references (\fI\%issue 387\fP)
.IP \(bu 2
Improve request\-response docs (\fI\%issue 391\fP)
.IP \(bu 2
Improve best practices docs (\fI\%issue 399\fP, \fI\%issue 400\fP, \fI\%issue 401\fP, \fI\%issue 402\fP)
.IP \(bu 2
Improve django integration docs (\fI\%issue 404\fP)
.IP \(bu 2
Document \fIbindaddress\fP request meta (\fI\%commit 37c24e01d7\fP)
.IP \(bu 2
Improve \fIRequest\fP class documentation (\fI\%issue 226\fP)
.UNINDENT
.SS Other
.INDENT 0.0
.IP \(bu 2
Dropped Python 2.6 support (\fI\%issue 448\fP)
.IP \(bu 2
Add \fI\%cssselect\fP python package as install dependency
.IP \(bu 2
Drop libxml2 and multi selector\(aqs backend support, \fI\%lxml\fP is required from now on.
.IP \(bu 2
Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.
.IP \(bu 2
Running test suite now requires \fImock\fP python library (\fI\%issue 390\fP)
.UNINDENT
.SS Thanks
.sp
Thanks to everyone who contribute to this release!
.sp
List of contributors sorted by number of commits:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
69 Daniel Graña <dangra@...>
37 Pablo Hoffman <pablo@...>
13 Mikhail Korobov <kmike84@...>
 9 Alex Cepoi <alex.cepoi@...>
 9 alexanderlukanin13 <alexander.lukanin.13@...>
 8 Rolando Espinoza La fuente <darkrho@...>
 8 Lukasz Biedrycki <lukasz.biedrycki@...>
 6 Nicolas Ramirez <nramirez.uy@...>
 3 Paul Tremberth <paul.tremberth@...>
 2 Martin Olveyra <molveyra@...>
 2 Stefan <misc@...>
 2 Rolando Espinoza <darkrho@...>
 2 Loren Davie <loren@...>
 2 irgmedeiros <irgmedeiros@...>
 1 Stefan Koch <taikano@...>
 1 Stefan <cct@...>
 1 scraperdragon <dragon@...>
 1 Kumara Tharmalingam <ktharmal@...>
 1 Francesco Piccinno <stack.box@...>
 1 Marcos Campal <duendex@...>
 1 Dragon Dave <dragon@...>
 1 Capi Etheriel <barraponto@...>
 1 cacovsky <amarquesferraz@...>
 1 Berend Iwema <berend@...>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS 0.18.4 (released 2013\-10\-10)
.INDENT 0.0
.IP \(bu 2
IPython refuses to update the namespace. fix #396 (\fI\%commit 3d32c4f\fP)
.IP \(bu 2
Fix AlreadyCalledError replacing a request in shell command. closes #407 (\fI\%commit b1d8919\fP)
.IP \(bu 2
Fix start_requests lazyness and early hangs (\fI\%commit 89faf52\fP)
.UNINDENT
.SS 0.18.3 (released 2013\-10\-03)
.INDENT 0.0
.IP \(bu 2
fix regression on lazy evaluation of start requests (\fI\%commit 12693a5\fP)
.IP \(bu 2
forms: do not submit reset inputs (\fI\%commit e429f63\fP)
.IP \(bu 2
increase unittest timeouts to decrease travis false positive failures (\fI\%commit 912202e\fP)
.IP \(bu 2
backport master fixes to json exporter (\fI\%commit cfc2d46\fP)
.IP \(bu 2
Fix permission and set umask before generating sdist tarball (\fI\%commit 06149e0\fP)
.UNINDENT
.SS 0.18.2 (released 2013\-09\-03)
.INDENT 0.0
.IP \(bu 2
Backport \fIscrapy check\fP command fixes and backward compatible multi
crawler process(\fI\%issue 339\fP)
.UNINDENT
.SS 0.18.1 (released 2013\-08\-27)
.INDENT 0.0
.IP \(bu 2
remove extra import added by cherry picked changes (\fI\%commit d20304e\fP)
.IP \(bu 2
fix crawling tests under twisted pre 11.0.0 (\fI\%commit 1994f38\fP)
.IP \(bu 2
py26 can not format zero length fields {} (\fI\%commit abf756f\fP)
.IP \(bu 2
test PotentiaDataLoss errors on unbound responses (\fI\%commit b15470d\fP)
.IP \(bu 2
Treat responses without content\-length or Transfer\-Encoding as good responses (\fI\%commit c4bf324\fP)
.IP \(bu 2
do no include ResponseFailed if http11 handler is not enabled (\fI\%commit 6cbe684\fP)
.IP \(bu 2
New HTTP client wraps connection losts in ResponseFailed exception. fix #373 (\fI\%commit 1a20bba\fP)
.IP \(bu 2
limit travis\-ci build matrix (\fI\%commit 3b01bb8\fP)
.IP \(bu 2
Merge pull request #375 from peterarenot/patch\-1 (\fI\%commit fa766d7\fP)
.IP \(bu 2
Fixed so it refers to the correct folder (\fI\%commit 3283809\fP)
.IP \(bu 2
added quantal & raring to support ubuntu releases (\fI\%commit 1411923\fP)
.IP \(bu 2
fix retry middleware which didn\(aqt retry certain connection errors after the upgrade to http1 client, closes GH\-373 (\fI\%commit bb35ed0\fP)
.IP \(bu 2
fix XmlItemExporter in Python 2.7.4 and 2.7.5 (\fI\%commit de3e451\fP)
.IP \(bu 2
minor updates to 0.18 release notes (\fI\%commit c45e5f1\fP)
.IP \(bu 2
fix contributters list format (\fI\%commit 0b60031\fP)
.UNINDENT
.SS 0.18.0 (released 2013\-08\-09)
.INDENT 0.0
.IP \(bu 2
Lot of improvements to testsuite run using Tox, including a way to test on pypi
.IP \(bu 2
Handle GET parameters for AJAX crawleable urls (\fI\%commit 3fe2a32\fP)
.IP \(bu 2
Use lxml recover option to parse sitemaps (\fI\%issue 347\fP)
.IP \(bu 2
Bugfix cookie merging by hostname and not by netloc (\fI\%issue 352\fP)
.IP \(bu 2
Support disabling \fIHttpCompressionMiddleware\fP using a flag setting (\fI\%issue 359\fP)
.IP \(bu 2
Support xml namespaces using \fIiternodes\fP parser in \fIXMLFeedSpider\fP (\fI\%issue 12\fP)
.IP \(bu 2
Support \fIdont_cache\fP request meta flag (\fI\%issue 19\fP)
.IP \(bu 2
Bugfix \fIscrapy.utils.gz.gunzip\fP broken by changes in python 2.7.4 (\fI\%commit 4dc76e\fP)
.IP \(bu 2
Bugfix url encoding on \fISgmlLinkExtractor\fP (\fI\%issue 24\fP)
.IP \(bu 2
Bugfix \fITakeFirst\fP processor shouldn\(aqt discard zero (0) value (\fI\%issue 59\fP)
.IP \(bu 2
Support nested items in xml exporter (\fI\%issue 66\fP)
.IP \(bu 2
Improve cookies handling performance (\fI\%issue 77\fP)
.IP \(bu 2
Log dupe filtered requests once (\fI\%issue 105\fP)
.IP \(bu 2
Split redirection middleware into status and meta based middlewares (\fI\%issue 78\fP)
.IP \(bu 2
Use HTTP1.1 as default downloader handler (\fI\%issue 109\fP and \fI\%issue 318\fP)
.IP \(bu 2
Support xpath form selection on \fIFormRequest.from_response\fP (\fI\%issue 185\fP)
.IP \(bu 2
Bugfix unicode decoding error on \fISgmlLinkExtractor\fP (\fI\%issue 199\fP)
.IP \(bu 2
Bugfix signal dispatching on pypi interpreter (\fI\%issue 205\fP)
.IP \(bu 2
Improve request delay and concurrency handling (\fI\%issue 206\fP)
.IP \(bu 2
Add RFC2616 cache policy to \fIHttpCacheMiddleware\fP (\fI\%issue 212\fP)
.IP \(bu 2
Allow customization of messages logged by engine (\fI\%issue 214\fP)
.IP \(bu 2
Multiples improvements to \fIDjangoItem\fP (\fI\%issue 217\fP, \fI\%issue 218\fP, \fI\%issue 221\fP)
.IP \(bu 2
Extend Scrapy commands using setuptools entry points (\fI\%issue 260\fP)
.IP \(bu 2
Allow spider \fIallowed_domains\fP value to be set/tuple (\fI\%issue 261\fP)
.IP \(bu 2
Support \fIsettings.getdict\fP (\fI\%issue 269\fP)
.IP \(bu 2
Simplify internal \fIscrapy.core.scraper\fP slot handling (\fI\%issue 271\fP)
.IP \(bu 2
Added \fIItem.copy\fP (\fI\%issue 290\fP)
.IP \(bu 2
Collect idle downloader slots (\fI\%issue 297\fP)
.IP \(bu 2
Add \fIftp://\fP scheme downloader handler (\fI\%issue 329\fP)
.IP \(bu 2
Added downloader benchmark webserver and spider tools \fIbenchmarking\fP
.IP \(bu 2
Moved persistent (on disk) queues to a separate project (\fI\%queuelib\fP) which scrapy now depends on
.IP \(bu 2
Add scrapy commands using external libraries (\fI\%issue 260\fP)
.IP \(bu 2
Added \fB\-\-pdb\fP option to \fBscrapy\fP command line tool
.IP \(bu 2
Added \fBXPathSelector.remove_namespaces()\fP which allows to remove all namespaces from XML documents for convenience (to work with namespace\-less XPaths). Documented in \fItopics\-selectors\fP\&.
.IP \(bu 2
Several improvements to spider contracts
.IP \(bu 2
New default middleware named MetaRefreshMiddldeware that handles meta\-refresh html tag redirections,
.IP \(bu 2
MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62
.IP \(bu 2
added from_crawler method to spiders
.IP \(bu 2
added system tests with mock server
.IP \(bu 2
more improvements to Mac OS compatibility (thanks Alex Cepoi)
.IP \(bu 2
several more cleanups to singletons and multi\-spider support (thanks Nicolas Ramirez)
.IP \(bu 2
support custom download slots
.IP \(bu 2
added \-\-spider option to "shell" command.
.IP \(bu 2
log overridden settings when scrapy starts
.UNINDENT
.sp
Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
130 Pablo Hoffman <pablo@...>
 97 Daniel Graña <dangra@...>
 20 Nicolás Ramírez <nramirez.uy@...>
 13 Mikhail Korobov <kmike84@...>
 12 Pedro Faustino <pedrobandim@...>
 11 Steven Almeroth <sroth77@...>
  5 Rolando Espinoza La fuente <darkrho@...>
  4 Michal Danilak <mimino.coder@...>
  4 Alex Cepoi <alex.cepoi@...>
  4 Alexandr N Zamaraev (aka tonal) <tonal@...>
  3 paul <paul.tremberth@...>
  3 Martin Olveyra <molveyra@...>
  3 Jordi Llonch <llonchj@...>
  3 arijitchakraborty <myself.arijit@...>
  2 Shane Evans <shane.evans@...>
  2 joehillen <joehillen@...>
  2 Hart <HartSimha@...>
  2 Dan <ellisd23@...>
  1 Zuhao Wan <wanzuhao@...>
  1 whodatninja <blake@...>
  1 vkrest <v.krestiannykov@...>
  1 tpeng <pengtaoo@...>
  1 Tom Mortimer\-Jones <tom@...>
  1 Rocio Aramberri <roschegel@...>
  1 Pedro <pedro@...>
  1 notsobad <wangxiaohugg@...>
  1 Natan L <kuyanatan.nlao@...>
  1 Mark Grey <mark.grey@...>
  1 Luan <luanpab@...>
  1 Libor Nenadál <libor.nenadal@...>
  1 Juan M Uys <opyate@...>
  1 Jonas Brunsgaard <jonas.brunsgaard@...>
  1 Ilya Baryshev <baryshev@...>
  1 Hasnain Lakhani <m.hasnain.lakhani@...>
  1 Emanuel Schorsch <emschorsch@...>
  1 Chris Tilden <chris.tilden@...>
  1 Capi Etheriel <barraponto@...>
  1 cacovsky <amarquesferraz@...>
  1 Berend Iwema <berend@...>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS 0.16.5 (released 2013\-05\-30)
.INDENT 0.0
.IP \(bu 2
obey request method when scrapy deploy is redirected to a new endpoint (\fI\%commit 8c4fcee\fP)
.IP \(bu 2
fix inaccurate downloader middleware documentation. refs #280 (\fI\%commit 40667cb\fP)
.IP \(bu 2
doc: remove links to diveintopython.org, which is no longer available. closes #246 (\fI\%commit bd58bfa\fP)
.IP \(bu 2
Find form nodes in invalid html5 documents (\fI\%commit e3d6945\fP)
.IP \(bu 2
Fix typo labeling attrs type bool instead of list (\fI\%commit a274276\fP)
.UNINDENT
.SS 0.16.4 (released 2013\-01\-23)
.INDENT 0.0
.IP \(bu 2
fixes spelling errors in documentation (\fI\%commit 6d2b3aa\fP)
.IP \(bu 2
add doc about disabling an extension. refs #132 (\fI\%commit c90de33\fP)
.IP \(bu 2
Fixed error message formatting. log.err() doesn\(aqt support cool formatting and when error occured, the message was:    "ERROR: Error processing %(item)s" (\fI\%commit c16150c\fP)
.IP \(bu 2
lint and improve images pipeline error logging (\fI\%commit 56b45fc\fP)
.IP \(bu 2
fixed doc typos (\fI\%commit 243be84\fP)
.IP \(bu 2
add documentation topics: Broad Crawls & Common Practies (\fI\%commit 1fbb715\fP)
.IP \(bu 2
fix bug in scrapy parse command when spider is not specified explicitly. closes #209 (\fI\%commit c72e682\fP)
.IP \(bu 2
Update docs/topics/commands.rst (\fI\%commit 28eac7a\fP)
.UNINDENT
.SS 0.16.3 (released 2012\-12\-07)
.INDENT 0.0
.IP \(bu 2
Remove concurrency limitation when using download delays and still ensure inter\-request delays are enforced (\fI\%commit 487b9b5\fP)
.IP \(bu 2
add error details when image pipeline fails (\fI\%commit 8232569\fP)
.IP \(bu 2
improve mac os compatibility (\fI\%commit 8dcf8aa\fP)
.IP \(bu 2
setup.py: use README.rst to populate long_description (\fI\%commit 7b5310d\fP)
.IP \(bu 2
doc: removed obsolete references to ClientForm (\fI\%commit 80f9bb6\fP)
.IP \(bu 2
correct docs for default storage backend (\fI\%commit 2aa491b\fP)
.IP \(bu 2
doc: removed broken proxyhub link from FAQ (\fI\%commit bdf61c4\fP)
.IP \(bu 2
Fixed docs typo in SpiderOpenCloseLogging example (\fI\%commit 7184094\fP)
.UNINDENT
.SS 0.16.2 (released 2012\-11\-09)
.INDENT 0.0
.IP \(bu 2
scrapy contracts: python2.6 compat (\fI\%commit a4a9199\fP)
.IP \(bu 2
scrapy contracts verbose option (\fI\%commit ec41673\fP)
.IP \(bu 2
proper unittest\-like output for scrapy contracts (\fI\%commit 86635e4\fP)
.IP \(bu 2
added open_in_browser to debugging doc (\fI\%commit c9b690d\fP)
.IP \(bu 2
removed reference to global scrapy stats from settings doc (\fI\%commit dd55067\fP)
.IP \(bu 2
Fix SpiderState bug in Windows platforms (\fI\%commit 58998f4\fP)
.UNINDENT
.SS 0.16.1 (released 2012\-10\-26)
.INDENT 0.0
.IP \(bu 2
fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (\fI\%commit 8c780fd\fP)
.IP \(bu 2
better backwards compatibility for scrapy.conf.settings (\fI\%commit 3403089\fP)
.IP \(bu 2
extended documentation on how to access crawler stats from extensions (\fI\%commit c4da0b5\fP)
.IP \(bu 2
removed .hgtags (no longer needed now that scrapy uses git) (\fI\%commit d52c188\fP)
.IP \(bu 2
fix dashes under rst headers (\fI\%commit fa4f7f9\fP)
.IP \(bu 2
set release date for 0.16.0 in news (\fI\%commit e292246\fP)
.UNINDENT
.SS 0.16.0 (released 2012\-10\-18)
.sp
Scrapy changes:
.INDENT 0.0
.IP \(bu 2
added \fItopics\-contracts\fP, a mechanism for testing spiders in a formal/reproducible way
.IP \(bu 2
added options \fB\-o\fP and \fB\-t\fP to the \fBrunspider\fP command
.IP \(bu 2
documented \fBtopics/autothrottle\fP and added to extensions installed by default. You still need to enable it with \fBAUTOTHROTTLE_ENABLED\fP
.IP \(bu 2
major Stats Collection refactoring: removed separation of global/per\-spider stats, removed stats\-related signals (\fBstats_spider_opened\fP, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.
.IP \(bu 2
added \fBprocess_start_requests()\fP method to spider middlewares
.IP \(bu 2
dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.
.IP \(bu 2
dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.
.IP \(bu 2
dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.
.IP \(bu 2
documented \fItopics\-api\fP
.IP \(bu 2
\fIlxml\fP is now the default selectors backend instead of \fIlibxml2\fP
.IP \(bu 2
ported FormRequest.from_response() to use \fI\%lxml\fP instead of \fI\%ClientForm\fP
.IP \(bu 2
removed modules: \fBscrapy.xlib.BeautifulSoup\fP and \fBscrapy.xlib.ClientForm\fP
.IP \(bu 2
SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (\fI\%commit 10ed28b\fP)
.IP \(bu 2
StackTraceDump extension: also dump trackref live references (\fI\%commit fe2ce93\fP)
.IP \(bu 2
nested items now fully supported in JSON and JSONLines exporters
.IP \(bu 2
added \fBcookiejar\fP Request meta key to support multiple cookie sessions per spider
.IP \(bu 2
decoupled encoding detection code to \fI\%w3lib.encoding\fP, and ported Scrapy code to use that mdule
.IP \(bu 2
dropped support for Python 2.5. See \fI\%http://blog.scrapy.org/scrapy\-dropping\-support\-for\-python\-25\fP
.IP \(bu 2
dropped support for Twisted 2.5
.IP \(bu 2
added \fBREFERER_ENABLED\fP setting, to control referer middleware
.IP \(bu 2
changed default user agent to: \fBScrapy/VERSION (+http://scrapy.org)\fP
.IP \(bu 2
removed (undocumented) \fBHTMLImageLinkExtractor\fP class from \fBscrapy.contrib.linkextractors.image\fP
.IP \(bu 2
removed per\-spider settings (to be replaced by instantiating multiple crawler objects)
.IP \(bu 2
\fBUSER_AGENT\fP spider attribute will no longer work, use \fBuser_agent\fP attribute instead
.IP \(bu 2
\fBDOWNLOAD_TIMEOUT\fP spider attribute will no longer work, use \fBdownload_timeout\fP attribute instead
.IP \(bu 2
removed \fBENCODING_ALIASES\fP setting, as encoding auto\-detection has been moved to the \fI\%w3lib\fP library
.IP \(bu 2
promoted \fItopics\-djangoitem\fP to main contrib
.IP \(bu 2
LogFormatter method now return dicts(instead of strings) to support lazy formatting (\fI\%issue 164\fP, \fI\%commit dcef7b0\fP)
.IP \(bu 2
downloader handlers (\fBDOWNLOAD_HANDLERS\fP setting) now receive settings as the first argument of the constructor
.IP \(bu 2
replaced memory usage acounting with (more portable) \fI\%resource\fP module, removed \fBscrapy.utils.memory\fP module
.IP \(bu 2
removed signal: \fBscrapy.mail.mail_sent\fP
.IP \(bu 2
removed \fBTRACK_REFS\fP setting, now \fItrackrefs\fP is always enabled
.IP \(bu 2
DBM is now the default storage backend for HTTP cache middleware
.IP \(bu 2
number of log messages (per level) are now tracked through Scrapy stats (stat name: \fBlog_count/LEVEL\fP)
.IP \(bu 2
number received responses are now tracked through Scrapy stats (stat name: \fBresponse_received_count\fP)
.IP \(bu 2
removed \fBscrapy.log.started\fP attribute
.UNINDENT
.SS 0.14.4
.INDENT 0.0
.IP \(bu 2
added precise to supported ubuntu distros (\fI\%commit b7e46df\fP)
.IP \(bu 2
fixed bug in json\-rpc webservice reported in \fI\%https://groups.google.com/d/topic/scrapy\-users/qgVBmFybNAQ/discussion\fP\&. also removed no longer supported \(aqrun\(aq command from extras/scrapy\-ws.py (\fI\%commit 340fbdb\fP)
.IP \(bu 2
meta tag attributes for content\-type http equiv can be in any order. #123 (\fI\%commit 0cb68af\fP)
.IP \(bu 2
replace "import Image" by more standard "from PIL import Image". closes #88 (\fI\%commit 4d17048\fP)
.IP \(bu 2
return trial status as bin/runtests.sh exit value. #118 (\fI\%commit b7b2e7f\fP)
.UNINDENT
.SS 0.14.3
.INDENT 0.0
.IP \(bu 2
forgot to include pydispatch license. #118 (\fI\%commit fd85f9c\fP)
.IP \(bu 2
include egg files used by testsuite in source distribution. #118 (\fI\%commit c897793\fP)
.IP \(bu 2
update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (\fI\%commit 2548dcc\fP)
.IP \(bu 2
added note to docs/topics/firebug.rst about google directory being shut down (\fI\%commit 668e352\fP)
.IP \(bu 2
dont discard slot when empty, just save in another dict in order to recycle if needed again. (\fI\%commit 8e9f607\fP)
.IP \(bu 2
do not fail handling unicode xpaths in libxml2 backed selectors (\fI\%commit b830e95\fP)
.IP \(bu 2
fixed minor mistake in Request objects documentation (\fI\%commit bf3c9ee\fP)
.IP \(bu 2
fixed minor defect in link extractors documentation (\fI\%commit ba14f38\fP)
.IP \(bu 2
removed some obsolete remaining code related to sqlite support in scrapy (\fI\%commit 0665175\fP)
.UNINDENT
.SS 0.14.2
.INDENT 0.0
.IP \(bu 2
move buffer pointing to start of file before computing checksum. refs #92 (\fI\%commit 6a5bef2\fP)
.IP \(bu 2
Compute image checksum before persisting images. closes #92 (\fI\%commit 9817df1\fP)
.IP \(bu 2
remove leaking references in cached failures (\fI\%commit 673a120\fP)
.IP \(bu 2
fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (\fI\%commit 11133e9\fP)
.IP \(bu 2
fixed struct.error on http compression middleware. closes #87 (\fI\%commit 1423140\fP)
.IP \(bu 2
ajax crawling wasn\(aqt expanding for unicode urls (\fI\%commit 0de3fb4\fP)
.IP \(bu 2
Catch start_requests iterator errors. refs #83 (\fI\%commit 454a21d\fP)
.IP \(bu 2
Speed\-up libxml2 XPathSelector (\fI\%commit 2fbd662\fP)
.IP \(bu 2
updated versioning doc according to recent changes (\fI\%commit 0a070f5\fP)
.IP \(bu 2
scrapyd: fixed documentation link (\fI\%commit 2b4e4c3\fP)
.IP \(bu 2
extras/makedeb.py: no longer obtaining version from git (\fI\%commit caffe0e\fP)
.UNINDENT
.SS 0.14.1
.INDENT 0.0
.IP \(bu 2
extras/makedeb.py: no longer obtaining version from git (\fI\%commit caffe0e\fP)
.IP \(bu 2
bumped version to 0.14.1 (\fI\%commit 6cb9e1c\fP)
.IP \(bu 2
fixed reference to tutorial directory (\fI\%commit 4b86bd6\fP)
.IP \(bu 2
doc: removed duplicated callback argument from Request.replace() (\fI\%commit 1aeccdd\fP)
.IP \(bu 2
fixed formatting of scrapyd doc (\fI\%commit 8bf19e6\fP)
.IP \(bu 2
Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (\fI\%commit 14a8e6e\fP)
.IP \(bu 2
added comment about why we disable ssl on boto images upload (\fI\%commit 5223575\fP)
.IP \(bu 2
SSL handshaking hangs when doing too many parallel connections to S3 (\fI\%commit 63d583d\fP)
.IP \(bu 2
change tutorial to follow changes on dmoz site (\fI\%commit bcb3198\fP)
.IP \(bu 2
Avoid _disconnectedDeferred AttributeError exception in Twisted>=11.1.0 (\fI\%commit 98f3f87\fP)
.IP \(bu 2
allow spider to set autothrottle max concurrency (\fI\%commit 175a4b5\fP)
.UNINDENT
.SS 0.14
.SS New features and settings
.INDENT 0.0
.IP \(bu 2
Support for \fI\%AJAX crawleable urls\fP
.IP \(bu 2
New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (\fI\%r2737\fP)
.IP \(bu 2
added \fB\-o\fP option to \fBscrapy crawl\fP, a shortcut for dumping scraped items into a file (or standard output using \fB\-\fP)
.IP \(bu 2
Added support for passing custom settings to Scrapyd \fBschedule.json\fP api (\fI\%r2779\fP, \fI\%r2783\fP)
.IP \(bu 2
New \fBChunkedTransferMiddleware\fP (enabled by default) to support \fI\%chunked transfer encoding\fP (\fI\%r2769\fP)
.IP \(bu 2
Add boto 2.0 support for S3 downloader handler (\fI\%r2763\fP)
.IP \(bu 2
Added \fI\%marshal\fP to formats supported by feed exports (\fI\%r2744\fP)
.IP \(bu 2
In request errbacks, offending requests are now received in \fIfailure.request\fP attribute (\fI\%r2738\fP)
.IP \(bu 2
.INDENT 2.0
.TP
.B Big downloader refactoring to support per domain/ip concurrency limits (\fI\%r2732\fP)
.INDENT 7.0
.IP \(bu 2
.INDENT 2.0
.TP
.B \fBCONCURRENT_REQUESTS_PER_SPIDER\fP setting has been deprecated and replaced by:
.INDENT 7.0
.IP \(bu 2
\fBCONCURRENT_REQUESTS\fP, \fBCONCURRENT_REQUESTS_PER_DOMAIN\fP, \fBCONCURRENT_REQUESTS_PER_IP\fP
.UNINDENT
.UNINDENT
.IP \(bu 2
check the documentation for more details
.UNINDENT
.UNINDENT
.IP \(bu 2
Added builtin caching DNS resolver (\fI\%r2728\fP)
.IP \(bu 2
Moved Amazon AWS\-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](\fI\%https://github.com/scrapinghub/scaws\fP) (\fI\%r2706\fP, \fI\%r2714\fP)
.IP \(bu 2
Moved spider queues to scrapyd: \fIscrapy.spiderqueue\fP \-> \fIscrapyd.spiderqueue\fP (\fI\%r2708\fP)
.IP \(bu 2
Moved sqlite utils to scrapyd: \fIscrapy.utils.sqlite\fP \-> \fIscrapyd.sqlite\fP (\fI\%r2781\fP)
.IP \(bu 2
Real support for returning iterators on \fIstart_requests()\fP method. The iterator is now consumed during the crawl when the spider is getting idle (\fI\%r2704\fP)
.IP \(bu 2
Added \fBREDIRECT_ENABLED\fP setting to quickly enable/disable the redirect middleware (\fI\%r2697\fP)
.IP \(bu 2
Added \fBRETRY_ENABLED\fP setting to quickly enable/disable the retry middleware (\fI\%r2694\fP)
.IP \(bu 2
Added \fBCloseSpider\fP exception to manually close spiders (\fI\%r2691\fP)
.IP \(bu 2
Improved encoding detection by adding support for HTML5 meta charset declaration (\fI\%r2690\fP)
.IP \(bu 2
Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (\fI\%r2688\fP)
.IP \(bu 2
Added \fBSitemapSpider\fP (see documentation in Spiders page) (\fI\%r2658\fP)
.IP \(bu 2
Added \fBLogStats\fP extension for periodically logging basic stats (like crawled pages and scraped items) (\fI\%r2657\fP)
.IP \(bu 2
Make handling of gzipped responses more robust (#319, \fI\%r2643\fP). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an \fIIOError\fP\&.
.IP \(bu 2
Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (\fI\%r2639\fP)
.IP \(bu 2
Added new command to edit spiders: \fBscrapy edit\fP (\fI\%r2636\fP) and \fI\-e\fP flag to \fIgenspider\fP command that uses it (\fI\%r2653\fP)
.IP \(bu 2
Changed default representation of items to pretty\-printed dicts. (\fI\%r2631\fP). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.
.IP \(bu 2
Added \fBspider_error\fP signal (\fI\%r2628\fP)
.IP \(bu 2
Added \fBCOOKIES_ENABLED\fP setting (\fI\%r2625\fP)
.IP \(bu 2
Stats are now dumped to Scrapy log (default value of \fBSTATS_DUMP\fP setting has been changed to \fITrue\fP). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.
.IP \(bu 2
Added support for dynamically adjusting download delay and maximum concurrent requests (\fI\%r2599\fP)
.IP \(bu 2
Added new DBM HTTP cache storage backend (\fI\%r2576\fP)
.IP \(bu 2
Added \fBlistjobs.json\fP API to Scrapyd (\fI\%r2571\fP)
.IP \(bu 2
\fBCsvItemExporter\fP: added \fBjoin_multivalued\fP parameter (\fI\%r2578\fP)
.IP \(bu 2
Added namespace support to \fBxmliter_lxml\fP (\fI\%r2552\fP)
.IP \(bu 2
Improved cookies middleware by making \fICOOKIES_DEBUG\fP nicer and documenting it (\fI\%r2579\fP)
.IP \(bu 2
Several improvements to Scrapyd and Link extractors
.UNINDENT
.SS Code rearranged and removed
.INDENT 0.0
.IP \(bu 2
.INDENT 2.0
.TP
.B Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (\fI\%r2630\fP)
.INDENT 7.0
.IP \(bu 2
original item_scraped signal was removed
.IP \(bu 2
original item_passed signal was renamed to item_scraped
.IP \(bu 2
old log lines \fBScraped Item...\fP were removed
.IP \(bu 2
old log lines \fBPassed Item...\fP were renamed to \fBScraped Item...\fP lines and downgraded to \fBDEBUG\fP level
.UNINDENT
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:
.INDENT 7.0
.IP \(bu 2
\fI\%w3lib\fP (several functions from \fBscrapy.utils.{http,markup,multipart,response,url}\fP, done in \fI\%r2584\fP)
.IP \(bu 2
\fI\%scrapely\fP (was \fBscrapy.contrib.ibl\fP, done in \fI\%r2586\fP)
.UNINDENT
.UNINDENT
.IP \(bu 2
Removed unused function: \fIscrapy.utils.request.request_info()\fP (\fI\%r2577\fP)
.IP \(bu 2
Removed googledir project from \fIexamples/googledir\fP\&. There\(aqs now a new example project called \fIdirbot\fP available on github: \fI\%https://github.com/scrapy/dirbot\fP
.IP \(bu 2
Removed support for default field values in Scrapy items (\fI\%r2616\fP)
.IP \(bu 2
Removed experimental crawlspider v2 (\fI\%r2632\fP)
.IP \(bu 2
Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (\fIDUPEFILTER_CLASS\fP setting) (\fI\%r2640\fP)
.IP \(bu 2
Removed support for passing urls to \fBscrapy crawl\fP command (use \fBscrapy parse\fP instead) (\fI\%r2704\fP)
.IP \(bu 2
Removed deprecated Execution Queue (\fI\%r2704\fP)
.IP \(bu 2
Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (\fI\%r2780\fP)
.IP \(bu 2
removed \fBCONCURRENT_SPIDERS\fP setting (use scrapyd maxproc instead) (\fI\%r2789\fP)
.IP \(bu 2
Renamed attributes of core components: downloader.sites \-> downloader.slots, scraper.sites \-> scraper.slots (\fI\%r2717\fP, \fI\%r2718\fP)
.IP \(bu 2
Renamed setting \fBCLOSESPIDER_ITEMPASSED\fP to \fBCLOSESPIDER_ITEMCOUNT\fP (\fI\%r2655\fP). Backwards compatibility kept.
.UNINDENT
.SS 0.12
.sp
The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.
.SS New features and improvements
.INDENT 0.0
.IP \(bu 2
Passed item is now sent in the \fBitem\fP argument of the \fBitem_passed\fP (#273)
.IP \(bu 2
Added verbose option to \fBscrapy version\fP command, useful for bug reports (#298)
.IP \(bu 2
HTTP cache now stored by default in the project data dir (#279)
.IP \(bu 2
Added project data storage directory (#276, #277)
.IP \(bu 2
Documented file structure of Scrapy projects (see command\-line tool doc)
.IP \(bu 2
New lxml backend for XPath selectors (#147)
.IP \(bu 2
Per\-spider settings (#245)
.IP \(bu 2
Support exit codes to signal errors in Scrapy commands (#248)
.IP \(bu 2
Added \fB\-c\fP argument to \fBscrapy shell\fP command
.IP \(bu 2
Made \fBlibxml2\fP optional (#260)
.IP \(bu 2
New \fBdeploy\fP command (#261)
.IP \(bu 2
Added \fBCLOSESPIDER_PAGECOUNT\fP setting (#253)
.IP \(bu 2
Added \fBCLOSESPIDER_ERRORCOUNT\fP setting (#254)
.UNINDENT
.SS Scrapyd changes
.INDENT 0.0
.IP \(bu 2
Scrapyd now uses one process per spider
.IP \(bu 2
It stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)
.IP \(bu 2
A minimal web ui was added, available at \fI\%http://localhost:6800\fP by default
.IP \(bu 2
There is now a \fIscrapy server\fP command to start a Scrapyd server of the current project
.UNINDENT
.SS Changes to settings
.INDENT 0.0
.IP \(bu 2
added \fIHTTPCACHE_ENABLED\fP setting (False by default) to enable HTTP cache middleware
.IP \(bu 2
changed \fIHTTPCACHE_EXPIRATION_SECS\fP semantics: now zero means "never expire".
.UNINDENT
.SS Deprecated/obsoleted functionality
.INDENT 0.0
.IP \(bu 2
Deprecated \fBrunserver\fP command in favor of \fBserver\fP command which starts a Scrapyd server. See also: Scrapyd changes
.IP \(bu 2
Deprecated \fBqueue\fP command in favor of using Scrapyd \fBschedule.json\fP API. See also: Scrapyd changes
.IP \(bu 2
Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)
.UNINDENT
.SS 0.10
.sp
The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.
.SS New features and improvements
.INDENT 0.0
.IP \(bu 2
New Scrapy service called \fBscrapyd\fP for deploying Scrapy crawlers in production (#218) (documentation available)
.IP \(bu 2
Simplified Images pipeline usage which doesn\(aqt require subclassing your own images pipeline now (#217)
.IP \(bu 2
Scrapy shell now shows the Scrapy log by default (#206)
.IP \(bu 2
Refactored execution queue in a common base code and pluggable backends called "spider queues" (#220)
.IP \(bu 2
New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.
.IP \(bu 2
Added documentation for Scrapy command\-line tool and all its available sub\-commands. (documentation available)
.IP \(bu 2
Feed exporters with pluggable backends (#197) (documentation available)
.IP \(bu 2
Deferred signals (#193)
.IP \(bu 2
Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)
.IP \(bu 2
Support for overriding default request headers per spider (#181)
.IP \(bu 2
Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)
.IP \(bu 2
Splitted Debian package into two packages \- the library and the service (#187)
.IP \(bu 2
Scrapy log refactoring (#188)
.IP \(bu 2
New extension for keeping persistent spider contexts among different runs (#203)
.IP \(bu 2
Added \fIdont_redirect\fP request.meta key for avoiding redirects (#233)
.IP \(bu 2
Added \fIdont_retry\fP request.meta key for avoiding retries (#234)
.UNINDENT
.SS Command\-line tool changes
.INDENT 0.0
.IP \(bu 2
New \fIscrapy\fP command which replaces the old \fIscrapy\-ctl.py\fP (#199)
\- there is only one global \fIscrapy\fP command now, instead of one \fIscrapy\-ctl.py\fP per project
\- Added \fIscrapy.bat\fP script for running more conveniently from Windows
.IP \(bu 2
Added bash completion to command\-line tool (#210)
.IP \(bu 2
Renamed command \fIstart\fP to \fIrunserver\fP (#209)
.UNINDENT
.SS API changes
.INDENT 0.0
.IP \(bu 2
\fBurl\fP and \fBbody\fP attributes of Request objects are now read\-only (#230)
.IP \(bu 2
\fBRequest.copy()\fP and \fBRequest.replace()\fP now also copies their \fBcallback\fP and \fBerrback\fP attributes (#231)
.IP \(bu 2
Removed \fBUrlFilterMiddleware\fP from \fBscrapy.contrib\fP (already disabled by default)
.IP \(bu 2
Offsite middelware doesn\(aqt filter out any request coming from a spider that doesn\(aqt have a allowed_domains attribute (#225)
.IP \(bu 2
Removed Spider Manager \fBload()\fP method. Now spiders are loaded in the constructor itself.
.IP \(bu 2
.INDENT 2.0
.TP
.B Changes to Scrapy Manager (now called "Crawler"):
.INDENT 7.0
.IP \(bu 2
\fBscrapy.core.manager.ScrapyManager\fP class renamed to \fBscrapy.crawler.Crawler\fP
.IP \(bu 2
\fBscrapy.core.manager.scrapymanager\fP singleton moved to \fBscrapy.project.crawler\fP
.UNINDENT
.UNINDENT
.IP \(bu 2
Moved module: \fBscrapy.contrib.spidermanager\fP to \fBscrapy.spidermanager\fP
.IP \(bu 2
Spider Manager singleton moved from \fBscrapy.spider.spiders\fP to the \fBspiders\(ga attribute of \(ga\(gascrapy.project.crawler\fP singleton.
.IP \(bu 2
.INDENT 2.0
.TP
.B moved Stats Collector classes: (#204)
.INDENT 7.0
.IP \(bu 2
\fBscrapy.stats.collector.StatsCollector\fP to \fBscrapy.statscol.StatsCollector\fP
.IP \(bu 2
\fBscrapy.stats.collector.SimpledbStatsCollector\fP to \fBscrapy.contrib.statscol.SimpledbStatsCollector\fP
.UNINDENT
.UNINDENT
.IP \(bu 2
default per\-command settings are now specified in the \fBdefault_settings\fP attribute of command object class (#201)
.IP \(bu 2
.INDENT 2.0
.TP
.B changed arguments of Item pipeline \fBprocess_item()\fP method from \fB(spider, item)\fP to \fB(item, spider)\fP
.INDENT 7.0
.IP \(bu 2
backwards compatibility kept (with deprecation warning)
.UNINDENT
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B moved \fBscrapy.core.signals\fP module to \fBscrapy.signals\fP
.INDENT 7.0
.IP \(bu 2
backwards compatibility kept (with deprecation warning)
.UNINDENT
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B moved \fBscrapy.core.exceptions\fP module to \fBscrapy.exceptions\fP
.INDENT 7.0
.IP \(bu 2
backwards compatibility kept (with deprecation warning)
.UNINDENT
.UNINDENT
.IP \(bu 2
added \fBhandles_request()\fP class method to \fBBaseSpider\fP
.IP \(bu 2
dropped \fBscrapy.log.exc()\fP function (use \fBscrapy.log.err()\fP instead)
.IP \(bu 2
dropped \fBcomponent\fP argument of \fBscrapy.log.msg()\fP function
.IP \(bu 2
dropped \fBscrapy.log.log_level\fP attribute
.IP \(bu 2
Added \fBfrom_settings()\fP class methods to Spider Manager, and Item Pipeline Manager
.UNINDENT
.SS Changes to settings
.INDENT 0.0
.IP \(bu 2
Added \fBHTTPCACHE_IGNORE_SCHEMES\fP setting to ignore certain schemes on !HttpCacheMiddleware (#225)
.IP \(bu 2
Added \fBSPIDER_QUEUE_CLASS\fP setting which defines the spider queue to use (#220)
.IP \(bu 2
Added \fBKEEP_ALIVE\fP setting (#220)
.IP \(bu 2
Removed \fBSERVICE_QUEUE\fP setting (#220)
.IP \(bu 2
Removed \fBCOMMANDS_SETTINGS_MODULE\fP setting (#201)
.IP \(bu 2
Renamed \fBREQUEST_HANDLERS\fP to \fBDOWNLOAD_HANDLERS\fP and make download handlers classes (instead of functions)
.UNINDENT
.SS 0.9
.sp
The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.
.SS New features and improvements
.INDENT 0.0
.IP \(bu 2
Added SMTP\-AUTH support to scrapy.mail
.IP \(bu 2
New settings added: \fBMAIL_USER\fP, \fBMAIL_PASS\fP (\fI\%r2065\fP | #149)
.IP \(bu 2
Added new scrapy\-ctl view command \- To view URL in the browser, as seen by Scrapy (\fI\%r2039\fP)
.IP \(bu 2
Added web service for controlling Scrapy process (this also deprecates the web console. (\fI\%r2053\fP | #167)
.IP \(bu 2
Support for running Scrapy as a service, for production systems (\fI\%r1988\fP, \fI\%r2054\fP, \fI\%r2055\fP, \fI\%r2056\fP, \fI\%r2057\fP | #168)
.IP \(bu 2
Added wrapper induction library (documentation only available in source code for now). (\fI\%r2011\fP)
.IP \(bu 2
Simplified and improved response encoding support (\fI\%r1961\fP, \fI\%r1969\fP)
.IP \(bu 2
Added \fBLOG_ENCODING\fP setting (\fI\%r1956\fP, documentation available)
.IP \(bu 2
Added \fBRANDOMIZE_DOWNLOAD_DELAY\fP setting (enabled by default) (\fI\%r1923\fP, doc available)
.IP \(bu 2
\fBMailSender\fP is no longer IO\-blocking (\fI\%r1955\fP | #146)
.IP \(bu 2
Linkextractors and new Crawlspider now handle relative base tag urls (\fI\%r1960\fP | #148)
.IP \(bu 2
Several improvements to Item Loaders and processors (\fI\%r2022\fP, \fI\%r2023\fP, \fI\%r2024\fP, \fI\%r2025\fP, \fI\%r2026\fP, \fI\%r2027\fP, \fI\%r2028\fP, \fI\%r2029\fP, \fI\%r2030\fP)
.IP \(bu 2
Added support for adding variables to telnet console (\fI\%r2047\fP | #165)
.IP \(bu 2
Support for requests without callbacks (\fI\%r2050\fP | #166)
.UNINDENT
.SS API changes
.INDENT 0.0
.IP \(bu 2
Change \fBSpider.domain_name\fP to \fBSpider.name\fP (SEP\-012, \fI\%r1975\fP)
.IP \(bu 2
\fBResponse.encoding\fP is now the detected encoding (\fI\%r1961\fP)
.IP \(bu 2
\fBHttpErrorMiddleware\fP now returns None or raises an exception (\fI\%r2006\fP | #157)
.IP \(bu 2
\fBscrapy.command\fP modules relocation (\fI\%r2035\fP, \fI\%r2036\fP, \fI\%r2037\fP)
.IP \(bu 2
Added \fBExecutionQueue\fP for feeding spiders to scrape (\fI\%r2034\fP)
.IP \(bu 2
Removed \fBExecutionEngine\fP singleton (\fI\%r2039\fP)
.IP \(bu 2
Ported \fBS3ImagesStore\fP (images pipeline) to use boto and threads (\fI\%r2033\fP)
.IP \(bu 2
Moved module: \fBscrapy.management.telnet\fP to \fBscrapy.telnet\fP (\fI\%r2047\fP)
.UNINDENT
.SS Changes to default settings
.INDENT 0.0
.IP \(bu 2
Changed default \fBSCHEDULER_ORDER\fP to \fBDFO\fP (\fI\%r1939\fP)
.UNINDENT
.SS 0.8
.sp
The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.
.SS New features
.INDENT 0.0
.IP \(bu 2
Added DEFAULT_RESPONSE_ENCODING setting (\fI\%r1809\fP)
.IP \(bu 2
Added \fBdont_click\fP argument to \fBFormRequest.from_response()\fP method (\fI\%r1813\fP, \fI\%r1816\fP)
.IP \(bu 2
Added \fBclickdata\fP argument to \fBFormRequest.from_response()\fP method (\fI\%r1802\fP, \fI\%r1803\fP)
.IP \(bu 2
Added support for HTTP proxies (\fBHttpProxyMiddleware\fP) (\fI\%r1781\fP, \fI\%r1785\fP)
.IP \(bu 2
Offiste spider middleware now logs messages when filtering out requests (\fI\%r1841\fP)
.UNINDENT
.SS Backwards\-incompatible changes
.INDENT 0.0
.IP \(bu 2
Changed \fBscrapy.utils.response.get_meta_refresh()\fP signature (\fI\%r1804\fP)
.IP \(bu 2
Removed deprecated \fBscrapy.item.ScrapedItem\fP class \- use \fBscrapy.item.Item instead\fP (\fI\%r1838\fP)
.IP \(bu 2
Removed deprecated \fBscrapy.xpath\fP module \- use \fBscrapy.selector\fP instead. (\fI\%r1836\fP)
.IP \(bu 2
Removed deprecated \fBcore.signals.domain_open\fP signal \- use \fBcore.signals.domain_opened\fP instead (\fI\%r1822\fP)
.IP \(bu 2
.INDENT 2.0
.TP
.B \fBlog.msg()\fP now receives a \fBspider\fP argument (\fI\%r1822\fP)
.INDENT 7.0
.IP \(bu 2
Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the \fBspider\fP argument and pass spider references. If you really want to pass a string, use the \fBcomponent\fP argument instead.
.UNINDENT
.UNINDENT
.IP \(bu 2
Changed core signals \fBdomain_opened\fP, \fBdomain_closed\fP, \fBdomain_idle\fP
.IP \(bu 2
.INDENT 2.0
.TP
.B Changed Item pipeline to use spiders instead of domains
.INDENT 7.0
.IP \(bu 2
The \fBdomain\fP argument of  \fBprocess_item()\fP item pipeline method was changed to  \fBspider\fP, the new signature is: \fBprocess_item(spider, item)\fP (\fI\%r1827\fP | #105)
.IP \(bu 2
To quickly port your code (to work with Scrapy 0.8) just use \fBspider.domain_name\fP where you previously used \fBdomain\fP\&.
.UNINDENT
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B Changed Stats API to use spiders instead of domains (\fI\%r1849\fP | #113)
.INDENT 7.0
.IP \(bu 2
\fBStatsCollector\fP was changed to receive spider references (instead of domains) in its methods (\fBset_value\fP, \fBinc_value\fP, etc).
.IP \(bu 2
added \fBStatsCollector.iter_spider_stats()\fP method
.IP \(bu 2
removed \fBStatsCollector.list_domains()\fP method
.IP \(bu 2
Also, Stats signals were renamed and now pass around spider references (instead of domains). Here\(aqs a summary of the changes:
.IP \(bu 2
To quickly port your code (to work with Scrapy 0.8) just use \fBspider.domain_name\fP where you previously used \fBdomain\fP\&. \fBspider_stats\fP contains exactly the same data as \fBdomain_stats\fP\&.
.UNINDENT
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B \fBCloseDomain\fP extension moved to \fBscrapy.contrib.closespider.CloseSpider\fP (\fI\%r1833\fP)
.INDENT 7.0
.IP \(bu 2
.INDENT 2.0
.TP
.B Its settings were also renamed:
.INDENT 7.0
.IP \(bu 2
\fBCLOSEDOMAIN_TIMEOUT\fP to \fBCLOSESPIDER_TIMEOUT\fP
.IP \(bu 2
\fBCLOSEDOMAIN_ITEMCOUNT\fP to \fBCLOSESPIDER_ITEMCOUNT\fP
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.IP \(bu 2
Removed deprecated \fBSCRAPYSETTINGS_MODULE\fP environment variable \- use \fBSCRAPY_SETTINGS_MODULE\fP instead (\fI\%r1840\fP)
.IP \(bu 2
Renamed setting: \fBREQUESTS_PER_DOMAIN\fP to \fBCONCURRENT_REQUESTS_PER_SPIDER\fP (\fI\%r1830\fP, \fI\%r1844\fP)
.IP \(bu 2
Renamed setting: \fBCONCURRENT_DOMAINS\fP to \fBCONCURRENT_SPIDERS\fP (\fI\%r1830\fP)
.IP \(bu 2
Refactored HTTP Cache middleware
.IP \(bu 2
HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (\fI\%r1843\fP )
.IP \(bu 2
Renamed exception: \fBDontCloseDomain\fP to \fBDontCloseSpider\fP (\fI\%r1859\fP | #120)
.IP \(bu 2
Renamed extension: \fBDelayedCloseDomain\fP to \fBSpiderCloseDelay\fP (\fI\%r1861\fP | #121)
.IP \(bu 2
Removed obsolete \fBscrapy.utils.markup.remove_escape_chars\fP function \- use \fBscrapy.utils.markup.replace_escape_chars\fP instead (\fI\%r1865\fP)
.UNINDENT
.SS 0.7
.sp
First release of Scrapy.
.SS Contributing to Scrapy
.sp
There are many ways to contribute to Scrapy. Here are some of them:
.INDENT 0.0
.IP \(bu 2
Blog about Scrapy. Tell the world how you\(aqre using Scrapy. This will help
newcomers with more examples and the Scrapy project to increase its
visibility.
.IP \(bu 2
Report bugs and request features in the \fI\%issue tracker\fP, trying to follow
the guidelines detailed in \fI\%Reporting bugs\fP below.
.IP \(bu 2
Submit patches for new functionality and/or bug fixes. Please read
\fI\%Writing patches\fP and \fI\%Submitting patches\fP below for details on how to
write and submit a patch.
.IP \(bu 2
Join the \fI\%scrapy\-developers\fP mailing list and share your ideas on how to
improve Scrapy. We\(aqre always open to suggestions.
.UNINDENT
.SS Reporting bugs
.sp
Well\-written bug reports are very helpful, so keep in mind the following
guidelines when reporting a new bug.
.INDENT 0.0
.IP \(bu 2
check the \fIFAQ\fP first to see if your issue is addressed in a
well\-known question
.IP \(bu 2
check the \fI\%open issues\fP to see if it has already been reported. If it has,
don\(aqt dismiss the report but check the ticket history and comments, you may
find additional useful information to contribute.
.IP \(bu 2
search the \fI\%scrapy\-users\fP list to see if it has been discussed there, or
if you\(aqre not sure if what you\(aqre seeing is a bug. You can also ask in the
\fI#scrapy\fP IRC channel.
.IP \(bu 2
write complete, reproducible, specific bug reports. The smaller the test
case, the better. Remember that other developers won\(aqt have your project to
reproduce the bug, so please include all relevant files required to reproduce
it.
.IP \(bu 2
include the output of \fBscrapy version \-v\fP so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.
.UNINDENT
.SS Writing patches
.sp
The better written a patch is, the higher chance that it\(aqll get accepted and
the sooner that will be merged.
.sp
Well\-written patches should:
.INDENT 0.0
.IP \(bu 2
contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you\(aqre doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.
.IP \(bu 2
pass all unit\-tests. See \fI\%Running tests\fP below.
.IP \(bu 2
include one (or more) test cases that check the bug fixed or the new
functionality added. See \fI\%Writing tests\fP below.
.IP \(bu 2
if you\(aqre adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See \fI\%Documentation policies\fP
below.
.UNINDENT
.SS Submitting patches
.sp
The best way to submit a patch is to issue a \fI\%pull request\fP on Github,
optionally creating a new issue first.
.sp
Remember to explain what was fixed or the new functionality (what it is, why
it\(aqs needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.
.sp
You can also discuss the new functionality (or bug fix) in \fI\%scrapy\-developers\fP
first, before creating the patch, but it\(aqs always good to have a patch ready to
illustrate your arguments and show that you have put some additional thought
into the subject.
.sp
Finally, try to keep aesthetic changes (\fI\%PEP 8\fP compliance, unused imports
removal, etc) in separate commits than functional changes, to make the pull
request easier to review.
.SS Coding style
.sp
Please follow these coding conventions when writing code for inclusion in
Scrapy:
.INDENT 0.0
.IP \(bu 2
Unless otherwise specified, follow \fI\%PEP 8\fP\&.
.IP \(bu 2
It\(aqs OK to use lines longer than 80 chars if it improves the code
readability.
.IP \(bu 2
Don\(aqt put your name in the code you contribute. Our policy is to keep
the contributor\(aqs name in the \fI\%AUTHORS\fP file distributed with Scrapy.
.UNINDENT
.SS Scrapy Contrib
.sp
Scrapy contrib shares a similar rationale as Django contrib, which is explained
in \fI\%this post\fP\&. If you
are working on a new functionality, please follow that rationale to decide
whether it should be a Scrapy contrib. If unsure, you can ask in
\fI\%scrapy\-developers\fP\&.
.SS Documentation policies
.INDENT 0.0
.IP \(bu 2
\fBDon\(aqt\fP use docstrings for documenting classes, or methods which are
already documented in the official (sphinx) documentation. For example, the
\fBItemLoader.add_value()\fP method should be documented in the sphinx
documentation, not its docstring.
.IP \(bu 2
\fBDo\fP use docstrings for documenting functions not present in the official
(sphinx) documentation, such as functions from \fBscrapy.utils\fP package and
its sub\-modules.
.UNINDENT
.SS Tests
.sp
Tests are implemented using the \fI\%Twisted unit\-testing framework\fP called
\fBtrial\fP\&.
.SS Running tests
.sp
To run all tests go to the root directory of Scrapy source code and run:
.INDENT 0.0
.INDENT 3.5
\fBbin/runtests.sh\fP (on unix)
.sp
\fBbin\eruntests.bat\fP (on windows)
.UNINDENT
.UNINDENT
.sp
To run a specific test (say \fBscrapy.tests.test_contrib_loader\fP) use:
.INDENT 0.0
.INDENT 3.5
\fBbin/runtests.sh scrapy.tests.test_contrib_loader\fP (on unix)
.sp
\fBbin\eruntests.bat scrapy.tests.test_contrib_loader\fP (on windows)
.UNINDENT
.UNINDENT
.SS Writing tests
.sp
All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.
.sp
Scrapy uses unit\-tests, which are located in the \fBscrapy.tests\fP package
(\fI\%scrapy/tests\fP directory). Their module name typically resembles the full
path of the module they\(aqre testing. For example, the item loaders code is in:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy.contrib.loader
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
And their unit\-tests are in:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
scrapy.tests.test_contrib_loader
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Versioning and API Stability
.SS Versioning
.sp
Scrapy uses the \fI\%odd\-numbered versions for development releases\fP\&.
.sp
There are 3 numbers in a Scrapy version: \fIA.B.C\fP
.INDENT 0.0
.IP \(bu 2
\fIA\fP is the major version. This will rarely change and will signify very
large changes. So far, only zero is available for \fIA\fP as Scrapy hasn\(aqt yet
reached 1.0.
.IP \(bu 2
\fIB\fP is the release number. This will include many changes including features
and things that possibly break backwards compatibility. Even Bs will be
stable branches, and odd Bs will be development.
.IP \(bu 2
\fIC\fP is the bugfix release number.
.UNINDENT
.sp
For example:
.INDENT 0.0
.IP \(bu 2
\fI0.14.1\fP is the first bugfix release of the \fI0.14\fP series (safe to use in
production)
.UNINDENT
.SS API Stability
.sp
API stability is one of Scrapy major goals for the \fI1.0\fP release, which doesn\(aqt
have a due date scheduled yet.
.sp
Methods or functions that start with a single dash (\fB_\fP) are private and
should never be relied as stable. Besides those, the plan is to stabilize and
document the entire API, as we approach the 1.0 release.
.sp
Also, keep in mind that stable doesn\(aqt mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.
.SS Experimental features
.sp
This section documents experimental Scrapy features that may become stable in
future releases, but whose API is not yet stable. Use them with caution, and
subscribe to the \fI\%mailing lists\fP to get
notified of any changes.
.sp
Since it\(aqs not revised so frequently, this section may contain documentation
which is outdated, incomplete or overlapping with stable documentation (until
it\(aqs properly merged) . Use at your own risk.
.sp
\fBWARNING:\fP
.INDENT 0.0
.INDENT 3.5
This documentation is a work in progress. Use at your own risk.
.UNINDENT
.UNINDENT
.SS Add commands using external libraries
.sp
You can also add Scrapy commands from an external library by adding \fIscrapy.commands\fP section into entry_points in the \fIsetup.py\fP\&.
.sp
The following example adds \fImy_command\fP command:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
from setuptools import setup, find_packages

setup(name=\(aqscrapy\-mymodule\(aq,
  entry_points={
    \(aqscrapy.commands\(aq: [
      \(aqmy_command=my_scrapy_module.commands:MyCommand\(aq,
    ],
  },
 )
.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \fBnews\fP
See what has changed in recent Scrapy versions.
.TP
.B \fBcontributing\fP
Learn how to contribute to the Scrapy project.
.TP
.B \fBversioning\fP
Understand Scrapy versioning and API stability.
.TP
.B \fBexperimental/index\fP
Learn about bleeding\-edge features.
.UNINDENT
.SH COPYRIGHT
2008-2013, Scrapy developers
.\" Generated by docutils manpage writer.
.
