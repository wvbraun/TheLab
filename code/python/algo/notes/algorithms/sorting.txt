Sorting -
	The process of placing elements from a collection in some kind
	of order. Like searching, the efficieny of a sorting algo is 
	related to the number of items being processed. For small
	collections, a complex sorting method may not be worth it. 
	On the other hand, for larger collections, we want to take
	advantages of as many improvements as possible. 

1. It is necessary to compare two values to see which is smaller
orlarger. In order to sort a collection, it will be necessary to
have some systematic way to compare values to see if they are out
of order. The total number of comparisons will be the most 
common way to measure a sort procedure. 

2. When values are not in the correct position with respect to 
one another, it may be necessary to exchange them. This exchange
is a costly operation and the total number of exchanges will be
important for evaluating the overall efficiency of the algorithm.

-------------------------------------------------------------------
The Bubble Sort - O(n**2):

	Makes multiple passes through a list. It compares adjacent 
	items and exchanges those that are out of order. Each pass
	through the list places the next largest value in its proper
	place. In essence, each item 'bubble' up to the location 
	where it belongs. 

The bubble sort compares two items to see if they are out of 
order. If there are n items in the list, then are are (n-1) pairs
of items that need to be compared on the first pass. It is 
important to note that once the largest value in the list is 
part of a pair, it will continually be moved along until the
pass is complete. At the start of the second pass, the 
largest value is now in place. There are (n - 1) items left to 
sort, meaning that there will be (n-2) pairs. since each pass
places the net largest value in place, the total number of passes
needed will be (n-1). After completing the (n-1) passes, the 
smalles item must be in the correct position with no further
processing required. 

The exchange operation, sometimes called a 'swap', is slightly
different in Python than in most languages. Typically, swapping
2 elements in a list requires a temp storage location (an 
additional memory location). important to note that once the largest value in the list is 
part of a pair, it will continually be moved along until the
pass is complete. 

Ex:

temp = alist[i]
alist[i] = alist[j]
alist[j] = temp

will exchange the ith and jth items in the list. Withouth the 
temp storage, one of the values would be overwritten. 

In python, you can perform simultaneous assignment. The statement
ex: 
a,b = b,a 	# a = b, b = a
will result in 2 assignment statements being done at the same time.

--------------------
Tracing the Bubble Sort (bubbletrace)

To analyze the bubble sort, we should note that regardless of 
how the items are arranged in the initial list, (n-1) passes
will bemade to sort a list of size n. The total number of 
comparisons is the sum of the first (n-1) integers. Recall that
the sum of the first n integers is:
(1/2)*(n**2) + (1/2)*n

The sum of the first n-1 integers is:
(1/2)*(n**2) - (1/2)*n. 

This is still O(n**2) comparisons. 

In the best case, if the list is already ordered, no exchanges
will be made. However, in the worst case, every comparison will
cause an exchange. On average, we exchange half of the time. 

	Comparisons of Each Pass of Bubble Sort 

+-------------------------------+
|	Pass	|	Comparisons		|
|-----------+-------------------|
|	1		|	(n-1)			|
|-----------+-------------------|
|	2		|	(n-2)			|
|-----------+-------------------|
|	3		|	(n-3)		    |
|-----------+-------------------|
|  ...		|	 ...			|
|-----------+-------------------|
| (n-1)		|	(1)				|
+-------------------------------+

A bubble sort is considered the most inefficent sorting method 
since it must exchange items before the final location is known. 
However, because the bubble sort makes passes through the entire
unsorted portion of the list, it has the capability to do something 
most sorting algorithms cannot. In particular, if during a pass
there are no exchanges, then we know that the list must be
sorted. A bubble sort can be modifed to stop early if it
finds that the list has become sorted. 

----------------------------------------------------------------
The Selection Sort - O(n**2):

	Improves on the bubble sort by making only one exchange for
	every pass through the list. In order to do this, a selection
	sort looks for the largest value as it makes a pass, and
	after completing the pass, places it inthe proper location. 
	After the first pass, the largest item is in the correct place. 
	After the second pass, the next largest item is in place. 
	This process continues and requires (n-2) passes to sort 
	n items, since the final item must be in place after the 
	(n-1) pass. 


Tracing the Selection Sort (selection_sort_code_trace)

You may notice the selection sort makes the same number of 
comparisons as the bubble sort, therefore it is also O(n**2). 
However, due to the reduction in the number of exchanges, the
selection sort typically executes faster in benchmak studies. 
In fact, for our list, the bubble sort makes 20 exchanges while
the selection sort only makes 8. 

---------------------------------------------------------------
The Insertion Sort - O(n**2): 

	It always maintains a sorted sublist in the lower positions
	of the list. Each new item is then 'inserted' back into
	the previous sublist such that the sorted sublist is one 
	item larger. 

	We being by assuming that a list with one item, position 0, 
	is already sorted. On each pass, one for each item 1 though
	(n-1), the current item is checked against those in the 
	already sorted sublist. As we look back into the already
	sorted sublist, we shift those items that are greater to the
	right. When we reach a smaller item, or the end of the 
	sublist, the current item can be inserted. 

	The implementation of insertionSort shows that there are
	again (n-1) passes to sort n items. The iteration starts
	at position 1 and moves through position (n-1), as these
	are the items that need to be inserted back into the 
	sorted sublists.

	The max number of comparisons for an insertion sort is the
	sum of the first (n-1) integers; this is O(n**2). However,
	in the best case only one comparison needs to be done on 
	each pass. 

	One note about shifting versus exchanges: In general, a 
	shift operation requiers approximately a third of the 
	processing work of an exchange since only one assignment
	is performed. 

----------------------------------------------------------------
The Shell Sort -
	
	The shell sort, sometimes called the 'diminishing increment sort' improves on the 
	insertion sort by breaking the original list into a number of smaller sublists, each
	of which is sorted using an insertion sort. Instead of breaking the list into sublists
	of contiguous items, the shell sort uses an increment i, sometimes called the gap, to
	create a sublist by choosing all items that are i items apart. 


Ex: list: 54 26 93 17 77 31 44 55 20


[54] 26 93 [17] 77 31 [44] 55 20 		- sublist 1
54 [26] 93 17 [77] 31 44 [55] 20		- sublist 2
54 26 [93] 17 77 [31] 44 55 [20] 		- sublist 3 

[17] ... [44] ... [54]
	[26] ... [55] ... [77]
	  [20] ... [31] ... [93]

17 26 20 44 55 31 54 77 93

17 20 26 31 44 54 55 77 93

This list has 9 items. If we use an increment of 3, there are then 3 sublists, each of which can 
be sorted by an insertion sort. After completing these sorts, we notice that we have moved the 
items very close to where the belong. From there we can perform four more shifts to complete the 
process. 


--------------------------
Tracing the Shell Sort

It tends to fall between O(n) and O(n**2).

--------------------------------------------------------------------------------------------------
The Merge Sort - 

	The merge sort is a recursive algorithm that continually splits a list in half. If the list is 
	empty, or has one item, it is sorted by definition (the base case). If the list has more than 
	one item, we split the list and recursively invoke a merge sort on both halves. Once the two 
	halves are sorted, the fundamental operation, called a merge, is performed. Merging is the process
	of taking two smaller sorted lists and combining them together into a single, sorted, new list. 
	

---------------------------
Tracing the Merge Sort:

In order to analyze the mergesort function, we must consider the two distinct processes that make up
its implemntation. First, the list is split into two halves. We already computed (in a binary search)
that we can divide a list in half log(n) timnes where n is the length of the list. The second process
is the merge. Each item in the list will eventually be processed and placed on the sorted list. So the
merge operation which results in a list of size n, requires n operations. The result of this analysis
is that log(n) splits, each of which costs n, for a total of (n)log(n) operations. A merge sort is 
an O(nlog(n)) algorithm. 

Recall that the slicing operator is O(k) where k is the size of the slice. In order to guranatee that
mergeSort will be O(nlogn) we need to remove the slice operator. This is possible if we simply pass 
the starting and ending indices along with the list when we make the recursive call. 

It is important to note that the mergesort function requires extra space to hold the two halves
as they are extracted with the slicing operations. This additional space can be a critical factor
if the list is large and can make this sort problematic when working on large data sets. 



------------------------------------------------------------------------------------------------------
The Quick Sort - 

	The quick sort uses divide and conquer to gain the same advantages as the merge sort, while
	not using additional storage. As a trade-off, however, it is posible that the list may not be
	divided in half. When this happens, we will notice that performance is diminished. 

	A quick sort first selects a value, which is called the pivot value. Although there are many 
	different ways to choice the pivot value, we will simply use the first item in the list. The role
	of the pivot value is to assist with splitting the list. The actual position where the pivot 
	value belongs in the final sorted list, commonly called the split point, will be used to divide 
	the list for subsequent calls to the quick sort. 

	Partitioning begins by locating two position markers - leftmark and rightmark - at the beginning
	and end of the remaining items in the list. 
	We begin by incrementing leftmark until we locate a value that is greater than the pivot value. 
	We then decrement rightmark until we find a value that is less than the pivot value. At this point
	we have discovered two items that are out of plce with respecct to the eventual split point. Now
	we can exchange these two items and repeate the process again. At the point where rightmark
	becomes left than leftmark, we stop. The position of rightmark is now the split point. The pivot
	value can be exchanged with the contents of the split point and the pivot value is now in place. 
	In addition, all the items to the left of the split point are less than the pivot value, and all
	the items to the right of the split point are greater than the pivot value. The list can now 
	be divided at the split point and the quick sort can be invoked recurisvely on the two halves. 

---------------
Tracing the Quick Sort

To analyze the quick sort function, note that for a list of length n, if the partition always occurs
in the middle of thelist, there willbe log(n) divisions. In order to find the split point, each of the
n items needs to be checked against the pivot value. The result is nlog(n). 

Unfortunately, in the worst case, the split points may not be in the middle and can be very skewed
to the left or the right, leaving a very uneven division. In this case, sorting a list of n items 
divides into sorting a list of 0 items and a list of n-1 items. Then sorting a list of n-1 divides 
into a list of size 0 and a list of size n-2, and so on. The result is an O(n**2) sort with all of the
overhead the recusion requires. 

Earlier we mentioned that there are different ways to choose the pivot value. In particular, we can
attemp to alleviate some of the potentinal for an uneven division by using a technique called 
median of three. To choose the pivot value, we will consider the first, the middle, and the last 
element in the list. In our example, those are 54, 77, and 20. Now pick the median value, in our case
54, and use it for the pivot value.  
